<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>LambdaClass Blog - Statistics</title>
    <subtitle>Deep technical insights on cryptography, distributed systems, zero-knowledge proofs, and cutting-edge software engineering from the LambdaClass team.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://blog.lambdaclass.com/tags/statistics/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2020-05-06T00:00:00+00:00</updated>
    <id>https://blog.lambdaclass.com/tags/statistics/atom.xml</id>
    <entry xml:lang="en">
        <title>A brief introduction to the beauty of Information Theory</title>
        <published>2020-05-06T00:00:00+00:00</published>
        <updated>2020-05-06T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/a-brief-introduction-to-the-beauty-of-information-theory/"/>
        <id>https://blog.lambdaclass.com/posts/a-brief-introduction-to-the-beauty-of-information-theory/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/a-brief-introduction-to-the-beauty-of-information-theory/">&lt;h4 id=&quot;or-how-to-be-a-hardcore-guess-who-gamer&quot;&gt;Or how to be a hardcore Guess Who gamer&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;em&gt;Authors: Juan Pablo Amoroso, Javier Rodríguez Chatruc, Camilo Plata, and Federico Carrone.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.&lt;br &#x2F;&gt;
— Claude Shannon, 1948&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Imagine you were tasked with designing a comunications system between a space station and a ground control headquarters back in Earth. The system would transmit and receive messages encoded in binary, that is, as a sequence of 1s and 0s. As the message travels, there may be interferences from other radio signals, so that what is picked up in ground control is not exactly the same as the original message. Under these circumstances, is it possible to devise a scheme that allows reliable comunication?&lt;&#x2F;p&gt;
&lt;p&gt;A simple workaround would be to add redundancy: send each bit a number of times, let’s say 5:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;11111&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;00000&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;…&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If ground control receives the message &lt;em&gt;11101&lt;&#x2F;em&gt; , they could be fairly certain that what was truly sent was &lt;em&gt;11111&lt;&#x2F;em&gt;. Although this simple system would work (up to a point), we can see that it is very wasteful: we have to send 4 extra bits for every bit in the original message. The &lt;em&gt;transmission rate&lt;&#x2F;em&gt; is therefore only 20%. Can we do any better?&lt;&#x2F;p&gt;
&lt;p&gt;There seems to be a dilemma here: if we want accuracy, we must lower the rate of transmission.&lt;&#x2F;p&gt;
&lt;p&gt;This is the problem Claude Shannon tackled in his 1948 paper &lt;em&gt;A Mathematical Theory of Communication&lt;&#x2F;em&gt;. In it, he proved that there is a limit for the rate of information that can be reliably transmitted over a noisy channel (the &lt;em&gt;Shannon limit&lt;&#x2F;em&gt;). However, below this limit we can transmit information with an increasingly small error. This important result tells us that there &lt;em&gt;exists&lt;&#x2F;em&gt; a code that allows arbitrary accuracy over a given comunication channel, but it does not tell us how to build it.&lt;&#x2F;p&gt;
&lt;p&gt;More precisely, let’s say a channel has a probability &lt;em&gt;p&lt;&#x2F;em&gt; of transmitting a bit correctly, and a corresponding probability of 1 —  &lt;em&gt;p&lt;&#x2F;em&gt; of sending the wrong bit, Shannon proved that the optimum rate of transmission is:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-kCNAUFlhWJv1kUzKatqINA.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-TgCse1znfuWYULYwXeo4Aw.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The plot is symmetrical around &lt;em&gt;p = 0.5&lt;&#x2F;em&gt; , with maxima at &lt;em&gt;p = 0&lt;&#x2F;em&gt; and &lt;em&gt;p = 1&lt;&#x2F;em&gt;. The case of &lt;em&gt;p = 0&lt;&#x2F;em&gt; is interesting, the channel has perfect noise: it flips all the bits in the original message. But if we know that, then the message is trivially deciphered, we just flip them back.&lt;&#x2F;p&gt;
&lt;p&gt;The formula is commonly stated in terms of &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Entropy_%28information_theory%29&quot;&gt;information entropy&lt;&#x2F;a&gt;, a measure Shannon devised that can be interpreted as the level of ‘uncertainty’ or ‘surprise’ associated with the channel.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-fCyO-nZidJEiOqwqDGWJ3g.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-s0z3MUCTtJvh_AsvxGg72g.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We can see that the entropy has a maximum at 1 when &lt;em&gt;p&lt;&#x2F;em&gt; = ½, and minima at 0 for &lt;em&gt;p =&lt;&#x2F;em&gt; 0 and &lt;em&gt;p =&lt;&#x2F;em&gt; 1.&lt;&#x2F;p&gt;
&lt;p&gt;More generally, given a random message &lt;em&gt;M&lt;&#x2F;em&gt; that can take &lt;em&gt;n&lt;&#x2F;em&gt; different values with probability &lt;em&gt;pᵢ&lt;&#x2F;em&gt; for &lt;em&gt;i =&lt;&#x2F;em&gt; 1,…,&lt;em&gt;n&lt;&#x2F;em&gt; , we define the entropy of the message as:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-ezweLprVK1INseQwDCN2yg.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;guess-who-example&quot;&gt;Guess Who example&lt;&#x2F;h4&gt;
&lt;p&gt;Let’s take a different approach. Suppose you are playing &lt;em&gt;Guess Who&lt;&#x2F;em&gt; , the game where you ask yes&#x2F;no questions about the appearance of your opponent’s character in order to single him or her out among a set of characters. You ask yourself: what order should I ask the questions in to maximise the probability of winning? Intutively, you try to ask first about features most of the characters have.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-TkW9quvg52IBgM06fM-7IA.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;Hardcore Guess Who gamers apply Information Theory for optimal results&lt;&#x2F;p&gt;
&lt;p&gt;Moreover, an optimal question is one that divides the population evenly, that is, one that regardless of the answer (&lt;em&gt;yes&lt;&#x2F;em&gt; or &lt;em&gt;no&lt;&#x2F;em&gt;) discards half the characters. In any other case, you are not gaining the optimal amount of information with each question.&lt;&#x2F;p&gt;
&lt;p&gt;But what if you can’t divide the characters evenly by their characteristics? To answer the question, first we recall the concept of entropy defined above. We can think of a question as a variable &lt;em&gt;X&lt;&#x2F;em&gt; that splits the population into groups &lt;em&gt;xᵢ&lt;&#x2F;em&gt; with probabilities &lt;em&gt;pᵢ&lt;&#x2F;em&gt;. For example, think of a question about the eye color of the character (the questions in the game are technically only &lt;em&gt;yes&lt;&#x2F;em&gt; or &lt;em&gt;no&lt;&#x2F;em&gt; but this can be generalized). With this in mind, the entropy of a question becomes:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-KZq1CO03SyEnsH0qLamzRQ.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The intuition here is that with each possible answer, we gain an amount of information  &lt;em&gt;— log&lt;&#x2F;em&gt; &lt;em&gt;p&lt;&#x2F;em&gt;(&lt;em&gt;x&lt;&#x2F;em&gt; ᵢ), meaning that if we receive an answer with a very low probability (i.e. we ask if the character has a feature that is shared by very few people, and the answer is yes), the amount of information we gained is higher than an answer with more probability.&lt;&#x2F;p&gt;
&lt;p&gt;On the other hand, entropy is related to uncertainty. For example, if we flip a coin, the uncertainty in the outcome is higher with a &lt;em&gt;p&lt;&#x2F;em&gt; = 0.5 than with any other value of &lt;em&gt;p&lt;&#x2F;em&gt;. And in our case, more uncertainty is better. Why? If we choose a question with an uneven distribution in the population, lets say 0.7 and 0.3, the odds are that our character is among the 70%, discarding with the &lt;em&gt;no&lt;&#x2F;em&gt; answer only the remaining 30%, but with a more even division (and therefore more uncertain), we always tend to discard 50% of the population, leading to an advantage in the long run. This means that the best questions to ask are those that maximize the entropy, i.e, the ones with the higher uncertainty.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;decision-trees&quot;&gt;Decision Trees&lt;&#x2F;h4&gt;
&lt;p&gt;One common use of entropy is in decision trees, where one uses a set of features (features that split the data into disjoint sets) to construct a flowchart for a classification problem. Here, a common question is: which order should we “apply” the features in to get the best splits? A possible solution is to recursively always use the feature that maximizes the &lt;em&gt;information gain&lt;&#x2F;em&gt;. If we’re working with a dataset &lt;em&gt;S&lt;&#x2F;em&gt; and our feature is called &lt;em&gt;X&lt;&#x2F;em&gt; , the information gained on &lt;em&gt;S&lt;&#x2F;em&gt; by &lt;em&gt;X&lt;&#x2F;em&gt; , &lt;em&gt;I&lt;&#x2F;em&gt;(&lt;em&gt;S&lt;&#x2F;em&gt; ,&lt;em&gt;X&lt;&#x2F;em&gt;), is calculated as:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-tIzlfBpMihRvpfZICpWZJA.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;where &lt;em&gt;H&lt;&#x2F;em&gt;(&lt;em&gt;S&lt;&#x2F;em&gt; |&lt;em&gt;X&lt;&#x2F;em&gt;) is the conditional entropy of &lt;em&gt;S&lt;&#x2F;em&gt; given &lt;em&gt;X&lt;&#x2F;em&gt;. Intuitively, this is just the reduction in the entropy of the dataset &lt;em&gt;S&lt;&#x2F;em&gt; if we know &lt;em&gt;X&lt;&#x2F;em&gt;. Thus, it makes sense to choose the features &lt;em&gt;X&lt;&#x2F;em&gt; that maximize this value, as they will be the ones that reduce uncertainty the most, effectively obtaining the best splits.&lt;&#x2F;p&gt;
&lt;p&gt;Algorithms that consider the information gain at each node to choose the next feature are called &lt;em&gt;greedy&lt;&#x2F;em&gt; algorithms. Such algorithms do not take into account the overall information gain and may lead in some cases to suboptimal queries, but they are well-behaved and have a straightforward approach.&lt;&#x2F;p&gt;
&lt;p&gt;As an example, consider the picture below, where a decision tree method was used on the famous Iris flower dataset and two features were selected, the petal width, first with 0.8 cm as a threshold and then 1.75 cm. Setting aside how these specific features are selected, why use the ≤ 0.8 first? With the information gain calculation we described, we can provide an answer. We will call the feature that separates petal width on 0.8 cm &lt;em&gt;X&lt;&#x2F;em&gt; and the other one &lt;em&gt;Y&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-dEesB-YyIVG81qhIDn_T_w.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Applying &lt;em&gt;X&lt;&#x2F;em&gt; first splits the 150 data points (usually one would split between training and test sets, here for simplicity we use the entire set) into two sets: one containing the entire &lt;em&gt;setosa&lt;&#x2F;em&gt; class (50 points, corresponding to ≤ 0.8 cm) and nothing else, and the other containing the rest. In that case the calculations yield:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-2dDOZS_8PGYonq3RZYoyAg.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;On the other hand, applying &lt;em&gt;Y&lt;&#x2F;em&gt; first gives us one set with 50 &lt;em&gt;setosa&lt;&#x2F;em&gt; , 49 &lt;em&gt;versicolor&lt;&#x2F;em&gt; and 5 &lt;em&gt;virginica&lt;&#x2F;em&gt; (≤ 1.75 cm) and another with no &lt;em&gt;setosa&lt;&#x2F;em&gt; , 1 &lt;em&gt;versicolor&lt;&#x2F;em&gt; and 45 &lt;em&gt;virginica&lt;&#x2F;em&gt;. This leaves us with:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-VCQsujq_mEufMZi1X4DGdw.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Thus the information gain from &lt;em&gt;X&lt;&#x2F;em&gt; (petal width being under or over 0.8 cm) is greater than the one from &lt;em&gt;Y&lt;&#x2F;em&gt; , and we should use it first. This makes sense intuitively, as &lt;em&gt;X&lt;&#x2F;em&gt; completely separates the &lt;em&gt;setosa&lt;&#x2F;em&gt; class from the other two, whereas using &lt;em&gt;Y&lt;&#x2F;em&gt; first gives a more entangled split.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h4&gt;
&lt;p&gt;It is hard to overstate the importance of Shannon’s work: the Theory of Information has found &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.britannica.com&#x2F;science&#x2F;information-theory&#x2F;Applications-of-information-theory&quot;&gt;many applications&lt;&#x2F;a&gt; in fields as diverse as statistical inference and machine learning, natural language processing, genetics, data compression, coding theory, and cryptography. With over 120,000 citations, few papers can boast a similar impact. In the words of information theorist Anthony Ephremides:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;It was like an earthquake and the aftershocks haven’t finished yet!&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Interview with Will Kurt on his latest book: Bayesian Statistics The Fun Way</title>
        <published>2019-06-04T00:00:00+00:00</published>
        <updated>2019-06-04T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/interview-with-will-kurt-on-his-latest-book-bayesian-statistics-the-fun-way/"/>
        <id>https://blog.lambdaclass.com/posts/interview-with-will-kurt-on-his-latest-book-bayesian-statistics-the-fun-way/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/interview-with-will-kurt-on-his-latest-book-bayesian-statistics-the-fun-way/">&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-LDJcJQMeyOPU9lqAs98JBQ.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Like most devs, I have a diverse set of interests: functional programming, operating systems, type systems, distributed systems, and data science. That is why I was excited when I learned that &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;willkurt&quot;&gt;Will Kurt&lt;&#x2F;a&gt;, the author of &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.manning.com&#x2F;books&#x2F;get-programming-with-haskell&quot;&gt;&lt;em&gt;Get Programming with Haskell,&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;__ wrote a a bayesian statistics book that is being published by No Starch Press. There aren’t many people that write books on different topics. I was sure that Will had something interesting to share in this new book. I wasn’t disappointed. The book is an excellent introduction, specially for those of us that have a rough time with advanced math but that want to advance in the data science field. I recommend reading the book after reading Think Stats, but before reading Bayesian Methods for Hackers, Bayesian Analysis with Python and Doing Bayesian Data Analysis.&lt;&#x2F;p&gt;
&lt;p&gt;If you like the interview I recommend that you also read the interviews we did with &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;notamonadtutorial.com&#x2F;inteview-with-thomas-wiecki-about-probabilistic-programming-and-pymc-66a12b6f3f2e&quot;&gt;Thomas Wiecki&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;notamonadtutorial.com&#x2F;interview-with-osvaldo-martin-about-bayesian-analysis-with-python-a696b2bce3ba&quot;&gt;Osvaldo Martin&lt;&#x2F;a&gt; about Bayesian analysis and probabilistic programming.&lt;&#x2F;p&gt;
&lt;p&gt;Finally I wanted to thank two members of my team (Pablo Amoroso and Juan Bono) for helping me with the interview.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;1. Why a new statistics book?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Nearly all of the many excellent books on Bayesian statistics out now assume you are either familiar with statistics already or have a pretty solid foundation in programming. Because of this the current state of Bayesian statistics in often as an advanced alternative to classical (i.e. frequentist) statistics. So even though Bayesian statistics is gaining a lot of popularity, it’s mostly amount people who already have a quantitative background.&lt;&#x2F;p&gt;
&lt;p&gt;When someone wants to simply “learn statistics” they usually pick up an introduction based on frequentist statistics and end up half understanding a bunch of tests and rules, and feel very confused by the subject. I wanted to write a book on Bayesian statistics that really anyone could pick up and use to gain real intuitions for how to think statistically and solve real problems using statistics. For me there’s no reason why Bayesian statistics can’t be a beginners first introduction to statistics.&lt;&#x2F;p&gt;
&lt;p&gt;I would love it if, one day, when people said “statistics” it implied Bayesian statistics and frequentist statistics was just an academic niche. To get there we need more books that introduce statistics to a wide audience using Bayesian methods and assume this may be the readers first exposure to stats. I toyed with the idea of just calling this book “Statistics the Fun Way”, but I know I would probably get angry emails from people buying the book help with stats 101 and getting very confused! Hopefully this book will be a small step in getting “stat 101” to be taught from the Bayesian perspective, and statistics can make sense from the beginning.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. Who is your intended audience for the book? Could anyone without a math background pick it up?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;My goal with Bayesian Statistics the Fun Way was to create a book that basically anyone with a high school math background could pick up and read. Even if you only vaguely remember algebra, the book moves at a pace that should be easy to follow. Bayesian statistics does require just a little calculus and is a lot easier with a bit of code, so I’ve included two appendices that cover enough R to work as an advanced calculator and enough background in the ideas of calculus that when the book needs talk about integrals you can understand. But I promise that there is no solving of any calculus problems required.&lt;&#x2F;p&gt;
&lt;p&gt;While I worked hard to limit the mathematical prerequisites for the book, as you read through the book you should start picking up on mathematical ways of thinking. If you really understand the math your using, you can make better use of it. So I don’t try to shy away from any of the real math, but rather work up to it slowly so that all the math seems obvious as you develop your understanding. Like many people, I used to believe that math was confusing and difficult to work with. In time I really saw that when math is done right, it should be almost obvious. Confusion in mathematics is usually just the result of moving too quickly, or leaving out important steps in the reasoning.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;3. Why should software developers learn probability and statistics?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I really believe everyone should learn some probability and statistics because it really does help to reason about the uncertain world which is our everyday life. For software developers in particular there are few common places that its useful to understand statistics. It’s pretty likely that at some point in your career in software, you’ll need to write code that makes some decision based on some uncertain measurement. Maybe it’s measuring the conversion rate on a web page, generating some random reward in a game, assigning users to groups randomly or even reading information from an uncertain sensor. In all these cases really understanding probability will be very helpful. In the software part of my career I’ve also found that probability can help a lot in troubleshooting bugs that are difficult to reproduce or to trace back to a complex problem. If a bug appears to be caused by insufficient memory, does adding more memory decrease the probability of the bug in a meaningful way? If there are two explanations for a complex bug, which should be investigated first? In all these cases probability can help. And of course with the rise of Machine Learning and Data Science, engineers are more and more likely to be working on software problems that involve working directly with probabilities.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;4. Could you give a brief summary of the difference between the frequentist and bayesian approaches to probability?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Frequentists interpret probability as a statement about how frequently an event should occur in repeated trials. So if we toss a coin twice we should expect to get 1 head because the frequency of heads is 1&#x2F;2. Bayesians interpret probability as a statement of our knowledge, basically as a continuous version of logic. The probability of getting heads in a coin toss is 0.5 because I don’t believe getting heads is any more likely than getting tails. For coin tosses both schools of thought work pretty well. But when you talk about things like the probability that your favorite football team will win the world cup, talking about degrees of belief makes a lot more sense. This additionally means that Bayesian statistics does not make statements about the world but about our understanding of the world. And since we each understand the world a bit differently, Bayesian statistics allows us to incorporate that difference into our analysis. Bayesian analysis is, in many ways, the science of changing your mind.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;5. Why did you choose to focus on the bayesian approach?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;There are plenty of really great philosophical reasons to focus on Bayesian statistics but for me there is a very practical reason: everything makes sense. From a small set of relatively intuitive rules you can build out the solutions to any problem you encounter. This gives Bayesian statistics a lot of power and flexibility, and also makes it much easier to learn. I think this is something programmers will really like about Bayesian reasoning. You aren’t applying ad hoc tests to a problem, but reasoning about your problem and coming up with a solution that makes sense. Bayesian statistics is really reasoning. You agree to the statistical analysis only when it genuinely makes sense and convinces you, not because some seemingly arbitrary test result achieves some equally arbitrary value. Bayesian statistics also allows us to disagree quantitatively. It’s quite common in everyday life that two people will see the same evidence and come to different conclusions. Bayesian statistics allows us to model this disagreement in a formal way so that we can see what evidence it would take to change our beliefs. You shouldn’t believe the results of a paper because of a p-value, you should believe them because the truly convince you.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;6. How Bayesian Statistics Is Related To Machine Learning&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;One way I’ve been thinking about the relationship between Bayesian Statistics and Machine Learning (especially neural networks) in the way that each deal with the fact that calculus can get really, really hard. Machine Learning is essentially understanding and solving really tricky derivatives. You come up with a function and a loss for it, then compute (automatically) the derivative and try to follow it until you get optimal parameters. People often snarkily remark that backpropagation is “just the chain rule”, but nearly all the really hard work in deep learning is applying that successfully.&lt;&#x2F;p&gt;
&lt;p&gt;Bayesian statistics is the other part of calculus, solving really tricky integrals. The Stan developer Michael Betancourt made a great comment that basically all Bayesian analysis is really computing expectations, which is solving integrals. Bayesian analysis leaves you with a posterior distribution but you can’t use a distribution for anything unless you integrate over it to get a concrete answer. Thankfully no one makes snarky comments about integrals because everyone knows that it can be really tricky in the simplest case. This xkcd makes that point nicely:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-vska22BJzFePmtcrzokZ0A.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;So in this strange way the current state Machine Learning and Bayesian Statistics are what happens when you push basic calculus ideas to the limits of what we can compute.&lt;&#x2F;p&gt;
&lt;p&gt;This relationship also outlines the key differences. When you think about derivatives you’re looking for a specific point related to a function. If you know location and time, the derivative is speed and can tell you when you went the fastest. Moving the needle in ML is getting a single metric better than anyone else. Integration is about summarizing an entire process. Again if you know location and time, the integral is distance and tells you how far you’ve traveled. Bayesian statistics is about summarizing all of your knowledge about a problem, but this allows us to not just give single predictions but also say how confident we are in a wide range of predictions. Advancement in Bayesian statistics is about understanding more complex systems of information.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;7. If your readers wanted to dig deeper into the subject of the book, where would you point them to (books, courses, blog posts, etc)?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The biggest inspiration for this book was E.T. Jaynes’ “Probability Theory: the Logic of Science”. My secret hope is that “Bayesian Statistics the Fun Way” can be a version of that book accessible to everyone. Jaynes’ book is really quite challenging to work through and is presents a pretty radical version of Bayesian statistics. Aubrey Clayton has done an amazing service by putting together a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rfKS69cIwHc&quot;&gt;series of lectures&lt;&#x2F;a&gt; on the key chapters of this book.&lt;&#x2F;p&gt;
&lt;p&gt;And of course if you liked reading the book you’d probably enjoy my &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.countbayesie.com&#x2F;&quot;&gt;blog&lt;&#x2F;a&gt;. I haven’t been posting much recently since I’ve been writing a “Bayesian Statistics the Fun Way” and before that “Get Programming with Haskell” but I’ve got a ton of posts in my head that I really want to get down on paper soon. Generally the blog, despite the name, is not strictly Bayesian. Typically if I have some statistics&#x2F;probability topic that I’m thinking about, it will get fleshed out into a blog post.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;8. In your experience, what is a concept from probability&#x2F;statistics that non experts find difficult to understand?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Honestly, the hardest part is interpreting probabilities. People really lost faith in a lot of Bayesian analysts like Nate Silver (and many others) when they were predicting 80% or so chance that Clinton would win the 2016 election and she didn’t. People felt like they had been tricked and everyone was wrong, but 80% chance really isn’t that high. If my doctor tells me I have an 80% chance to live I’m going to be really nervous.&lt;&#x2F;p&gt;
&lt;p&gt;A common approach to this problem is to point to probabilities themselves and say that they are a poor way to express uncertainty. The fix then is that you should be using odds or likelihood ratios or some decibel-like system similar to Jaynes’s idea of evidence. But after really thinking about probability for along time I haven’t found that there’s a universally good way to express uncertainty.&lt;&#x2F;p&gt;
&lt;p&gt;The heart of the problem is that, deep down, we really want to believe that the world is certain. Even among experienced probabilists there’s this persistent nagging feeling that maybe if you do the right analysis, learn the right prior, add another layer into your hierarchical model you can get it right and remove or at least dramatically reduce uncertainty. Part of what draws me to probability is the weird mixture of trying to make sense of the world and the mediation on the fact that even when trying your hardest, the world will surprise you.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;9. What are your thoughts on p-values as a measure of statistical significance? Could you give us a brief description of p-hacking?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;There’s two things wrong with p-values. First of all, p-values are not the way sane people answer questions. Imagine how this conversation would sound at work:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Manager: “Did you fix that bug assigned to you?”&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;You: “Well I’m pretty sure I didn’t not fix it…”&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Manager: “If you fixed it, just mark it fixed.”&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;You: “Oh no, I really can’t say that I fixed it…”&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Manager: “So you want to mark it ‘will not fix’?”&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;You: “No, no, I’m pretty sure that’s not the case”&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;p-values confuse people because they are, quite literally, confusing. Bayesian statistics gives you a posterior probability, which is exactly the positive answer to the question being posed that you want. In the previous dialog the Bayesian says “I’m pretty sure it’s fixed”, if the manager wants you to be more sure, you collect more data and then you can say “I’m basically certain it’s fixed”.&lt;&#x2F;p&gt;
&lt;p&gt;The second problem is the culture of arbitrarily picking 0.05 as some magic value that has meaning. Related to the previous question about understanding probabilities, a 5% chance of something occuring does not make it very rare. Rolling a 20 sided die and getting a 20 has a 5% chance, and anyone who knows of Dungeons and Dragons (D&amp;amp;D) knows that this is far from impossible. Outside of role playing games, focusing on a die roll is not a great system of verifying true from false.&lt;&#x2F;p&gt;
&lt;p&gt;And that brings us to p-hacking. Imagine you’re playing D&amp;amp;D with some friends and you role twenty 20-sided dice all at one. You then point one that landed on 20 and proclaim “that was the die I meant to roll, the rest are all just test dice.” It’s still cheating even if you technically did roll a 20. That’s what p-hacking essentially is. You keep doing analysis until you find something that is ‘significant’, and then claim that’s what you were looking for the entire time.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;10. Any closing recommendations on what book to read next after reading your book?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Now that I’ve finished writing this book I finally have time to start catching up on other books that I didn’t have time to read while writing it! I’m really enjoying Osvaldo Martin’s “Bayesian Analysis with Python” (I know Not Monad Tutorial interviewed him not long ago). It’s a great book that approaches Bayesian analysis through PyMC3. I really think the world of probabilistic programming is very exciting and will be more and more an essential part of practical Bayesian statistics. Another book I really want to read is Richard McElreath’s “Statistical Rethinking”. It has a second edition coming out soon so I’m slightly hesitant to get copy before that. McElreath has put up a bunch of great supporting material on his &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;xcelab.net&#x2F;rm&#x2F;statistical-rethinking&#x2F;&quot;&gt;website&lt;&#x2F;a&gt;, so I might not be able to wait until the 2nd edition to get a copy. Both of these sources would be great next steps following “Bayesian Statistics the Fun Way”. Another good recommendations would be Kruschke’s “Doing Bayesian Data Analysis”.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
