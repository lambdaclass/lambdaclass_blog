<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>LambdaClass Blog - zero knowledge proofs</title>
    <subtitle>Deep technical insights on cryptography, distributed systems, zero-knowledge proofs, and cutting-edge software engineering from the LambdaClass team.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://blog.lambdaclass.com/tags/zero-knowledge-proofs/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2024-02-17T00:00:00+00:00</updated>
    <id>https://blog.lambdaclass.com/tags/zero-knowledge-proofs/atom.xml</id>
    <entry xml:lang="en">
        <title>Our highly subjective view on the history of Zero-Knowledge Proofs</title>
        <published>2024-02-17T00:00:00+00:00</published>
        <updated>2024-02-17T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/our-highly-subjective-view-on-the-history-of-zero-knowledge-proofs/"/>
        <id>https://blog.lambdaclass.com/posts/our-highly-subjective-view-on-the-history-of-zero-knowledge-proofs/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/our-highly-subjective-view-on-the-history-of-zero-knowledge-proofs/">&lt;p&gt;Zero-knowledge, Succinct, Non-interactive ARguments of Knowledge (zk-SNARKs) are powerful cryptographic primitives that allow one party, the prover, to convince another party, the verifier, that a given statement is true without revealing anything else other than the validity of the statement. They have gained widespread attention due to their applications in verifiable private computation, providing proof of the correctness of the execution of computer programs and helping scale blockchains. We think SNARKs will have a significant impact in shaping our world, as we describe in our &lt;a href=&quot;&#x2F;transforming-the-future-with-zero-knowledge-proofs-fully-homomorphic-encryption-and-new-distributed-systems-algorithms&#x2F;&quot;&gt;post&lt;&#x2F;a&gt;. SNARKs acts as an umbrella for different types of proof systems, using different polynomial commitment schemes (PCS), arithmetization schemes, interactive oracle proofs (IOP) or probabilistically checkable proofs (PCP). However, the basic ideas and concepts date back to the mid-1980’s. The development significantly accelerated after the introduction of Bitcoin and Ethereum, which proved to be an exciting and powerful use case since you can scale them by using Zero-Knowledge proofs (generally called Validity Proofs for this particular usecase). SNARKs are an essential tool for blockchain scalability. As Ben-Sasson describes, the last years have seen a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;starkware&#x2F;cambrian-explosion-of-cryptographic-proofs-5740a41cdbd2&quot;&gt;cambrian explosion of cryptographic proofs&lt;&#x2F;a&gt;. Each proof system offers advantages and disadvantages and was designed with certain tradeoffs in mind. Advances in hardware, better algorithms, new arguments, and gadgets result in enhanced performance and the birth of new systems. Many of them are used in production, and we keep pushing the boundaries. Will we have a general proof system for all applications or several systems suited for different needs? We think that it is unlikely that one proof system will rule them all because:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. The diversity of applications.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. The types of constraints we have (regarding memory, verification times, proving times).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. The need for robustness (if one proof system gets broken, we still have others).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Even if proof systems change a lot, they all offer a significant property: proofs can be verified quickly. Having a layer that verifies proofs and can be easily adapted to handle new proof systems solves the difficulties associated with changing the base layer, such as Ethereum. To give an overview of the different characteristics of SNARKs:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Cryptographic assumptions: collision-resistant hash functions, discrete log problem over elliptic curves, knowledge of exponent.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Transparent vs trusted setup.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Prover time: linear vs superlinear.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Verifier time: constant time, logarithmic, sublinear, linear.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Proof size.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Ease of recursion.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Arithmetization scheme.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Univariate vs multivariate polynomials.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This post will look into the origins of SNARKs, some fundamental building blocks, and the rise (and fall) of different proof systems. The post does not intend to be an exhaustive analysis of proof systems. We focus instead on those that had an impact on us. Of course, these developments were only possible with the great work and ideas of the pioneers of this field.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;fundamentals&quot;&gt;Fundamentals&lt;&#x2F;h2&gt;
&lt;p&gt;As we mentioned, zero-knowledge proofs are not new. The definitions, foundations, important theorems, and even important protocols were established from mid-1980s. Some of the key ideas and protocols that we use to build modern SNARKs were proposed in 1990s (the sumcheck protocol) or even before the advent of Bitcoin (GKR in 2007). The main problems with its adoption were related to the lack of a powerful usecase (internet was not as developed in the 1990s), and the amount of computational power needed.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zero-knowledge-proofs-the-origins-1985-1989&quot;&gt;Zero-knowledge proofs: the origins (1985&#x2F;1989)&lt;&#x2F;h3&gt;
&lt;p&gt;The field of zero-knowledge proofs made its appearance in academic literature with the paper by &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;people.csail.mit.edu&#x2F;silvio&#x2F;Selected%20Scientific%20Papers&#x2F;Proof%20Systems&#x2F;The_Knowledge_Complexity_Of_Interactive_Proof_Systems.pdf&quot;&gt;Goldwasser, Micali and Rackoff&lt;&#x2F;a&gt;. For a discussion on the origins, you can see the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=uchjTIlPzFo&quot;&gt;following video&lt;&#x2F;a&gt;. The paper introduced the notions of completeness, soundness, and zero-knowledge, providing constructions for quadratic residuosity and quadratic non-residuosity.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;sumcheck-protocol-1992&quot;&gt;Sumcheck protocol (1992)&lt;&#x2F;h3&gt;
&lt;p&gt;The &lt;a href=&quot;&#x2F;have-you-checked-your-sums&#x2F;&quot;&gt;sumcheck protocol&lt;&#x2F;a&gt; was proposed by &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;146585.146605&quot;&gt;Lund, Fortnow, Karloff, and Nisan&lt;&#x2F;a&gt; in 1992. It is one of the most important building blocks for succinct interactive proofs. It helps us reduce a claim over the sum of a multivariate polynomial’s evaluations to a single evaluation at a randomly chosen point.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;goldwasser-kalai-rothblum-gkr-2007&quot;&gt;Goldwasser-Kalai-Rothblum (GKR) (2007)&lt;&#x2F;h3&gt;
&lt;p&gt;The &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;wp-content&#x2F;uploads&#x2F;2016&#x2F;12&#x2F;2008-DelegatingComputation.pdf&quot;&gt;GKR protocol&lt;&#x2F;a&gt; is an interactive protocol that has a prover that runs linearly in the number of gates of a circuit, while the verifier runs sublinearly in the size of the circuit. In the protocol, the prover and verifier agree on an arithmetic circuit of fan-in-two over a finite field of depth $d$, with layer $d$ corresponding to the input layer and layer $0$ being the output layer. The protocol starts with a claim regarding the output of the circuit, which is reduced to a claim over the values of the previous layer. Using recursion, we can turn this into a claim over the circuit’s inputs, which can be checked easily. These reductions are achieved via the sumcheck protocol.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;kzg-polynomial-commitment-scheme-2010&quot;&gt;KZG polynomial commitment scheme (2010)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iacr.org&#x2F;archive&#x2F;asiacrypt2010&#x2F;6477178&#x2F;6477178.pdf&quot;&gt;Kate, Zaverucha, and Goldberg&lt;&#x2F;a&gt; introduced in 2010 a commitment scheme for polynomials using a bilinear pairing group. The commitment consists of a single group element, and the committer can efficiently open the commitment to any correct evaluation of the polynomial. Moreover, due to batching techniques, the opening can be done to several evaluations. KZG commitments provided one of the basic building blocks for several efficient SNARKs, such as Pinocchio, Groth16, and Plonk. It is also at the heart of the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;ethereum&#x2F;EIPs&#x2F;blob&#x2F;master&#x2F;EIPS&#x2F;eip-4844.md&quot;&gt;EIP-4844&lt;&#x2F;a&gt;. To get an intuition on batching techniques, you can see our post on the &lt;a href=&quot;&#x2F;mina-to-ethereum-bridge&#x2F;&quot;&gt;Mina-Ethereum bridge&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;practical-snarks-using-elliptic-curves&quot;&gt;Practical SNARKs using elliptic curves&lt;&#x2F;h2&gt;
&lt;p&gt;The first practical constructions for SNARKs appeared in 2013. These required a preprocessing step to generate the proving and verifying keys, and were program&#x2F;circuit specific. These keys could be quite large, and depended on secret parameters which should remain unknown to the parties; otherwise, they could forge proofs. Transforming code into something that could be proven required compiling the code to a system of polynomial constraints. At first, this had to be done in a manual way, which is time-consuming and error-prone. The advances in this area tried to remove some of the main problems:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Have more efficient provers.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Reduce the amount of preprocessing.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Having universal rather than circuit specific setups.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. Avoid having trusted setups.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    5. Developing ways to describe circuits using a high-level language, instead of writing the polynomial constraints manually.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h3 id=&quot;pinocchio-2013&quot;&gt;Pinocchio (2013)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2013&#x2F;279&quot;&gt;Pinocchio&lt;&#x2F;a&gt; is the first practical, usable zk-SNARK. The SNARK is based on quadratic arithmetic programs (QAP). The proof size was originally 288 bytes. Pinocchio’s toolchain provided a compiler from C code to arithmetic circuits, which was further transformed into a QAP. The protocol required that the verifier generate the keys, which are circuit-specific. It used elliptic curve pairings to check the equations. The asymptotics for proof generation and key setup were linear in the computation size, and the verification time was linear in the size of the public inputs and outputs.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;groth-16-2016&quot;&gt;Groth 16 (2016)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2016&#x2F;260.pdf&quot;&gt;Groth&lt;&#x2F;a&gt; introduced a &lt;a href=&quot;&#x2F;groth16&#x2F;&quot;&gt;new argument of knowledge with increased performance&lt;&#x2F;a&gt; for problems described by an R1CS. It has the smallest proof size (only three group elements) and fast verification involving three pairings. It also involves a preprocessing step to obtain the structured reference string. The main drawback is that it requires a different trusted setup per program that we want to prove, which is inconvenient. Groth16 was used in ZCash.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;bulletproofs-ipa-2016&quot;&gt;Bulletproofs &amp;amp; IPA (2016)&lt;&#x2F;h3&gt;
&lt;p&gt;One of the weak points of the KZG PCS is that it requires a trusted setup. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2016&#x2F;263&quot;&gt;Bootle et al.&lt;&#x2F;a&gt; introduced an efficient zero-knowledge argument system of openings of Pedersen commitments that satisfy an inner product relation. The inner product argument has a linear prover, with logarithmic communication and interaction, but with linear time verification. They also developed a polynomial commitment scheme that does not require a trusted setup. PCS using these ideas are used by Halo 2 and Kimchi.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;sonic-marlin-and-plonk-2019&quot;&gt;Sonic, Marlin, and Plonk (2019)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;099&quot;&gt;Sonic&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;953&quot;&gt;Plonk&lt;&#x2F;a&gt;, and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;1047&quot;&gt;Marlin&lt;&#x2F;a&gt; solve the problem of the trusted setup per program that we had in Groth16, by introducing universal and updatable structured reference strings. Marlin provides a proof system based on R1CS and is at the core of Aleo.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;all-you-wanted-to-know-about-plonk&#x2F;&quot;&gt;Plonk&lt;&#x2F;a&gt; introduced a new arithmetization scheme (later called Plonkish) and the use of the grand-product check for the copy constraints. Plonkish also allowed the introduction of specialized gates for certain operations, the so-called custom gates. Several projects have customized versions of Plonk, including Aztec, zkSync, Polygon ZKEVM, Mina’s Kimchi, Plonky2, Halo 2, and Scroll, among others.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;lookups-2018-2020&quot;&gt;Lookups (2018&#x2F;2020)&lt;&#x2F;h3&gt;
&lt;p&gt;Gabizon and Williamson introduced &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2020&#x2F;315&quot;&gt;plookup&lt;&#x2F;a&gt; in 2020, using the grand product check to prove that a value is included in a precomputed value table. Though lookup arguments were previously presented in &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2018&#x2F;380&quot;&gt;Arya&lt;&#x2F;a&gt;, the construction required the determination of the multiplicities for the lookups, which makes the construction less efficient. The &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;086&quot;&gt;PlonkUp&lt;&#x2F;a&gt; paper showed how to introduce the plookup argument into Plonk. The problem with these lookup arguments was that they forced the prover to pay the price for the whole table, independently of his number of lookups. This implies a considerable cost for large tables, and a lot of effort has been devoted to reducing the cost of the prover to just the number of lookups he uses.&lt;br &#x2F;&gt;
Haböck introduced &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1530&quot;&gt;LogUp&lt;&#x2F;a&gt;, which uses the logarithmic derivative to turn the grand-product check into a sum of reciprocals. LogUp is crucial for performance in the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;toposware.medium.com&#x2F;beyond-limits-pushing-the-boundaries-of-zk-evm-9dd0c5ec9fca&quot;&gt;Polygon ZKEVM&lt;&#x2F;a&gt;, where they need to split the whole table into several STARK modules. These modules have to be linked correctly, and cross-table lookups enforce this. The introduction of &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2023&#x2F;1284&quot;&gt;LogUp-GKR&lt;&#x2F;a&gt; uses the GKR protocol to increase the performance of LogUp. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;621&quot;&gt;Caulk&lt;&#x2F;a&gt; was the first scheme with prover time sublinear in the table size by using preprocessing time $\mathcal{O}(N \log N)$ and storage $\mathcal{O}(N)$, where $N$ is the table size. Several other schemes followed, such as &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1565&quot;&gt;Baloo&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1447&quot;&gt;flookup&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1763&quot;&gt;cq&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;957&quot;&gt;caulk+&lt;&#x2F;a&gt;. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2023&#x2F;1216&quot;&gt;Lasso&lt;&#x2F;a&gt; presents several improvements, avoiding committing to the table if it has a given structure. Besides, Lasso’s prover only pays for table entries accessed by the lookup operations. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2023&#x2F;1217&quot;&gt;Jolt&lt;&#x2F;a&gt; leverages Lasso to prove the execution of a virtual machine via lookups&lt;&#x2F;p&gt;
&lt;h3 id=&quot;spartan-2019&quot;&gt;Spartan (2019)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;550&quot;&gt;Spartan&lt;&#x2F;a&gt; provides an IOP for circuits described using R1CS, leveraging the properties of multivariate polynomials and the sumcheck protocol. Using a suitable polynomial commitment scheme, it results in a transparent SNARK with a linear time prover.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;hyperplonk-2022&quot;&gt;HyperPlonk (2022)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1355.pdf&quot;&gt;HyperPlonk&lt;&#x2F;a&gt; builds on the ideas of Plonk using multivariate polynomials. Instead of quotients to check the constraints’ enforcement, it relies on the sumcheck protocol. It also supports constraints of a high degree without harming the running time of the prover. Since it relies on multivariate polynomials, there is no need to carry out FFTs, and the prover’s running time is linear in the circuit size. HyperPlonk introduces a new permutation IOP suitable for smaller fields and a sum check-based batch opening protocol, which reduces the prover’s work, proof size, and the verifier’s time.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;folding-schemes-2008-2021&quot;&gt;Folding schemes (2008&#x2F;2021)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2021&#x2F;370&quot;&gt;Nova&lt;&#x2F;a&gt; introduces the idea of a folding scheme, which is a new approach to achieve incrementally verifiable computation (IVC). The concept of IVC dates back to &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;https:&#x2F;&#x2F;iacr.org&#x2F;archive&#x2F;tcc2008&#x2F;49480001&#x2F;49480001.pdf&quot;&gt;Valiant&lt;&#x2F;a&gt; who showed how to merge two proofs of length $k$ into a single proof of length $k$. The idea is that we can prove any long-running computation by recursively proving that the execution from step $i$ to step $ I + 1$ is correct and verifying a proof that shows that the transition from step $i - 1$ to step $i$ was correct. Nova deals well with uniform computations; it was later extended to handle different types of circuits with the introduction of &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1758&quot;&gt;Supernova&lt;&#x2F;a&gt;. Nova uses a relaxed version of R1CS and works over amicable elliptic curves. Working with amicable cycles of curves (for example, the Pasta curves) to achieve IVC is also used in Pickles, Mina’s main building block to achieve a succinct state. However, the idea of folding differs from recursive SNARK verification. The accumulator idea is more deeply connected to the concept of batching proofs. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;1021.pdf&quot;&gt;Halo&lt;&#x2F;a&gt; introduced the notion of accumulation as an alternative to recursive proof composition. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2023&#x2F;620&quot;&gt;Protostar&lt;&#x2F;a&gt; provides a non-uniform IVC scheme for Plonk that supports high-degree gates and vector lookups.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;using-collision-resistant-hash-functions&quot;&gt;Using collision-resistant hash functions&lt;&#x2F;h2&gt;
&lt;p&gt;Around the same time that Pinocchio was developed, there were some ideas to generate circuits&#x2F;arithmetization schemes that could prove the correctness of the execution of a virtual machine. Even though developing the arithmetization of a virtual machine could be more complex or less efficient than writing dedicated circuits for some programs, it offered the advantage that any program, no matter how complicated, could be proven by showing that it was executed correctly in the virtual machine. The ideas in TinyRAM were later improved with the design of the Cairo vm, and subsequent virtual machines (such as zk-evms or general purpose zkvms). The use of collision-resistant hash functions removed the need for trusted setups or use of elliptic curve operations, at the expense of longer proofs.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;tinyram-2013&quot;&gt;TinyRAM (2013)&lt;&#x2F;h3&gt;
&lt;p&gt;In &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2013&#x2F;507&quot;&gt;SNARKs for C&lt;&#x2F;a&gt;, they developed a SNARK based on a PCP to prove the correctness of the execution of a C program, which is compiled to TinyRAM, a reduced instruction set computer. The computer used a Harvard architecture with byte-level addressable random-access memory. Leveraging nondeterminism, the circuit’s size is quasilinear in the size of the computation, efficiently handling arbitrary and data-dependent loops, control flow, and memory accesses.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;starks-2018&quot;&gt;STARKs (2018)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2018&#x2F;046&quot;&gt;STARKs&lt;&#x2F;a&gt; were introduced by Ben Sasson et al. in 2018. They achieve $\mathcal{O}(\log^2 n )$ proof sizes, with fast prover and verifier, do not require a trusted setup, and are conjectured to be post-quantum secure. They were first used by Starkware&#x2F;Starknet, together with the Cairo vm. Among its key introductions are the algebraic intermediate representation (AIR) and the &lt;a href=&quot;&#x2F;how-to-code-fri-from-scratch&#x2F;&quot;&gt;FRI protocol&lt;&#x2F;a&gt; (Fast Reed-Solomon Interactive Oracle Proof of Proximity). It is also used by other projects (Polygon Miden, Risc0, Winterfell, Neptune) or has seen adaptations of some components (zkSync’s Boojum, Plonky2, Starky).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ligero-2017&quot;&gt;Ligero (2017)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1608&quot;&gt;Ligero&lt;&#x2F;a&gt; introduces a proof system that achieves proofs whose size is $\mathcal{O}(\sqrt{n})$, where $n$ is the size of the circuit. It arranges the polynomial coefficients in matrix form and uses linear codes.&lt;br &#x2F;&gt;
&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2021&#x2F;1043&quot;&gt;Brakedown&lt;&#x2F;a&gt; builds on Ligero and introduces the idea of field-agnostic polynomial commitment schemes.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;some-new-developments&quot;&gt;Some new developments&lt;&#x2F;h2&gt;
&lt;p&gt;The use of different proof systems in production showed the merits of each of the approaches, and led to new developments. For example, plonkish arithmetization offers a simple way to include custom gates and lookup arguments; FRI has shown great performance as PCS, leading to Plonky. Similarly, the use of the grand product check in AIR (leading to randomized AIR with preprocessing) improved its performance and simplified memory access arguments. Commitments based on hash functions have gained popularity, based on the speed of hash functions in hardware or the introduction of new SNARK-friendly hash functions.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;new-polynomial-commitment-schemes-2023&quot;&gt;New polynomial commitment schemes (2023)&lt;&#x2F;h3&gt;
&lt;p&gt;With the advent of efficient SNARKs based on multivariate polynomials, such as Spartan or HyperPlonk, there has been an increased interest in new commitment schemes suited for this kind of polynomials. &lt;a href=&quot;&#x2F;snarks-on-binary-fields-binius&#x2F;&quot;&gt;Binius&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2023&#x2F;917&quot;&gt;Zeromorph&lt;&#x2F;a&gt;, and &lt;a href=&quot;&#x2F;how-does-basefold-polynomial-commitment-scheme-generalize-fri&#x2F;&quot;&gt;Basefold&lt;&#x2F;a&gt; all propose new forms to commit to multilinear polynomials. Binius offers the advantage of having zero overhead to represent data types (whereas many proof systems use at least 32-bit field elements to represent single bits) and works over binary fields. The commitment adapts brakedown, which was designed to be field agnostic. Basefold generalizes FRI to codes other than Reed-Solomon, leading to a field-agnostic PCS.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;customizable-constraint-systems-2023&quot;&gt;Customizable Constraint Systems (2023)&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2023&#x2F;552&quot;&gt;CCS&lt;&#x2F;a&gt; generalizes R1CS while capturing R1CS, Plonkish, and AIR arithmetization without overheads. Using CCS with Spartan IOP yields SuperSpartan, which supports high-degree constraints without having the prover to incur cryptographic costs that scale with the degree of the constraint. In particular, SuperSpartan yields a SNARK for AIR with a linear time prover.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h2&gt;
&lt;p&gt;This post describes the advances of SNARKs since their introduction in the mid-1980s. Advances in computer science, mathematics, and hardware, together with the introduction of blockchain, have led to new and more efficient SNARKs, opening the door for many applications that could transform our society. Researchers and engineers have proposed improvements and adaptations to SNARKs according to their needs, focusing on proof size, memory use, transparent setup, post-quantum security, prover time, and verifier time. While there were originally two main lines (SNARKs vs STARKs), the boundary between both has begun to fade, trying to combine the advantages of the different proof systems. For example, combining different arithmetization schemes with new polynomial commitment schemes. We can expect that new proof systems will continue to rise, with increased performance, and it will be hard for some systems that require some time to adapt to keep up with these developments unless we can easily use these tools without having to change some core infrastructure.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>A fast trust minimized intent based bridge solution for Ethereum and L2s powered by multi-proof storage proofs</title>
        <published>2024-01-28T00:00:00+00:00</published>
        <updated>2024-01-28T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/a-fast-trust-minimized-intent-based-bridge-solution-for-ethereum-and-l2s-powered-by-multi-proof-storage-proofs/"/>
        <id>https://blog.lambdaclass.com/posts/a-fast-trust-minimized-intent-based-bridge-solution-for-ethereum-and-l2s-powered-by-multi-proof-storage-proofs/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/a-fast-trust-minimized-intent-based-bridge-solution-for-ethereum-and-l2s-powered-by-multi-proof-storage-proofs/">&lt;p&gt;&lt;strong&gt;Authors:&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;thisisrj&quot;&gt;Roberto Catalan&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;federicocarrone&quot;&gt;Federico Carrone&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Bridges are generally insecure and economically inefficient. They exhibit an asymmetry between users and bridge operators, where users can easily lose funds. We propose a bridge design that is simple, modular, and utilizes multi-storage proofs and the native messaging system between Ethereum and Layer 2 networks (L2s) as a fallback mechanism.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bridging-is-a-trust-issue&quot;&gt;Bridging is a trust issue&lt;&#x2F;h2&gt;
&lt;p&gt;How can we offer a system where the users don’t have to trust a facilitator to exchange their assets from an L2 to Ethereum?&lt;&#x2F;p&gt;
&lt;p&gt;We propose a simple protocol that follows these steps:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. The user specifies a destination address on Ethereum and locks the tokens X to be bridged into an L2 escrow smart contract.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. A market maker monitors a change of state in the escrow smart contract.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. a. The market maker calls the transfer function of the PaymentRegistry Contract in Ethereum.  &lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;b. The transfer function of the PaymentRegistry contract in Ethereum pays the tokens X to the User.
4. A storage proof is generated, containing evidence of a transfer from the market maker’s Ethereum account to the user-specified address in Ethereum.
5. Ethereum PaymentRegistry storage information is used as part of a storage proof.
6. L2 Escrow contract verifies the storage proof of the PaymentRegistry contract in Ethereum and pays the MM with the initial tokens locked by the user.
&lt;img src=&quot;&#x2F;images&#x2F;2024&#x2F;01&#x2F;image.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The same design can be expanded to be used to bridge tokens from an L2 to another L2. The same design can include multi-proof storage proofs instead of using only one. We also have implemented a fallback mechanism using the native message mechanism between Ethereum and L2s in case the storage proof providers are offline.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Fallback mechanism&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
If the storage proof providers are not available, the market maker can prove to the Escrow contract that they fulfilled the user’s intent through the rollup’s native messaging system. Using this messaging system has the same trust assumptions as the L2s used in the transfer.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;risks&quot;&gt;Risks&lt;&#x2F;h2&gt;
&lt;p&gt;For the user, the risks include the existence of a bug in the code of the smart contract, the existence of a bug in the circuits of the ZK&#x2F;validity proof verification and the fact that the storage proof provider can go offline. The first risk is mitigated by having a very simple smart contract. The second risk is mitigated by using multi-proof storage proofs and multiple ZK&#x2F;validity proof implementations or TEEs. If the storage proof provider goes offline the fallback mechanism can be used.&lt;&#x2F;p&gt;
&lt;p&gt;The risks for market makers are the same as for users, plus the risk of reorganization of the chain and the fact that the market maker receives the same tokens on the L2s rather than on Ethereum.&lt;&#x2F;p&gt;
&lt;p&gt;Since the capital is locked for a short period (until the proof is generated or the message arrives), the risks are minimized and the attack surface is smaller for the market maker.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;What are our disadvantages?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
The biggest disadvantage of this solution is that users can only bridge tokens that are present in both the origin and destination chains.&lt;br &#x2F;&gt;
Another disadvantage is that the risks don’t disappear; they are simply transferred to the market maker.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How can users cancel their order?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Initially, we are not going to offer the ability to cancel orders. The main reason is to avoid any timing attacks. For instance, a user could create an order and cancel it right after the market maker has paid them on the destination chain, thereby stealing funds from the market maker.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Is there any real-world implementation of this bridge?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Yes. We have already implemented this between Starknet and Ethereum. We plan to integrate zkSync, Arbitrum, Optimism, Scroll, Base, and Linea next.&lt;br &#x2F;&gt;
All integrations require the same codebase with a few modifications, except for Starknet, which is not EVM compatible.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How fast is it?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
From the user’s perspective, the bridging is completed in less than 30 seconds, as quickly as the time it takes the market maker to observe the user’s deposit and execute a transfer.&lt;br &#x2F;&gt;
From the market maker’s perspective, they will be able to withdraw the money after paying the user and generating the storage proof. This normally takes between 5 and 15 minutes. It’s important to also consider that the market maker will need to rebalance their liquidity using the native bridge and wait for the finality of the native bridge to rebalance their portfolio.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How cheap is it?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
The cost of this bridge is similar to an ERC20 transfer plus the cost of proving the state of the L1 and L2. This second cost tends towards zero since it’s amortized by multiple calls that use the same proof, and the proving cost is minimal compared to on-chain transfers.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What is new in this design? Didn’t storage proofs solve this problem already?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Storage proofs alone don’t fundamentally change the design of a traditional bridge. They merely enable a safer coordination mechanism.&lt;br &#x2F;&gt;
Locking the user’s capital first provides guarantees to the market maker that they will receive the funds in exchange for fulfilling the user’s intent.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Couldn’t you solve this problem without Storage Proofs? What do they add to the table?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Yes, Storage Proofs are not 100% necessary to solve this problem. But they are a key technological component for a future proof architecture. If we want this protocol to scale, storage proofs are the best way to do this. It will allow us to prove many orders together.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What are the benefits against an Optimistic Oracle?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Optimistic Oracles were a great solution before Storage Proofs were a feasible solution, their main disadvantages are:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Optimistic Oracles relay in Game Theory to work and it&amp;#39;s difficult to bootstrap an ecosystem to make them robust.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Codebases are complex&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The settlement period takes a few hours (depending on the solution) and end up creating inefficiencies for the market makers.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;On the other hand our protocol with the native messaging and storage proofs takes no longer than 15 minutes (between Ethereum and L2) to unlock the funds. The protocol codebase is no more than 500 lines of code and the risks are easy to understand by all the players and therefore easy to come up with ways to mitigate them.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Did anybody do something similar beforehand?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Hop Protocol was one of the first bridges to allow cross-chain swaps between rollups and Ethereum with an AMM-based design using multiple messaging systems (native and optimistic). The main issue lies in the capital inefficiency of an AMM model and the significant security risks of locking large amounts of capital in complex cross-chain communications.&lt;&#x2F;p&gt;
&lt;p&gt;Across was the first bridge to leverage intents for a faster bridge experience and lower capital costs per transaction. However, by using an Optimistic Oracle, it naturally has a challenging period that the market has to wait to get its funds back. To optimize some of the problems their settlement mechanism introduces, they offer financial products around their main bridge solution, such as Liquidity Pools that front the capital to the market makers.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What are our advantages?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Our bet is that zero-knowledge proofs will continue to improve, becoming faster and safer, thus enhancing our solution and allowing us to offer better prices by lowering risks and shortening the repayment period.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Fast and cheap bridging experience for the user&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Short capital lock-up period for the market maker&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Low on-chain complexity. The smart contracts in total are not larger than 300 lines of code.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;&#x2F;h2&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Speeding up message passing with EigenLayer to allow cross-chain swap settlements between L2s. The protocol should have the option to send faster messages between rollups and Ethereum with similar trust assumptions of the native messaging system.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Introducing Partially Filled Orders, offering cheaper but slower transfers with batching.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Intent-based DeFi Pooling.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Unified wallet abstraction across multiple L2s.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;</content>
        
    </entry>
    <entry xml:lang="en">
        <title>An overview of the Groth16 proof system</title>
        <published>2023-10-17T00:00:00+00:00</published>
        <updated>2023-10-17T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/groth16/"/>
        <id>https://blog.lambdaclass.com/posts/groth16/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/groth16/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;Over the last decade, SNARKs (succinct, non-interactive arguments of knowledge) and STARKs (scalable, transparent arguments of knowledge) have been gaining attention due to their applications in verifiable private computation and scalability of blockchains.&lt;&#x2F;p&gt;
&lt;p&gt;Groth introduced this &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2016&#x2F;260.pdf&quot;&gt;proof system&lt;&#x2F;a&gt; in 2016 and saw an early application in ZCash. The protocol relies on pairing-friendly elliptic curves, such as BN254, BLS12-381, and BLS12-377 (more later). Its proof size is among the smallest (consisting of only three elliptic curve elements) and fastest to verify. The main drawback is that it needs a trusted setup per program. In other words, we need to regenerate all the parameters whenever we want to prove a new program (or change the original one).&lt;&#x2F;p&gt;
&lt;p&gt;In this post, we will describe the main ingredients of Groth16 and how it works. As stated in the roadmap, we are implementing the protocol in the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;lambdaworks&quot;&gt;Lambdaworks library&lt;&#x2F;a&gt;. It is also used as a project in the ongoing &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;sparkling_water_bootcamp&#x2F;tree&#x2F;main&quot;&gt;Sparkling Water Bootcamp&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;arithmetization&quot;&gt;Arithmetization&lt;&#x2F;h2&gt;
&lt;p&gt;To prove the execution of a given program, we have to transform it to a SNARK (succinct, non-interactive argument of knowledge) friendly form. One of such forms is arithmetic circuit satisfiability, where one can prove knowledge of a valid circuit assignment. This first step, known as arithmetization, is the program’s transformation into an arithmetic circuit or equivalent form.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;r1cs&quot;&gt;R1CS&lt;&#x2F;h2&gt;
&lt;p&gt;Arithmetic circuits can be expressed equivalently as (quadratic) rank one constraint systems (R1CS), which are systems of equations of the form:&lt;br &#x2F;&gt;
$$(Az)\times (Bz) = Cz$$&lt;br &#x2F;&gt;
where $A, B, C$ are matrices of size $m + 1$ rows by $n + 1$ columns, $z$ is a (column) vector of size $n + 1$ and $\times$ indicates the componentwise product of the resulting vectors.&lt;&#x2F;p&gt;
&lt;p&gt;We can alternatively view this compact form as&lt;br &#x2F;&gt;
$\left( \sum_k a_{0k} z_k \right) \left( \sum_k b_{0k} z_k \right) - \left( \sum_k c_{0k} z_k \right) = 0$&lt;br &#x2F;&gt;
$\left( \sum_k a_{1k} z_k \right) \left( \sum_k b_{1k} z_k \right) - \left( \sum_k c_{1k} z_k \right) = 0$&lt;br &#x2F;&gt;
$\left( \sum_k a_{2k} z_k \right) \left( \sum_k b_{2k} z_k \right) - \left( \sum_k c_{2k} z_k \right) = 0$&lt;br &#x2F;&gt;
$\vdots$&lt;br &#x2F;&gt;
$\left( \sum_k a_{mk} z_k \right) \left( \sum_k b_{mk} z_k \right) - \left( \sum_k c_{mk} z_k \right) = 0$&lt;&#x2F;p&gt;
&lt;p&gt;We could express these equations more compactly by using polynomials and prove the solution of the R1CS system more concisely. To this end, we will introduce quadratic arithmetic programs, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;vitalik.ca&#x2F;general&#x2F;2016&#x2F;12&#x2F;10&#x2F;qap.html&quot;&gt;QAP&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;quadratic-arithmetic-program&quot;&gt;Quadratic Arithmetic Program&lt;&#x2F;h2&gt;
&lt;p&gt;We can interpret each column of the $A$ matrix as evaluations of some polynomial over some suitable domain. This is a common practice in many SNARKs, where we try to encode a vector as a polynomial; see, for example, our &lt;a href=&quot;&#x2F;diving-deep-fri&#x2F;&quot;&gt;post about STARKs&lt;&#x2F;a&gt;. We sample $D_0 = { x_0 , x_1 , … , x_n }$ over the finite field and define the polynomial $A_i (x)$ as the polynomial of at most degree $n$ such that $A_i ( x_k ) = a_{ki}$.&lt;&#x2F;p&gt;
&lt;p&gt;For performance reasons, it is convenient to select as interpolation domain $D_0$ the n-th roots of unity since we can use the Fast Fourier Transform to interpolate. Similarly, we can interpret the columns of $B$ and $C$ as polynomials $B_k (x)$ and $C_k (x)$. Taking advantage of these polynomials, we can express the R1CS system in polynomial form,&lt;br &#x2F;&gt;
$P (x) = \left( \sum_k A_{k} (x) z_k \right) \left( \sum_k B_{k} (x) z_k \right) - \left( \sum_k C_{k} (x) z_k \right)$&lt;&#x2F;p&gt;
&lt;p&gt;We can see that if we have a valid solution for the R1CS, the polynomial $P (x)$ evaluates to $0$ over $D_0$ (since we require the polynomial to interpolate the values of the columns of the matrices). Therefore, we can express the condition as&lt;br &#x2F;&gt;
$P (x) = 0$ for $x \in D_0$&lt;br &#x2F;&gt;
We now introduce the vanishing polynomial over the set $D_0$, $Z_D (x) = \prod_k (x - x_k )$&lt;br &#x2F;&gt;
So, if the polynomial $P (x)$ evaluates to $0$ over $D_0$, it is divisible by $Z_D (x)$. This can be written as there is some polynomial $h (x)$ such that&lt;br &#x2F;&gt;
$P (x) = h(x) Z_D (x)$&lt;br &#x2F;&gt;
The degree of the polynomial $h(x)$ is the degree of $P$ minus the degree of $Z_D$. An honest prover should be able to find the resulting quotient and use it to show that he correctly executed the program.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;transforming-qap-into-a-zero-knowledge-proof&quot;&gt;Transforming QAP into a zero-knowledge proof&lt;&#x2F;h2&gt;
&lt;p&gt;We need to make some transformation to the above problem if we want to turn it into a zero-knowledge proof. For a more detailed description of this process, see &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.rareskills.io&#x2F;post&#x2F;groth16&quot;&gt;here&lt;&#x2F;a&gt;. We must ensure that the prover cannot cheat and that the verifier cannot learn anything about the private input or witness. One key ingredient is a polynomial commitment scheme (PCS): we can make the prover commit to a given polynomial so that he cannot change it later. One such commitment scheme is the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;dankradfeist.de&#x2F;ethereum&#x2F;2020&#x2F;06&#x2F;16&#x2F;kate-polynomial-commitments.html&quot;&gt;KZG commitment&lt;&#x2F;a&gt;, where we use &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;static1.squarespace.com&#x2F;static&#x2F;5fdbb09f31d71c1227082339&#x2F;t&#x2F;5ff394720493bd28278889c6&#x2F;1609798774687&#x2F;PairingsForBeginners.pdf&quot;&gt;pairing-friendly elliptic curves&lt;&#x2F;a&gt; to bind the prover to a polynomial. The scheme’s security relies on the hardness of the discrete logarithm problem over the curve. Pairings can be considered an operation that allows a one-time multiplication between points in an elliptic curve. In our case, we will work over type3 III pairings, $\dagger : G_1 \times G_2 \rightarrow G_t$, which have the following nice property (bilinearity):&lt;br &#x2F;&gt;
$(a g_1 ) \dagger (b g_2 ) = (ab) (g_1 \dagger g_2)$&lt;br &#x2F;&gt;
To commit to a polynomial using KZG, we need to sample a random scalar $\tau$ (which is considered toxic waste and should be forgotten, or we could forge proofs) and generate the following sequence of points in the elliptic curve, whose generator is $g_1$,&lt;br &#x2F;&gt;
$P_0 = g_1$,&lt;br &#x2F;&gt;
$P_1 = \tau g_1$&lt;br &#x2F;&gt;
$P_2 = \tau^2 g_1$&lt;br &#x2F;&gt;
$\vdots$&lt;br &#x2F;&gt;
$P_n = \tau^n g_1$&lt;br &#x2F;&gt;
Then, given a polynomial $p(x) = a_0 + a_1 x + a_2 x^2 + … + a_n x^n$ we compute the commitment as&lt;br &#x2F;&gt;
$\mathrm{cm} (p) = a_0 P_0 + a_1 P_1 + … + a_n P_n$&lt;br &#x2F;&gt;
which is the same as $\mathrm{cm} (p) = p(\tau) g_1$, that is, hiding the evaluation of $p(x)$ inside the elliptic curve. Because the discrete log problem is hard, we cannot use our knowledge of $g_1$ and $\mathrm{cm} (p)$ to obtain $p(\tau)$.&lt;&#x2F;p&gt;
&lt;p&gt;To check that the polynomial $p(x)$ evaluates to $v$ at $z$ we can use the fact that&lt;br &#x2F;&gt;
$p(x) - v = (x - z)q(x)$&lt;br &#x2F;&gt;
where $q(x)$ is the quotient polynomial of the division of $p(x)$ by $x - z$. The prover can produce proof of such evaluation by committing to $q(x)$ using the same trick. Still, the verifier will need some additional information (included in the verifying key), $g_2$ (the generator of the group $G_2$), and $\tau g_2$ (remember, nobody must know $\tau$). Then, using pairings, the verifier can check the evaluation using the points in the elliptic curves,&lt;br &#x2F;&gt;
$(\mathrm{cm} (p) - vg_1 \dagger g_2) = a = p(\tau) (g_1 \dagger g_2)$&lt;br &#x2F;&gt;
$\mathrm{cm} (q) \dagger (\tau g_2 - z g_2) = b = q(\tau) ( \tau - z)(g_1 \dagger g_2)$&lt;br &#x2F;&gt;
If $a$ and $b$ are the same, and since $\tau$ is a random point with high probability, we assume that $p(z) = v$ (This depends on the Schwartz-Zippel lemma).&lt;&#x2F;p&gt;
&lt;p&gt;Remember that we want to prove that the verifier knows some $w$ and a polynomial $h(x)$ of degree $m - 1$ such that if $z= (1, x, w)$, the following condition holds&lt;br &#x2F;&gt;
$\left( \sum_k A_{k} (x) z_k \right) \left( \sum_k B_{k} (x) z_k \right) = \left( \sum_k C_{k} (x) z_k \right) + h(x)Z_D (x)$&lt;&#x2F;p&gt;
&lt;p&gt;If we force the prover first to commit to the polynomials $A_k (x)$ and $B_k (x)$ and then produce the quotient polynomial, we have to make sure that he cannot forge $C_k (x)$ to fulfill the previous condition. To do so, we are going to introduce random shifts ($\alpha$ and $\beta$) to the evaluations:&lt;br &#x2F;&gt;
$\mathrm{cm} (\sum A_i z_i ) = \sum (A_i (\tau) z_i) g_1 + \alpha g_1$&lt;br &#x2F;&gt;
$\mathrm{cm} (\sum B_i z_i) = \sum (B_i (\tau) z_i) g_2 + \beta g_2$&lt;br &#x2F;&gt;
The $B_i (x)$ are committed to using group $G_2$ so that we can compute the product on the left-hand side through a pairing,&lt;br &#x2F;&gt;
$(\mathrm{cm} (\sum A_i z_i )) \dagger ( \mathrm{cm} (\sum B_i z_i )) = (\sum A_i (\tau) z_i )(\sum B_i (\tau) z_i ) (g_1 \dagger g_2)$&lt;&#x2F;p&gt;
&lt;p&gt;Because we introduce these shifts, we need to modify the $C_k$ term accordingly,&lt;br &#x2F;&gt;
$\begin{equation}\left( \alpha + \sum_k A_{k} (x) z_k \right) \left( \beta + \sum_k B_{k} (x) z_k \right) = \ \alpha \beta + \left( \sum_k (C_{k} (x) + \beta A_k (x) + \alpha B_k (x)) z_k \right) + h(x)Z_D (x) \end{equation}$&lt;br &#x2F;&gt;
Since the prover cannot know $\alpha$ and $\beta$, we need to provide them hidden as part of the trusted setup, as $\alpha g_1$ and $\beta g_2$, so that we can compute&lt;br &#x2F;&gt;
$(\alpha g_1) \dagger (\beta g_2) = \alpha \beta (g_1 \dagger g_2)$&lt;br &#x2F;&gt;
so that we can compare this result to the pairing between the shifted $A_i$ and $B_i$.&lt;&#x2F;p&gt;
&lt;p&gt;Also, since the prover does not have $\alpha$ and $\beta$, he needs to be supplied with all the elements of the form $C_{k} (x) + \beta A_k (x) + \alpha B_k (x)$. However, when we want to calculate the product between these terms and $z$, we must recall that $z$ contains both the public input and the witness. The verifier cannot learn anything about the witness (therefore, the evaluations involving the witness should be provided by the prover). We introduce two additional variables, $\gamma$, and $\delta$, to split the variable $z$ between public input and witness. The first $k$ terms correspond to the public input, and these are encoded as&lt;br &#x2F;&gt;
$K_i^v = \gamma^{- 1} (C_{i} (\tau) + \beta A_i (\tau) + \alpha B_i (\tau)) g_1$&lt;br &#x2F;&gt;
for $i = 0, 1, 2 … , k$. For the witness, we have&lt;br &#x2F;&gt;
$K_i^p = \delta^{- 1} (C_{i} (\tau) + \beta A_i (\tau) + \alpha B_i (\tau)) g_1$&lt;br &#x2F;&gt;
With these new parameters, we get&lt;br &#x2F;&gt;
$\begin{equation}\left( \alpha + \sum_j A_{j} (x) z_j \right) \left( \beta + \sum_j B_{j} (x) z_j \right) = \ \alpha \beta + \gamma \left( \sum_i^k \gamma^{- 1} (C_{i} (x) + \beta A_i (x) + \alpha B_i (x)) x_i \right) + \&lt;br &#x2F;&gt;
\delta \left( \sum_{j = k + 1}^n \delta^{- 1} (C_{i} (x) + \beta A_i (x) + \alpha B_i (x)) x_i \right) + h(x)Z_D (x) \end{equation}$&lt;br &#x2F;&gt;
We can combine the last two terms into one (since they contain all the information that the verifier must not learn)&lt;br &#x2F;&gt;
$D = \left( \sum_{j = k + 1}^n \delta^{- 1} (C_{i} (x) + \beta A_i (x) + \alpha B_i (x)) x_i \right) + h(x)Z_D (x)\delta^{- 1}$&lt;&#x2F;p&gt;
&lt;p&gt;Since we want to compute the product $h(x) Z_D(x)$ with the help of one pairing, we can compute the following group elements,&lt;br &#x2F;&gt;
$Z_0 = \delta^{ - 1} Z_D (\tau)$&lt;br &#x2F;&gt;
$Z_1 = \delta^{ - 1} \tau Z_D (\tau)$&lt;br &#x2F;&gt;
$Z_2 = \delta^{ - 1} \tau^2 Z_D (\tau)$&lt;br &#x2F;&gt;
$\vdots$&lt;br &#x2F;&gt;
$Z_{m - 1} = \delta^{ - 1} \tau^{ m - 1 } Z_D (\tau)$&lt;&#x2F;p&gt;
&lt;p&gt;With these changes, the right-hand side of the QAP is the sum of 3 terms:&lt;br &#x2F;&gt;
A constant (related to the random shifts).&lt;br &#x2F;&gt;
A term involving the public input.&lt;br &#x2F;&gt;
A term that contains the secret terms (known only to the prover).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;&#x2F;h2&gt;
&lt;p&gt;Groth16 requires sampling five random field elements to generate the proving and verifying key, $t, \alpha, \beta, \gamma, \delta$. These are toxic waste and should be discarded and wholly forgotten once the keys have been generated.&lt;&#x2F;p&gt;
&lt;p&gt;We will use a pairing-friendly elliptic curve (with type III pairing), with subgroups $G_1$ and $G_2$ of prime order $r$. We will call the generators $g_1$ and $g_2$, respectively. To make notation easier, we will write&lt;br &#x2F;&gt;
$[x]_1 = x g_1$&lt;br &#x2F;&gt;
$[x]_2 = x g_2$&lt;br &#x2F;&gt;
to denote points in $G_1$ and $G_2$, where $x g$ means the scalar product of $x$ and the generator of the group (i.e., applying x times the elliptic curve group operation to the generator). We will follow the notation given by &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2018&#x2F;691.pdf&quot;&gt;DIZK&lt;&#x2F;a&gt;. First, we compute the following vectors,&lt;br &#x2F;&gt;
$K_i^v (t) = \gamma^{-1} \left( \beta A_i(t) + \alpha B_i (t) + C_i (t)\right)$&lt;br &#x2F;&gt;
for $i = 0, 1, 2 , … k$,&lt;br &#x2F;&gt;
$K_i^p (t) = \delta^{-1} \left( \beta A_i(t) + \alpha B_i (t) + C_i (t)\right)$&lt;br &#x2F;&gt;
for $i = k+1, 1, 2 , … n$ and&lt;br &#x2F;&gt;
$Z_k (t) = t^k Z_D (t) \delta^{-1}$&lt;br &#x2F;&gt;
for $k = 0, 1, 2, … m - 1$.&lt;br &#x2F;&gt;
The proving key consists of the following elements:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. $[\alpha]_1$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. $[\beta]_1$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. $[\beta]_2$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. $[\delta]_1$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    5. $[\delta]_2$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    6. $[A_0 (t) ]_1, [A_1 (t) ]_1 , ... , [A_n (t) ]_1$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    7. $[B_0 (t) ]_1, [B_1 (t) ]_1 , ... , [B_n (t) ]_1$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    8. $[B_0 (t) ]_2, [B_1 (t) ]_2 , ... , [B_n (t) ]_2$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    9. $[K_{ k + 1 }^p (t)] , [ K_{ k + 2 }^p (t)] , ... , [K_n^p (t)]$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    10. $[Z_0 (t)] , [Z_1 (t)] , ... , [ Z_{ m - 1 } (t)]$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The verifying key is much shorter and will contain in addition the value of one pairing because that value is constant:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. $[\alpha]_1 \dagger [\beta]_2$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. $[\gamma]_2$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. $[\delta]_2$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. $[K_0^v (t)]_1 , [K_1^v (t)]_1 , ... , [K_k^v (t)]_1$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;proof-generation&quot;&gt;Proof generation&lt;&#x2F;h2&gt;
&lt;p&gt;The prover receives the proving key and knows the polynomials representing the program and the public input, and he wants to prove that he has a witness satisfying that program. First, the prover needs to calculate the quotient polynomial $h(x)$ or, more precisely, its coefficients. The prover has to calculate&lt;br &#x2F;&gt;
$$h(x) = \frac{\sum A_k(x) z_k \sum B_k (x) z_k - \sum C_k (x) z_k}{Z_D (X) }$$&lt;&#x2F;p&gt;
&lt;p&gt;The best way to evaluate this quotient is by choosing a domain $D_{ev}$, of size at least the degree of the quotient polynomial plus one and not containing elements from $D_0$ (the interpolation domain) and evaluating numerator and denominator at all the elements of $D_{ev}$. Since we have at least as many evaluations of the polynomial $h (x)$ as its degree plus one, we can reconstruct $h(x)$ via interpolation. In practice, the fastest way to do this is by using the Fast Fourier Transform for evaluation and interpolation. The prover now possesses a vector of coefficients $h_0 , h_1 , h_2 , … , h_m$.&lt;&#x2F;p&gt;
&lt;p&gt;To ensure that the proof is zero-knowledge, the prover sample two random scalars, $r$ and $s$.&lt;&#x2F;p&gt;
&lt;p&gt;The prover can compute the three elements of the proof, $\pi = ([\pi_1 ]_1 , [\pi_2 ]_2 , [\pi_3 ]_1)$ by doing the following calculations,&lt;br &#x2F;&gt;
$[\pi_1 ]_1 = [\alpha]_1 + \sum z_k [A_k (t) ]_1 + r [\delta]_1$&lt;br &#x2F;&gt;
$[\pi_2 ]_2 = [\beta]_2 + \sum z_k [B_k (t) ]_2 + s[\delta]_2$&lt;br &#x2F;&gt;
$[\pi_2 ]_1 = [\beta]_1 + \sum z_k [B_k (t) ]_1 + s[\delta]_1$&lt;br &#x2F;&gt;
$[h(t)z(t)]_1 = \sum h_i [Z_i (t)]_1$&lt;br &#x2F;&gt;
$[\pi_3 ]_1 = \sum w_i [K_i^p ]_1 + [h(t)z(t)]_1 + s[\pi_1 ]_1 + r [\pi_2 ]_1 - rs [\delta]_1$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;verification&quot;&gt;Verification&lt;&#x2F;h2&gt;
&lt;p&gt;The verifier has the verifying key, the public input and parses the proof as $[\pi_1 ]_1, [\pi_2 ]_2, [\pi_3 ]_1$ and computes the following:&lt;br &#x2F;&gt;
$[\pi_1 ]_1 \dagger [\pi_2 ]_2 = P_1$&lt;br &#x2F;&gt;
$[\pi_3 ]_1 \dagger [\delta]_2 + [\alpha]_1 \dagger [\beta]_2 + \left(\sum x_i [K_i^v ]_1 \right) \dagger [\gamma]_2 = P_2$&lt;&#x2F;p&gt;
&lt;p&gt;The proof is valid if $P_1$ and $P_2$ coincide. This is equivalent to checking the modified QAP.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h2&gt;
&lt;p&gt;In this post, we covered the Groth16 protocol, which provides a framework to prove the correctness of a computation without revealing sensitive information. It has concise proofs and an elegant verification but requires a trusted setup for every program we want to prove. We saw the steps to transform the program into arithmetic circuits or their equivalent R1CS, which can then be compiled into a quadratic arithmetic program. We explained how the protocol transforms the basic equations to ensure that the prover cannot cheat and the verifier does not learn anything about the private data. In an upcoming post, we will cover how to code Groth16 from scratch.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Transforming the Future with Zero-Knowledge Proofs, Fully Homomorphic Encryption and new Distributed Systems algorithms</title>
        <published>2023-04-13T00:00:00+00:00</published>
        <updated>2023-04-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/transforming-the-future-with-zero-knowledge-proofs-fully-homomorphic-encryption-and-new-distributed-systems-algorithms/"/>
        <id>https://blog.lambdaclass.com/posts/transforming-the-future-with-zero-knowledge-proofs-fully-homomorphic-encryption-and-new-distributed-systems-algorithms/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/transforming-the-future-with-zero-knowledge-proofs-fully-homomorphic-encryption-and-new-distributed-systems-algorithms/">&lt;p&gt;Disclaimer: To maintain brevity and clarity, we have simplified certain concepts. In this discussion, Zero Knowledge Proofs and Computational Integrity are considered as a single concept, and we will not address the distinct security properties of Proof of Work and Proof of Stake.&lt;&#x2F;p&gt;
&lt;p&gt;The evolution of every scientific discipline or engineering field experiences cycles akin to those observed in economies. Incremental advancements are made daily by corporations, individuals, and academic institutions. Occasionally, a researcher or engineer makes a groundbreaking discovery that alters the course of the field. One such example is Sir Isaac Newton, who made significant contributions to calculus, motion, optics, and gravitation during the time of the bubonic plague, which claimed millions of lives. His relentless pursuit of knowledge throughout the pandemic proved instrumental in shaping the development of mathematics, physics, and engineering. Our comfortable modern lives stand upon the foundation of these monumental discoveries.&lt;&#x2F;p&gt;
&lt;p&gt;The general public is aware of the big breakthroughs made in the aerospatial industry, energy production, internet of things, and last but not least artificial intelligence. However, most don’t know that during the COVID pandemic, enormous advances were made in cryptography. 47 years ago Diffie and Hellman wrote in their famous cryptography paper: “we stand today on the brink of a revolution in cryptography”, which enabled two people to exchange confidential information even when they can only communicate via a channel monitored by an adversary. This revolution enabled electronic commerce and the communication between citizens of the free world. We believe the discoveries made by researchers and engineers in cryptography during this COVID pandemic will be as important as the discoveries made by Diffie and Hellman in the upcoming decades.&lt;&#x2F;p&gt;
&lt;p&gt;One of the big discoveries has been how to make Zero-Knowledge Proofs fast enough for real-world applications. This technology has been around since 1984 but as Diffie also said, “Lots of people working in cryptography have no deep concern with real application issues. They are trying to discover things clever enough to write papers about”. Fortunately for humanity, researchers and engineers have made this technology practical enough in the last decade (especially the last 2 years) to be useful.&lt;&#x2F;p&gt;
&lt;p&gt;The financial system depends on the existence of intermediaries: an army of auditors, regulators, and accountants. The correct working of the financial machine depends on the integrity of its financial institutions. Integrity is maintained due to positive economic incentives and jail time, fines, and costly lawsuits if the intermediaries don’t do what the state and society expect from them. Bitcoin, a result of the 2008 crisis, created a permissionless financial system where its users can send and receive digital money without intermediaries and without anybody being able to block transactions. In countries like Argentina, Nigeria, or Lebanon, where stagnation and inflation erode its citizens’ trust in the financial system and the state, Bitcoin and stablecoins on top of Ethereum are used on a daily basis by the young population to save and avoid capital controls. In developed countries, its usage is not as massive since the traditional financial system and the state is trusted by most citizens. However, the world is becoming more complex. Banks are failing in the US and Europe, a new war is taking place in Europe, debt levels are not sustainable in many countries, the fight between left and the right is retaking the main stage, tension between the West and the East increases, and technological change keeps accelerating.&lt;&#x2F;p&gt;
&lt;p&gt;New applications built on top of unstoppable and trustless technologies that don’t depend on social trust will grow and thrive in this type of environment. Everything is being questioned. Only things that can’t be questioned will fully resist the passage of time. This will happen not only in developing countries but also in developed ones. Systems like Bitcoin, where everyone can verify how it’s running, are more resilient and become more useful by the day in a world that is getting more complex.&lt;&#x2F;p&gt;
&lt;p&gt;Bitcoin’s focus has been to become a new type of monetary asset and financial network. For this reason, the development of more complex programs on top of Bitcoin has always been restricted by design. Newer blockchains like Ethereum added the ability to create new types of applications. DeFi Protocols that enabled lending and borrowing, exchange of digital currencies and the ability to buy, sell and trade digital collectives and arts rapidly grew on top of Ethereum. However the cost of creating and transferring relevant amounts of assets in blockchains is costly. The ability to create more complex applications that sit on top of blockchains is also very limited. Applications can’t run more than a few milliseconds on Ethereum.&lt;&#x2F;p&gt;
&lt;p&gt;These systems do not rely on social integrity like traditional systems. Instead, they operate as a permissionless and censorship-resistant network, allowing anyone to add a node and submit updates to its state. To ensure verification, each node must re-execute all transactions, which makes the system decentralized and secure, albeit slower than centralized systems. Consequently, this imposes a limitation on the types of applications that can be built on blockchains. Applications requiring frequent database state updates, such as those exceeding a few times per second, or machine learning algorithms, are not feasible on blockchain platforms.&lt;&#x2F;p&gt;
&lt;p&gt;This is where Zero Knowledge Proofs (ZKPs) and other cryptographic and distributed systems primitives will help society create tools that can be used by everyone. ZKPs enable a party to demonstrate a statement to other parties without revealing any information beyond the proof. In more concrete terms, this enables a person to show another person that the computation they did is correct without having to redo it and without even having to grant access to the data that was used. An important aspect of this is that the verification is done in a much faster time than the proving. In even simpler terms, it proves that the output of a certain computation is correct. The verification is way easier and faster to do than the execution or proving. Anybody can check the proof, and this saves computing time and money.&lt;&#x2F;p&gt;
&lt;p&gt;At the beginning it’s difficult to grasp, even for engineers, that such a technology is even possible. The mathematics behind it, until recently, seemed magical, and that’s why it was called moon math. Thanks to ZKPs, transferring money in blockchains similar to Bitcoin is cheaper and way faster since there is no need to re-execute each transaction by each node. Only one node is needed to process all the transactions and prove them using a ZKPs, while the rest simply need to verify it, saving valuable computing resources. Among other things, ZKPs enable creating a financial system that doesn’t depend on social trust like traditional finance and that doesn’t depend as much on re-executing algorithms as Bitcoin.&lt;&#x2F;p&gt;
&lt;p&gt;Zero Knowledge Proofs facilitate the development of an entirely new range of applications that are executed and proven on a single computer outside the blockchain, with verification occurring within Ethereum. The verification cost is way cheaper than the time it takes to prove or execute it. Ethereum will evolve from a slow yet secure distributed mainframe, where execution time is shared among all users to run small programs, into a distributed computer that stores and verifies proofs generated externally from the blockchain.&lt;&#x2F;p&gt;
&lt;p&gt;Not only will blockchains benefit from the development of new cryptographic primitives like Zero Knowledge Proofs (ZKPs), but other areas will also be significantly impacted. As AI-generated content begins to overshadow human-generated content on the internet, ZKPs will become essential for verifying that such content was produced by unbiased AI models. “Proof of humanity” systems are already employing ZKPs to ensure the accurate computation of a human accessing specific resources.&lt;&#x2F;p&gt;
&lt;p&gt;Hardware is another area where ZKPs will make an impact. Similar to how graphics cards in the 1990s revolutionized the video game industry, zero-knowledge hardware acceleration will be integrated into computers to enhance efficiency.&lt;&#x2F;p&gt;
&lt;p&gt;ZKPs can also be utilized to balance storage and computation securely. For instance, security cameras generate vast amounts of data. ZKPs can provide a compact proof that AI models did not detect any critical information in the video, allowing the system to delete the footage and save storage space.&lt;&#x2F;p&gt;
&lt;p&gt;ZKPs will even be used for national security purposes. As energy production shifts from centralized power plants to distributed sources like solar panels and wind turbines, verifying the proper execution of software on their controllers becomes vital. In the coming decades, ZKPs will play a crucial role in securing these devices.&lt;&#x2F;p&gt;
&lt;p&gt;Software industry regulations are inevitable, and industries such as online casinos and ad networks using Real-Time Bidding protocols will be legally required to demonstrate that they have not deceived their clients. Laws protecting users from large tech corporations are already in place in Europe, partly due to concerns about data misuse by foreign powers to influence political campaigns.&lt;&#x2F;p&gt;
&lt;p&gt;Requirements for secure storage and processing of encrypted data will become increasingly necessary. Fully Homomorphic Encryption (FHE), a technology akin to ZKPs, will be one of the tools utilized for this purpose. FHE enables computation on encrypted data, ensuring privacy. As FHE becomes more efficient and practical, most databases will integrate some FHE functionality, preventing administrators from accessing user data directly.&lt;&#x2F;p&gt;
&lt;p&gt;Zero-knowledge proofs (ZKPs), which generate evidence for a third party to confirm the accurate execution of a computation, and Fully Homomorphic Encryption (FHE), which enables calculations on encrypted data, will be combined with distributed systems algorithms that are capable of tolerating significant network failures similar to those employed by Bitcoin. Together they will be utilized to comply with regulations while creating trustless applications.&lt;&#x2F;p&gt;
&lt;p&gt;In the past decade, we have successfully launched applications serving dozens of millions of users. Leveraging our expertise, we are now dedicated to providing both technical and financial support to help others create startups focused on developing and implementing these vital technologies. As society grapples with the challenges of our rapidly evolving world these innovations will prove to be indispensable.&lt;&#x2F;p&gt;
&lt;p&gt;Federico Carrone.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Diving DEEP FRI in the STARK world: learning your daily moon math with a concrete example</title>
        <published>2023-03-06T00:00:00+00:00</published>
        <updated>2023-03-06T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/diving-deep-fri/"/>
        <id>https://blog.lambdaclass.com/posts/diving-deep-fri/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/diving-deep-fri/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;At LambdaClass, we are building &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;lambdaworks&quot;&gt;Lambdaworks&lt;&#x2F;a&gt;, a library for developing zero-knowledge stuff. One important proof system is &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2018&#x2F;046.pdf&quot;&gt;STARKs&lt;&#x2F;a&gt; (Scalable, transparent arguments of knowledge). STARKs are a powerful tool that allows us to prove the integrity of a given computation. For an overview of STARKs, you can look at our &lt;a href=&quot;&#x2F;lambdaworks-or-how-we-decided-to-created-our-zksnarks-library-and-a-stark-prover&#x2F;t&quot;&gt;previous post&lt;&#x2F;a&gt; or the excellent tutorials by Starkware, such as &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;starkware.co&#x2F;stark-101&#x2F;&quot;&gt;STARK-101&lt;&#x2F;a&gt; (for the rust version, you can follow &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;STARK101-rs&#x2F;&quot;&gt;this link&lt;&#x2F;a&gt;) and the posts on &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;starkware&#x2F;arithmetization-i-15c046390862&quot;&gt;arithmetization I&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;starkware&#x2F;arithmetization-ii-403c3b3f4355&quot;&gt;II&lt;&#x2F;a&gt;, and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;aszepieniec.github.io&#x2F;stark-anatomy&#x2F;overview&quot;&gt;Anatomy of a STARK&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In this post, we will do a pen-and-paper example of STARKs, so we can follow all the steps needed to generate and validate a proof (we will skip the hashing part, though). One important aspect to point out is that, in this case, we are not interested in the security properties of what we do (but it should really matter in real life). Let’s jump into the problem…&lt;&#x2F;p&gt;
&lt;h2 id=&quot;problem-statement&quot;&gt;Problem statement&lt;&#x2F;h2&gt;
&lt;p&gt;Suppose we want to compute a sequence given by the following relations:&lt;br &#x2F;&gt;
$a_0=3$&lt;br &#x2F;&gt;
$a_{n+1}={a_n}^2$&lt;br &#x2F;&gt;
The sequence gives the square of the previous number, starting with the value 3. We will use as modulus the prime 17 (a Fermat prime, $2^4+1$), and we will understand all operations done modulo 17. The advantage of 17 is that it contains a multiplicative group of 16 elements, which is helpful for STARKs (in general, we want $p-1$ to be $2^m\times q$, where $m$ should be sufficiently large and $q$ is an odd prime). The first four elements of the sequence are:&lt;br &#x2F;&gt;
$a_0 = 3$&lt;br &#x2F;&gt;
$a_1 = {a_0}^2 = 9$&lt;br &#x2F;&gt;
$a_2 = {a_1}^2 = 9^2 = 81 \equiv 13 \pmod{17}$&lt;br &#x2F;&gt;
$a_3 = {a_2}^2 = 13^2 = 169 \equiv 16 \pmod{17}$&lt;br &#x2F;&gt;
The first step is to interpret these values as evaluations of a polynomial over a suitable domain. We are working with $p=17$, whose multiplicative group has 16 elements: $\{ 1 , 2 , 3 , 4 , \dots , 15 , 16 \}$. We will choose the following subgroup $D_t = {1 , 13 , 16, 4 }$, which is none other than the group formed by all powers of $13$ modulo $17$:&lt;br &#x2F;&gt;
$13^0 = 1$&lt;br &#x2F;&gt;
$13^1 = 13$&lt;br &#x2F;&gt;
$13^2 = 169 \equiv 16 \pmod{17}$&lt;br &#x2F;&gt;
$13^3 \equiv 4 \pmod{17}$&lt;br &#x2F;&gt;
$13^4 \equiv 1 \pmod{17}$&lt;br &#x2F;&gt;
From now on, we will drop the $\pmod{17}$ as understood from the context. We see that the powers of $13$ repeat every 4, which is the order of the element in the multiplicative group. Using this and calling the polynomial interpolating the trace as $t(x)$, we have:&lt;br &#x2F;&gt;
$t(1) = 3$&lt;br &#x2F;&gt;
$t(13) = 9$&lt;br &#x2F;&gt;
$t(16) = 13$&lt;br &#x2F;&gt;
$t(4) = 16$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;interpolation&quot;&gt;Interpolation&lt;&#x2F;h2&gt;
&lt;p&gt;We can use Lagrange interpolation to find the polynomial (for larger problems, it is best to use the Fast-Fourier Transform):&lt;br &#x2F;&gt;
$t(x) = L_1(x)t(1) + L_2(x)t(13) + L_3(x)t(16) + L_4(x)t(4)$&lt;br &#x2F;&gt;
The Lagrange polynomial $L_1(x)$ is given by&lt;br &#x2F;&gt;
$$L_1(x) = \frac{(x-13)(x-16)(x-4)}{(1-13)(1-16)(1-4)}$$&lt;br &#x2F;&gt;
Doing the operations, we get&lt;br &#x2F;&gt;
$L_1 (x)t(1) = 5(x^3 + x^2 + x + 1)$&lt;br &#x2F;&gt;
Th other polynomials are&lt;br &#x2F;&gt;
$L_2 (x)t(13) = 8x^3 + 2x^2 + 9x + 15$&lt;br &#x2F;&gt;
$L_3 (x)t(16) = x^3 + 16x^2 + x + 16$&lt;br &#x2F;&gt;
$L_4 (x)t(4) = 16x^3 + 13x^2 + x + 4$&lt;br &#x2F;&gt;
The trace interpolating polynomial is thus&lt;br &#x2F;&gt;
$t(x) = 13x^3 + 2x^2 + 16x + 6$&lt;br &#x2F;&gt;
If we evaluate the polynomial at $D_t$, you can check that we get the same values as in the trace execution table.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;committing-to-the-trace-polynomial&quot;&gt;Committing to the trace polynomial&lt;&#x2F;h2&gt;
&lt;p&gt;We have to commit to the trace interpolating polynomial. To do so, we perform a low-degree extension by choosing a larger domain, different from the original domain. If we choose $h = 9$ and its powers, we get a cyclic subgroup with $8$ elements, $\{ h^0 , h^1 , h^2 , \dots , h^7 \}$. This group contains elements from $D_t$, so we shift it to another domain by introducing an element from the coset, $w$, and forming the following domain,&lt;br &#x2F;&gt;
$$ D_0 = \{ wh^0 , wh^1 , wh^2, \dots , wh^7 \}$$&lt;br &#x2F;&gt;
We can choose $w = 3$, and so the domain becomes&lt;br &#x2F;&gt;
$$ D_0 = \{ 3, 10, 5, 11, 14 , 7 , 12 , 6 \}$$&lt;br &#x2F;&gt;
To commit, we evaluate $t(x)$ over all values in $D_0$ and form a Merkle tree whose leaves are those values.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;$x$&lt;&#x2F;th&gt;&lt;th&gt;$t(x)$&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;11&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;6&lt;&#x2F;td&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;enter-the-constraints&quot;&gt;Enter the constraints&lt;&#x2F;h2&gt;
&lt;p&gt;We now need to focus on the constraints over the trace elements the calculation gives. In this problem, we have two constraints:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Boundary condition. This applies to the first row, where $t(1)=3$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Transition constraint. These are given by the multivariate polynomial $P(x,y) = y - x^2$, where if $x = a_n$, then $y = a_{n+1}$.  &lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;What we need to do at this point is compose the trace polynomial with these constraints to enforce them over the whole trace.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;boundary-constraint&quot;&gt;Boundary constraint&lt;&#x2F;h3&gt;
&lt;p&gt;The first constraint is&lt;br &#x2F;&gt;
$p_1 (x) = t(x)-3$&lt;br &#x2F;&gt;
To ensure that it is enforced on the first step, the polynomial $p_1 (x)$ must be divisible by $x-1$ (a property of polynomials says that $p(a)=b$ if and only if $r(x) = p(x)-b$ is divisible by $x-a$).&lt;br &#x2F;&gt;
We have&lt;br &#x2F;&gt;
$p_1 (x) = 13x^3 + 2x^2 + 16x + 3$&lt;br &#x2F;&gt;
If we factorize this polynomial, we get&lt;br &#x2F;&gt;
$p_1 (x) = 13(x-1)(x^2 + 9x + 5)$&lt;br &#x2F;&gt;
which has the factor $(x-1)$. If we divide, we get&lt;br &#x2F;&gt;
$C_1 (x) = 13 (x^2 + 9x + 5)$&lt;br &#x2F;&gt;
You can check that if we want $t (x) - a$ to be divisible by $x-1$, the necessarily $a=3$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;transition-verification-constraint&quot;&gt;Transition verification constraint&lt;&#x2F;h3&gt;
&lt;p&gt;To evaluate the second constraint, we need to be able to choose an element of the trace and the next. We can do it by noting that the elements of $D_t$ are generated by $g = 13$, so if we select $x=x_0$, then $y=g x_0$ is the next. So, $y=t(gx)=t(13x)$ and&lt;br &#x2F;&gt;
$t(gx) = x^3 + 15x^2 + 4x + 6$&lt;br &#x2F;&gt;
We now replace these polynomials into the transition verification polynomial, $P(x,y)$, to get $p_2(x)$&lt;br &#x2F;&gt;
$p_2 (x) = P(t(x) , t(gx)) = x^6 + 16 x^5 + 5x^4 + 2x^3 + 7x^2 + 16x + 4$&lt;br &#x2F;&gt;
You can check that if we choose $x \in {1, 13, 16 }$ the polynomial evaluates to $0$. This is expected, since the elements $a_n$ and $a_{n+1}$ are linked by the formula $a_{n+1}= {a_n}^2$. This is no longer the case for $4$ since there is no next element. As before, if the constraints are valid, then $p_2 (x)$ should be divisible by $Z_2 (x)$, which is the vanishing polynomial over the domain where the constraints are enforced. In our case,&lt;br &#x2F;&gt;
$Z_2 (x) = (x-1)(x-13)(x-16)$&lt;br &#x2F;&gt;
We can also write it as&lt;br &#x2F;&gt;
$$Z_2 = \frac{x^4 - 1}{x-4}$$&lt;br &#x2F;&gt;
where we just remove the elements in which the constraints are not enforced. We verified that $p_2 (x)=0$ for $x \in {1, 13, 16 }$, so $p_2 (x)$ has factors $(x-1)(x-13)(x-16)$. Its complete factorization is&lt;br &#x2F;&gt;
$p_2 (x) = (x-1)(x-13)(x-16)(x^3 + 12 x^2 + 9x + 16)$&lt;br &#x2F;&gt;
Thus,&lt;br &#x2F;&gt;
$$C_2 (x) = \frac{p_2 (x)}{Z_2 (x)} = x^3 + 12 x^2 + 9x + 16$$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-constraint-composition-polynomial&quot;&gt;The (constraint) composition polynomial&lt;&#x2F;h2&gt;
&lt;p&gt;We are now in a condition to build the composition polynomial&lt;br &#x2F;&gt;
$$H(x) = C_1 (x) (\alpha_1 x^{ D - D_1 } + \beta_1 ) + C_2 (x) (\alpha_2 x^{ D - D_2 } + \beta_2 )$$&lt;br &#x2F;&gt;
where the $\alpha_k$ and $\beta_k$ are values provided by the verifier. The terms $D - D_k$ are added so that all the polynomials in the linear combination have the same degree. We want the total degree to be a power of $2$, so $D=4$.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose the verifier samples as random coefficients the following: $\alpha_1 = 1$, $\beta_1 = 3$, $\alpha_2 = 2$, $\beta_2 = 4$. Then,&lt;br &#x2F;&gt;
$C_1 (x) (1 x^{ 4 - 2 } + 3 ) = 13x^4 + 15 x^3 + 2 x^2 + 11x + 8$&lt;br &#x2F;&gt;
$C_2 (x) (2 x^{ 4 - 3 } + 4 ) = 2x^4 + 11 x^3 + 15 x^2 + 13$&lt;br &#x2F;&gt;
Then,&lt;br &#x2F;&gt;
$H (x) = 15 x^4 + 9 x^3 + 11 x + 4$&lt;br &#x2F;&gt;
Splitting the polynomial into odd and even terms,&lt;br &#x2F;&gt;
$H_1 (x^2) = 15 x^4 + 4$&lt;br &#x2F;&gt;
$H_2 (x^2) = 9x^2 + 11$&lt;br &#x2F;&gt;
so that&lt;br &#x2F;&gt;
$H(x) = H_1 ( x^2 ) + x H_2 (x^2)$&lt;br &#x2F;&gt;
We can commit to the polynomial $H(x)$ or its parts, $H_1(x)$ and $H_2(x)$ by evaluating over $D_0$ and forming a Merkle tree.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;$x$&lt;&#x2F;th&gt;&lt;th&gt;$H_1(x)$&lt;&#x2F;th&gt;&lt;th&gt;$H_2(x)$&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;11&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;sampling-outside-the-original-domain&quot;&gt;Sampling outside the original domain&lt;&#x2F;h2&gt;
&lt;p&gt;The verifier now chooses a random point, $z$, outside the trace interpolation and evaluation domains. In our example, the points outside those are $\{ 2, 8, 9 , 15 \}$. Suppose the verifier selected $z = 8$. Then,&lt;br &#x2F;&gt;
$H ( 8 ) = 10$&lt;br &#x2F;&gt;
with each part being&lt;br &#x2F;&gt;
$H_1 (8^2) = 6$&lt;br &#x2F;&gt;
$H_2 (8^2) = 9$&lt;br &#x2F;&gt;
We need to check that the composition polynomial and trace elements are related. To be able to evaluate the constraints numerically, we need both $t(z)$ and $t(gz)$ (remember, $g$ is the generator of the trace interpolating domain) since we have to calculate $P(x,y)$. The necessary values are:&lt;br &#x2F;&gt;
$t(8) = 16$&lt;br &#x2F;&gt;
$t(13 \times 8) = t(2) = 14$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;why-does-the-verifier-need-this&quot;&gt;Why does the verifier need this?&lt;&#x2F;h2&gt;
&lt;p&gt;The verifier can now check that the trace and composition polynomial are related:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. $p_1 (8) = t(8) - 3 = 13$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. $Z_1 (8) = 8 - 1 = 7$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. $C_1 (8) = p_1 (8) &#x2F; Z_1 (8) = 13 \times 7^{-1} = 14$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. $C_1 (8) (1\times 8^2 +3) = 3$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    5. $p_2 (8) = t(2) - t(8)^2 = 13$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    6. $Z_2 (8) = 8$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    7. $C_2 (8) = p_2 (8)&#x2F; Z_2 (8) = 13 \times 8^{-1} = 8$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    8. $C_2 (8) (2\times 8 +4) = 7$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    9. $H (8) = C_1 (8) + C_2 (8) = 3 + 7 = 10$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We see that the evaluation of $H_1 (z^2)$ and $H_2 (z^2)$ matches the calculation of $H(z)$ from the trace elements.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ensuring-the-prover-does-not-cheat&quot;&gt;Ensuring the prover does not cheat&lt;&#x2F;h2&gt;
&lt;p&gt;How does the verifier check that the values we passed are indeed the trace and composition polynomial evaluations at $z$ and $gz$? We can use the same trick: if the polynomial $y(x)$ evaluates to $b$ in $x=a$, then $y(x) - b$ is divisible by $x - a$. We form the DEEP composition polynomial,&lt;br &#x2F;&gt;
$$ P_0 (x) = \gamma_1\frac{t(x)-t(z)}{x-z} + \gamma_2 \frac{t(x)- t(gz)}{x-gz}+\gamma_3 \frac{H_1 (x^2) - H_1 (z^2) }{x-z^2 } + \gamma_4 \frac{H_2 (x^2) - H_2 (z^2) }{x - z^2}$$&lt;br &#x2F;&gt;
Let’s calculate each term&lt;br &#x2F;&gt;
$$\frac{t(x)-t(8)}{x-8} = 13(x+13)(x+3) = 13 (x^2 + 16 x + 5)$$&lt;br &#x2F;&gt;
$$\frac{t(x)-t(2)}{x-2} = 13(x+8)(x+2) = 13 (x^2 + 10 x + 16)$$&lt;br &#x2F;&gt;
$$\frac{H_1 (x^2) - H_1 (8^2) }{x-8^2 } = 15(x+15)(x+8)(x+2) $$&lt;br &#x2F;&gt;
$$\frac{H_2 (x^2) - H_1 (8^2) }{x-8^2 } = 9(x+8) $$&lt;&#x2F;p&gt;
&lt;p&gt;Each term is a polynomial, so the linear combination is also a polynomial. By applying the FRI protocol, we must prove to the verifier that this is close to a low-degree polynomial. The polynomial is (using $\gamma_i = 1$),&lt;br &#x2F;&gt;
$P_0 ( x ) = 15 x^3 + 15 x + 1$&lt;br &#x2F;&gt;
We can commit to this polynomial using $D_0$ and forming a Merkle tree,&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;$x$&lt;&#x2F;th&gt;&lt;th&gt;$P_0(x)$&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;9&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;11&lt;&#x2F;td&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;td&gt;6&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;6&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Splitting into odd and even terms,&lt;br &#x2F;&gt;
$xP_{0,odd} (x) = 15 x^3 + 15 x$&lt;br &#x2F;&gt;
$P_{0,even} (x) = 1$&lt;br &#x2F;&gt;
The verifier samples $\beta_0 = 4$. Then,&lt;br &#x2F;&gt;
$P_1 (y=x^2) = 9y +10$&lt;br &#x2F;&gt;
The domain is given by points of the form $y=x^2$, so $D_1 = \{ 9, 15, 8, 2\}$. The leaves of the Merkle tree are&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;$y$&lt;&#x2F;th&gt;&lt;th&gt;$P_1(y)$&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;9&lt;&#x2F;td&gt;&lt;td&gt;6&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;td&gt;9&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;8&lt;&#x2F;td&gt;&lt;td&gt;11&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;We repeat the process,&lt;br &#x2F;&gt;
$yP_{0,odd} (y) = 9y$&lt;br &#x2F;&gt;
$P_{0,even} (y) = 10$&lt;br &#x2F;&gt;
The verifier samples $\beta_1 = 3$&lt;br &#x2F;&gt;
$P_2 (z=y^2) = 3$.&lt;br &#x2F;&gt;
And we ended with a constant polynomial. This second domain is $D_2 = \{13, 4\}$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;checking-fri-layers&quot;&gt;Checking FRI layers&lt;&#x2F;h2&gt;
&lt;p&gt;To generate the proof, the verifier chooses an element from $D_0$. We have to send him all the elements needed to reconstruct the evaluations of the composition polynomial and the FRI steps. Say he chooses $x=10$, which corresponds to the index equal to $1$. To evaluate everything, we must pass the evaluation at $x$ and $-x$ for each layer and the trace polynomial evaluated at $x$ and $gx$.&lt;&#x2F;p&gt;
&lt;p&gt;From $P_0(x)$ we pass the values $P_0(x=10)=4$ and $P_0(x=7)=15$, together with their authentication paths.&lt;br &#x2F;&gt;
From $P_1(x)$ we pass the values $P_1(x=15)=9$ and $P_1(x=2)=11$ and their authentication paths.&lt;br &#x2F;&gt;
From $P_2(x)$, we only need the constant value of $3$.&lt;&#x2F;p&gt;
&lt;p&gt;Checking the correctness of FRI requires verifying that each value corresponds to its Merkle tree and the colinearity test,&lt;br &#x2F;&gt;
$$P_{i+1}(x^2)=\frac{P_i(x) + P_i(-x)}{2}+\beta_i \frac{P_i (x) - P_i (-x)}{2x}$$&lt;br &#x2F;&gt;
Let’s check the jump from each layer:&lt;br &#x2F;&gt;
$$P_1(15) = 16 = \frac{P_0(10) + P_0(7)}{2} + 4 \frac{P_0 (10) - P_0 (7)}{2\times 10}$$&lt;br &#x2F;&gt;
We can see that&lt;br &#x2F;&gt;
$$\frac{P_0(10) + P_0(7)}{2} = 1$$&lt;br &#x2F;&gt;
and&lt;br &#x2F;&gt;
$$ 4 \frac{P_0 (10) - P_0 (7)}{2\times 10} = 8$$&lt;&#x2F;p&gt;
&lt;p&gt;Let’s jump onto the next layer,&lt;br &#x2F;&gt;
$$P_{2}(y^2)=\frac{P_1(y) + P_1(-y)}{2}+\beta_1 \frac{P_1 (y) - P_1 (-y)}{2y}$$&lt;br &#x2F;&gt;
Replacing the values,&lt;br &#x2F;&gt;
$$P_{2}(y^2) = 3$$&lt;br &#x2F;&gt;
and&lt;br &#x2F;&gt;
$$ \frac{P_1(15) + P_1(2)}{2} = 10$$&lt;br &#x2F;&gt;
$$ 3\frac{P_1 (15) - P_1 (2)}{2\times 15} = 10$$&lt;br &#x2F;&gt;
But&lt;br &#x2F;&gt;
$$ 10 + 10 = 3 = P_2(4)$$&lt;br &#x2F;&gt;
which completes the check. You can try selecting other indices and verifying the proof.&lt;&#x2F;p&gt;
&lt;p&gt;The only remaining check shows that the trace and composition polynomial are related. We leave it as a challenge (the answer will appear shortly)&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;This post covered a pen-and-paper example of computational integrity using STARKs. We chose a sequence where each element is the square of the previous one, starting from 3. We stated the problem, interpreted the computation as evaluating a polynomial over a suitable domain, and performed Lagrange interpolation. After that, we enforced the constraints over the execution trace and obtained the composition polynomial. To improve soundness, we forced the prover to evaluate at a point $z$ outside the domain and showed that the trace and composition polynomial are related. Then, we created a rational function that ensured the prover did not cheat and sent the correct values. If the prover is honest, then the resulting function is a polynomial, and we proved by showing that it is close to a low-degree polynomial using FRI. If you want to try more complicated examples, follow the updates at Lambdaworks.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>LambdaWorks or how we decided to create our zkSNARKs library and a STARK prover</title>
        <published>2023-03-01T00:00:00+00:00</published>
        <updated>2023-03-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/lambdaworks-or-how-we-decided-to-created-our-zksnarks-library-and-a-stark-prover/"/>
        <id>https://blog.lambdaclass.com/posts/lambdaworks-or-how-we-decided-to-created-our-zksnarks-library-and-a-stark-prover/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/lambdaworks-or-how-we-decided-to-created-our-zksnarks-library-and-a-stark-prover/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;We think that most ZK libraries are not yet easy-to-use. Most of them assume that the user had a significant cryptography background, making it hard for a newcomer to learn from them, even if he had all the code in front of him. We also found that some commonly used libraries had poor documentation or hard-to-follow examples for beginners. In addition to this some libraries don’t follow state of the art engineering practices that are crucial to build reliable systems that go to production. There are many efforts like Cairo, Noir that don’t have these issues but they are full blown programming languages. We wanted a tool to build languages like those, new proving systems or anything that we need.&lt;&#x2F;p&gt;
&lt;p&gt;So, we decided to start building our &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;lambdaworks&quot;&gt;LambdaWorks&lt;&#x2F;a&gt; library with the following goals in mind:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Implemented in Rust with WASM support and an FFI API in other mainstream languages&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Easy to use API&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Contains most famous proving systems (Groth16, Plonk, STARKs, Plonky2 and maybe Halo2) and recursion&#x2F;IVC (Nova, Supernova)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. Allow for hardware acceleration, such as GPU and FPGA integration&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    5. Clear documentation with different kinds of tutorials, from starters to advanced users&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Given their importance and applications, we decided to begin our library by implementing the STARKs’ prover. We had to implement finite field arithmetic and basic cryptographic stuff, such as Merkle trees and hash functions. We will continue with elliptic curves and SNARKs.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;starks&quot;&gt;STARKs&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2018&#x2F;046.pdf&quot;&gt;STARKs&lt;&#x2F;a&gt; (scalable, transparent arguments of knowledge) are cryptographic primitives, which are a convenient means to an end. The goal we are after is computational integrity, that is, showing that a computation was performed correctly (according to a set of instructions). For example, we want to prove that we computed the first 5000 values of a sequence correctly, or we ran a given machine learning algorithm, or we processed 4000 transactions in a blockchain. STARKs provide us with short proof of the integrity of the computation. The advantage STARKs gives us is that checking the proof is much faster than performing the naïve verification (re-executing the program by the verifier).&lt;&#x2F;p&gt;
&lt;p&gt;There are many interesting resources to learn the basics of STARKs, such as &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;starkware.co&#x2F;stark-101&#x2F;&quot;&gt;Starkware’s STARK 101&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;aszepieniec.github.io&#x2F;stark-anatomy&#x2F;overview&quot;&gt;Anatomy of a STARK&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;andrewmilson&#x2F;ministark&quot;&gt;Ministark&lt;&#x2F;a&gt;, as well as Starkware’s blog on arithmetization (&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;starkware&#x2F;arithmetization-i-15c046390862&quot;&gt;parts I&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;starkware&#x2F;arithmetization-ii-403c3b3f4355&quot;&gt;II&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;The STARK protocol contains the following steps:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Arithmetization&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Transformation to polynomial equations.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * FRI, which has two steps: commitment and decommitment.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;arithmetization&quot;&gt;Arithmetization&lt;&#x2F;h2&gt;
&lt;p&gt;An execution trace is a table containing $w$ columns (the registers) and $T$ rows representing each state of the system. A trace looks like this:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Register 1&lt;&#x2F;th&gt;&lt;th&gt;Register 2&lt;&#x2F;th&gt;&lt;th&gt;$\dots$&lt;&#x2F;th&gt;&lt;th&gt;Register w&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;$x_{1,0}$&lt;&#x2F;td&gt;&lt;td&gt;$x_{2,0}$&lt;&#x2F;td&gt;&lt;td&gt;$\dots$&lt;&#x2F;td&gt;&lt;td&gt;$x_{w,0}$&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;$x_{1,1}$&lt;&#x2F;td&gt;&lt;td&gt;$x_{2,1}$&lt;&#x2F;td&gt;&lt;td&gt;$\dots$&lt;&#x2F;td&gt;&lt;td&gt;$x_{w,1}$&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;$\vdots$&lt;&#x2F;td&gt;&lt;td&gt;$\vdots$&lt;&#x2F;td&gt;&lt;td&gt;$\ddots$&lt;&#x2F;td&gt;&lt;td&gt;$\vdots$&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;$x_{1,T}$&lt;&#x2F;td&gt;&lt;td&gt;$x_{2,T}$&lt;&#x2F;td&gt;&lt;td&gt;$\dots$&lt;&#x2F;td&gt;&lt;td&gt;$x_{w,T}$&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;We will interpret each column (register) as the evaluation of a polynomial over a domain (we will call it the trace evaluation domain). For example, we can say that $f_1(x)$ is the polynomial representing column 1 and thus:&lt;br &#x2F;&gt;
$f_1(0)=x_{1,0}$&lt;br &#x2F;&gt;
$f_1(1)=x_{1,1}$&lt;br &#x2F;&gt;
$\vdots$&lt;br &#x2F;&gt;
$f_1(T)=x_{1,T}$&lt;&#x2F;p&gt;
&lt;p&gt;To make things easier and faster, we will use as trace evaluation domain a multiplicative subgroup, $\mathbb{Z_p}^\star$ of size $2^n$, such that $2^n \geq T$. That group has a generator, $\omega$, which spans all elements in the subgroup. The subgroup can be represented by the powers of $\omega$, $\{ 1, \omega , \omega^2 , \omega^3 ,…, \omega^N \}$. Our trace polynomial satisfies then&lt;br &#x2F;&gt;
$f_1(1)=x_{1,0}$&lt;br &#x2F;&gt;
$f_1(\omega)=x_{1,1}$&lt;br &#x2F;&gt;
$\vdots$&lt;br &#x2F;&gt;
$f_1(\omega^{T-1})=x_{1,T}$&lt;&#x2F;p&gt;
&lt;p&gt;The elements in the execution trace satisfy certain relations given by the computation and boundary conditions. We call these relations constraints. They can be broadly classified into two groups:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Boundary constraints.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Transition constraints.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Boundary constraints are rather straightforward: they specify the value of a register at a given time. For example, when we initialize the computations, each register has a given value. In the case of the Fibonacci sequence,&lt;br &#x2F;&gt;
$a_0=a_1=1$&lt;br &#x2F;&gt;
If our trace consists of a single column representing the sequence, the first two elements are equal to one:&lt;br &#x2F;&gt;
$x_{1,0}=1$&lt;br &#x2F;&gt;
$x_{1,1}=1$&lt;&#x2F;p&gt;
&lt;p&gt;We can translate the constraints into polynomial relations. We know that $x_{1,0}=f_1(1)$ and $x_{1,1}=f_1(\omega)$. If the constraint holds, say at $x=\omega$, then the monomial $x-\omega$ divides $f_1(x)-1$. This means that the result of the division of $f(x)-1$ by $x-\omega$ is a polynomial,&lt;br &#x2F;&gt;
$$ Q_{BC,1}(x)=\frac{f_1(x)-1}{x-\omega} $$&lt;br &#x2F;&gt;
Analogously,&lt;br &#x2F;&gt;
$$ Q_{BC,0}(x)=\frac{f_1(x)-1}{x-1} $$&lt;&#x2F;p&gt;
&lt;p&gt;One drawback in this approach is that if we have $n$ boundary constraints, we get $n$ polynomials. One optimization is to interpolate boundary constraints and obtain a new polynomial. In this case,&lt;br &#x2F;&gt;
$f_{BC}(1)=1$&lt;br &#x2F;&gt;
$f_{BC}(\omega)=1$&lt;br &#x2F;&gt;
Combining everything, we get&lt;br &#x2F;&gt;
$$ Q_{BC}(x)=\frac{f(x)-f_{BC}(x)}{Z_{BC}(x)}$$&lt;br &#x2F;&gt;
where $Z_{BC}(x)$ is the polynomial vanishing on the points where the boundary conditions are enforced:&lt;br &#x2F;&gt;
$Z_{BC}(x)=(x-1)(x-\omega)$&lt;&#x2F;p&gt;
&lt;p&gt;Transition constraints are relations between different rows that can be applied at various calculation points. In the case of the Fibonacci sequence, we have $a_{n+2}=a_{n+1}+a_n$ for every $n={0,1,…T-2 }$. In terms of the trace polynomial,&lt;br &#x2F;&gt;
$f_1(\omega^2 x)-f_1(\omega x)-f_1(x)=0$&lt;br &#x2F;&gt;
If the constraint is satisfied, the following function should be a polynomial,&lt;br &#x2F;&gt;
$$Q_T(x)=\frac{f_1(\omega^2 x)-f_1(\omega x)-f_1(x)}{Z_T(x)} $$&lt;br &#x2F;&gt;
where $Z_T(x)$ is the vanishing polynomial where the transition constraints are enforced,&lt;br &#x2F;&gt;
$Z_T(x)=\prod_{k=0}^{T-2} (x-\omega^k)$&lt;&#x2F;p&gt;
&lt;p&gt;Transition constraints are commonly expressed as multivariate polynomials linking two consecutive rows of the execution trace. For example, if we denote by $x$ a given row and $y$ is the next, a constraint could be something like&lt;br &#x2F;&gt;
$P(x,y)=y-x^2=0$&lt;br &#x2F;&gt;
If we compose the constraint polynomial with the trace polynomial, we have $x=t(x)$, $y=t(\omega x)$, so&lt;br &#x2F;&gt;
$t(\omega x) - t(x)^2=0$&lt;&#x2F;p&gt;
&lt;p&gt;If we did the calculations properly, then $Q_{BC}(x)$ and $Q_T(x)$ should be polynomials; if not, they are rational functions (quotients of two polynomials). We can reduce proving that each of them is a polynomial by taking a random linear combination&lt;br &#x2F;&gt;
$$ CP(x)=\alpha_{BC} Q_{BC}(x)+\alpha_{T} Q_T(x) $$&lt;br &#x2F;&gt;
If $Q_{BC}(x)$ and $Q_T(x)$ are both polynomials, so is $CP(x)$. But if at least one of them is a rational function, then $CP(x)$ is unlikely to be a polynomial.&lt;&#x2F;p&gt;
&lt;p&gt;Given that proving that $CP(x)$ is a polynomial is difficult, we will show that it is close to a low-degree polynomial. To do so, we will project $CP(x)$ to a new function with a smaller degree. We will continue taking projections until we reach a constant polynomial. The critical ingredient is that the projection operation respects the distance. If the original function is far from a low-degree polynomial, then the projections will also be far from it. Before jumping to the procedure (called FRI), we must commit to the trace polynomials.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;committing-to-the-trace&quot;&gt;Committing to the trace&lt;&#x2F;h2&gt;
&lt;p&gt;We need to evaluate the trace polynomials over a much larger domain; the domain size is $\beta 2^n$, where $\beta$ is the blowup factor. To avoid problems, we shift the domain by multiplying the elements by $h$, which belongs to the coset. The low-degree extension domain (simply domain) is given by&lt;br &#x2F;&gt;
$$D = \{ h, h \eta , h \eta^2 , … , h \eta^{ 2^n -1} \} $$&lt;br &#x2F;&gt;
Here $\eta$ is the generator of the subgroup of order $\beta 2^n$ so that it does not get confused with $\omega$ (though we could relate them by taking $\omega=\eta^\beta$).&lt;br &#x2F;&gt;
We evaluate the trace polynomials over this large domain and obtain vectors representing each evaluation:&lt;br &#x2F;&gt;
$$[ f_1 (h) , f_1 (h \eta) ,… , f_1 (h \eta^{ 2^n -1} )]$$&lt;br &#x2F;&gt;
$$[f_2 (h) , f_2 (h \eta) , … , f_2 (h \eta^{ 2^n -1} )]$$&lt;br &#x2F;&gt;
$$[f_w (h) , f_w (h \eta ) , … , f_w ( h \eta^{ 2^n -1} )]$$&lt;&#x2F;p&gt;
&lt;p&gt;To commit to these evaluations, we build Merkle trees, and the prover sends the root of the Merkle trees to the verifier. To make things easier, the elements of each row of the low-degree extension of the trace are grouped into a single leaf.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;committing-to-the-composition-polynomial&quot;&gt;Committing to the composition polynomial&lt;&#x2F;h2&gt;
&lt;p&gt;We use the same domain $$D = \{ h, h \eta , h \eta^2 , … , h \eta^{ 2^n -1} \} $$ to evaluate the composition polynomial. We can then create a Merkle tree from these evaluations and send the root to the verifier.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;relating-the-lde-of-execution-trace-and-the-composition-polynomial&quot;&gt;Relating the LDE of execution trace and the composition polynomial&lt;&#x2F;h2&gt;
&lt;p&gt;At some point, the verifier will ask the prover for the value of the composition polynomial at one point, $z$, that is, $CP(z)$. The verifier needs to be sure that the composition polynomial results from applying the polynomial constraints onto the trace polynomials. Given the value $z \in D$ (in DEEP, the value of $z$ is sampled outside the domain), the prover needs to send the values of the trace polynomials at given points so that the verifier can check the calculation. For example, in the case of Fibonacci (we will ignore all other constraints just for simplicity),&lt;br &#x2F;&gt;
$P(u,v,w)=w-v-u=0$&lt;br &#x2F;&gt;
$P(t(x),t(\omega x),t(\omega^2 x))=t(\omega^2x)-t(\omega x)-t(x)=0$&lt;br &#x2F;&gt;
To create the composition polynomial, we must divide the previous polynomial by the corresponding vanishing polynomial. So, if we pick $x=z$, we have&lt;&#x2F;p&gt;
&lt;p&gt;$$Q(z)=\frac{t(\omega^2z)-t(\omega z)-t(z)}{Z_D(z)}$$&lt;&#x2F;p&gt;
&lt;p&gt;The prover needs to send those three values. Note that $z=h \eta^k$, so the prover needs to send the values of $t(\omega^2 h \eta^k)$, $t(\omega h \eta^k)$, $t( h \eta^k)$, which are separated by $\beta$ elements in the Merkle tree. The verifier takes the three values, evaluates the vanishing polynomials, and checks that&lt;br &#x2F;&gt;
$Q(z)=CP(z)$&lt;&#x2F;p&gt;
&lt;p&gt;This way, the verifier is convinced that the composition polynomial is related to the execution trace via the constraint polynomials.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;fri-protocol&quot;&gt;FRI protocol&lt;&#x2F;h2&gt;
&lt;p&gt;The prover must show that $CP(x)$ is close to a low-degree polynomial. To do so, he will randomly fold the polynomial, reducing the degree, until he gets a constant polynomial (in optimizations, obtaining a constant polynomial is unnecessary, as the prover could send all the coefficients of a polynomial and have the verifier check it). The FRI protocol has two steps: commit and decommit.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;commitment&quot;&gt;Commitment&lt;&#x2F;h3&gt;
&lt;p&gt;The prover takes $CP(x)$ and splits it in the following way:&lt;br &#x2F;&gt;
$$g(x^2)=\frac{CP(x)+CP(-x)}{2}$$&lt;br &#x2F;&gt;
$$x h(x^2)=\frac{CP(x)-CP(-x)}{2}$$&lt;br &#x2F;&gt;
so that&lt;br &#x2F;&gt;
$$CP(x)=g(x^2)+x h(x^2)$$&lt;br &#x2F;&gt;
The verifier chooses a random value $\alpha_0$, and the prover forms the polynomial,&lt;br &#x2F;&gt;
$P_1(x)=g(x^2)+\alpha_0 h(x^2)$&lt;br &#x2F;&gt;
with the new domain $D_1 = \{ h^2 , h^2 \eta^2 , … , h^2 \eta^m \}$ having half the size of $D$.&lt;&#x2F;p&gt;
&lt;p&gt;The prover can perform the low-degree extension by evaluating $P_1(x)$ over $D_1$ and then commit to it by creating a Merkle tree and sending the root. He can continue with the procedure by halving the degree at each step. For step $k$, we have&lt;br &#x2F;&gt;
$$P_k(y^2)=\frac{P_{k-1}(y)+P_{k-1}(-y)}{2}+\alpha_{k-1}\left(\frac{P_{k-1}(y)-P_{k-1}(-y)}{2}\right)$$&lt;br &#x2F;&gt;
and&lt;br &#x2F;&gt;
$$D_k = \{ h^{ 2^{k-1} } , (h \eta)^{ 2^{k-1} } , … ,( \eta^l h)^{ 2^{k-1} } \}$$&lt;br &#x2F;&gt;
The prover evaluates $P_k(x)$ over $D_k$ and commits to it, sending the Merkle root.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;decommitment&quot;&gt;Decommitment&lt;&#x2F;h3&gt;
&lt;p&gt;The verifier chooses at random a point $q$ belonging to $D$. The prover needs to convince him that the trace polynomials and composition polynomial are related (we covered that previously) and that the elements of consecutive FRI layers are also related. For each layer, the prover needs to send two elements to the verifier, $P_k(z)$ and $P_k(-z)$. He also needs to show that these elements belong to the corresponding Merkle tree, so the authentication paths for each element are also required.&lt;&#x2F;p&gt;
&lt;p&gt;The verifier can check the correctness of the FRI layers by performing a colinearity check. Given $P_k(z)$, $P_k(-z)$ and $P_{k+1}(z^2)$, the verifier can compute&lt;br &#x2F;&gt;
$$g_{k+1}(z^2)=\frac{P_k(z)+P_k(-z)}{2}$$&lt;br &#x2F;&gt;
$$h_{k+1}(z^2)=\frac{P_k(z)-P_k(-z)}{2z}$$&lt;br &#x2F;&gt;
and get the value for the next layer&lt;br &#x2F;&gt;
$$u_{k+1}=g_{k+1}(z^2)+\alpha_k h_{k+1}(z^2)$$&lt;br &#x2F;&gt;
If the prover performed the calculations correctly, then&lt;br &#x2F;&gt;
$$u_{k+1}=P_{k+1}(z^2)$$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;a-toy-example-for-fri&quot;&gt;A toy example for FRI&lt;&#x2F;h2&gt;
&lt;p&gt;We will use a simple example to understand how everything works on FRI. We choose $p=17$, whose multiplicative group has order $16=2^4$ and set $\eta=3$, which is a primitive root of unity (that is, $3^{16}=1$ and $3^k \neq 1$ for $0 &amp;lt;k&amp;lt;16$). Our composition polynomial is $P_0 (x) = x^3 + x^2 + 1$. The domain for the LDE is simply $ D_0 = \mathbb{Z_{17}}^\star = \{1 , 2 , 3 , 4 , 5 , 6 , … , 16 \}$. The following table contains the LDE of $P_0(x)$:&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Index&lt;&#x2F;th&gt;&lt;th&gt;$x$&lt;&#x2F;th&gt;&lt;th&gt;$P_0(x)$&lt;&#x2F;th&gt;&lt;th&gt;Index&lt;&#x2F;th&gt;&lt;th&gt;$x$&lt;&#x2F;th&gt;&lt;th&gt;$P_0(x)$&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;8&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;9&lt;&#x2F;td&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;9&lt;&#x2F;td&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;td&gt;8&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;td&gt;11&lt;&#x2F;td&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;td&gt;12&lt;&#x2F;td&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;6&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;td&gt;11&lt;&#x2F;td&gt;&lt;td&gt;8&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;td&gt;6&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Suppose the verifier samples $\beta_0=3$. The prover performs the random folding over $P_0(x)$,&lt;br &#x2F;&gt;
$g_1( x^2 ) = 1 + x^2$&lt;br &#x2F;&gt;
$xh_1 ( x^2 ) = x^3 $&lt;br &#x2F;&gt;
so&lt;br &#x2F;&gt;
$P_1 ( x^2 ) = 1 + ( 1 + \beta_0) x^2$&lt;br &#x2F;&gt;
To make things simpler,&lt;br &#x2F;&gt;
$P_1(y)=1+4y$&lt;br &#x2F;&gt;
with $y = x^2$. The new domain is obtained by squaring the elements of $D_0$. The LDE of $P_1(y)$ is&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Index&lt;&#x2F;th&gt;&lt;th&gt;$y$&lt;&#x2F;th&gt;&lt;th&gt;$P_1(y)$&lt;&#x2F;th&gt;&lt;th&gt;Index&lt;&#x2F;th&gt;&lt;th&gt;$y$&lt;&#x2F;th&gt;&lt;th&gt;$P_1(y)$&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;td&gt;14&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;9&lt;&#x2F;td&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;5&lt;&#x2F;td&gt;&lt;td&gt;8&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;13&lt;&#x2F;td&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;6&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;15&lt;&#x2F;td&gt;&lt;td&gt;10&lt;&#x2F;td&gt;&lt;td&gt;7&lt;&#x2F;td&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;9&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The verifier samples $\beta_1=2$ and the prover folds $P_1(y)$ to get $P_2(z)$,&lt;br &#x2F;&gt;
$P_2(z)=1+4\beta_1=9$&lt;br &#x2F;&gt;
which is a constant polynomial. The domain $D_2 = \{ 1 , 13 , 16 , 4 \}$. All the elements in the LDE evaluate to 9, so there is no need for a table.&lt;&#x2F;p&gt;
&lt;p&gt;The evaluations of the polynomials $P_0(x)$, $P_1(x)$, and $P_2(x)$ are each committed using a Merkle tree and sent to the verifier.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose the verifier selects index 4 in the LDE to check the correctness of the FRI layers. The prover needs to send him the following:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * $P_0(13)=4$ and $P_0(-13)=P_0(4)=13$ and their authentication paths.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * $P_1(16)=14$ and $P_1(-16)=P_1(1)=5$ and their authentication paths.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * $P_2(4)=9$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can see that, for the first layer, the prover passes the values at positions 4 and 12, then 4 and 1 (which is $index+|D_1|&#x2F;2$, where $|D_1|$ is the number of elements in $D_1$, but since 8 exceeds the maximum value, we wrap around).&lt;&#x2F;p&gt;
&lt;p&gt;The verifier does the following calculation,&lt;br &#x2F;&gt;
$$u=\frac{P_0(13)+P_0(4)}{2}+\beta_0\left(\frac{P_0(13)-P_0(4)}{2\times 13}\right)$$&lt;&#x2F;p&gt;
&lt;p&gt;Recall that division by $t$ is simply multiplication by $t^{-1}$. In the case of $2$, we have $2^{-1}=9$, since $2\times 9=18\equiv 1 \pmod{17}$. Thus,&lt;br &#x2F;&gt;
$$u=2^{-1}\left(4+13\right)+3\times 9^{-1}\left(4-13\right)$$&lt;br &#x2F;&gt;
The first term is $0$, while the second is $48\equiv 14 \pmod{17}$, so&lt;br &#x2F;&gt;
$u=14$.&lt;br &#x2F;&gt;
Next, he checks&lt;br &#x2F;&gt;
$u=P_1(16)$&lt;br &#x2F;&gt;
Both are $14$, so the first layer is correct.&lt;&#x2F;p&gt;
&lt;p&gt;The verifier moves on to the next layer. He needs to calculate&lt;br &#x2F;&gt;
$$u=\frac{P_1(16)+P_1(1)}{2}+\beta_1\left(\frac{P_1(16)-P_1(1)}{2\times 16}\right)$$&lt;&#x2F;p&gt;
&lt;p&gt;If we work the calculations,&lt;br &#x2F;&gt;
$$u=\frac{2}{2}+2\left(\frac{9}{2 \times 16}\right)$$&lt;br &#x2F;&gt;
But this is just&lt;br &#x2F;&gt;
$$u=1+(-9)=1+8=9$$&lt;br &#x2F;&gt;
Now,&lt;br &#x2F;&gt;
$$P_2(4)=9=u$$&lt;br &#x2F;&gt;
so all the layers have been checked. You should try selecting an index and showing that all the calculations match.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;STARKs are powerful cryptographic primitives allowing a party to prove the integrity of a computation. To generate the proof, we obtain the execution trace of the program and interpret each column of the trace as the evaluations of a polynomial over a “nice” domain. The rows of the execution trace are related by low-degree polynomials, which determine the constraints. When we compose the constraint polynomials with the trace polynomials, we enforce the constraints over the execution trace. We can then divide them by the vanishing polynomial over their validity domain (the places where each constraint is enforced); if the constraints hold, the division is exact and yields a polynomial. Instead of proving that the result is a polynomial, STARKs show that the result is close to a low-degree polynomial. FRI randomly folds the function, halving the degree at each step; the critical point is that this folding preserves distance from low-degree polynomials. The protocol contains two phases: commit, in which the prover binds himself to evaluations of the polynomials over their corresponding domain, and decommit, where he generates the proof that allows the verifier to check the calculations. In an upcoming post, we will cover some optimizations and examples.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Everything you wanted to know about periodic constraints in STARKs but nobody told you</title>
        <published>2023-02-24T00:00:00+00:00</published>
        <updated>2023-02-24T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/periodic-constraints-and-recursion-in-zk-starks/"/>
        <id>https://blog.lambdaclass.com/posts/periodic-constraints-and-recursion-in-zk-starks/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/periodic-constraints-and-recursion-in-zk-starks/">&lt;p&gt;As you might have already guessed we are studying all the literature and code available to become one of big players in the industry. We want to thank &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;EliBenSasson&quot;&gt;Eli Ben-Sasson&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;StarkWareLtd&quot;&gt;Starkware&lt;&#x2F;a&gt; for the amazing work they’ve been doing in the space and for helping us to learn all this. We also want to thank &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;maxgillett&quot;&gt;Max Gillett&lt;&#x2F;a&gt; for the time he has invested in talking with us about all these things. They have been amazing with us and we hope we can continue learning a lot from them.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2018&#x2F;046.pdf&quot;&gt;ZK-STARKs&lt;&#x2F;a&gt; (zero-knowledge scalable, transparent, post-quantum arguments of knowledge) are cryptographic tools that allow one party to prove the integrity of a computation. For example, a party can show that he computed the first 1000 elements of a Fibonacci sequence correctly, ran a given machine learning algorithm, or correctly processed 5000 Ethereum transactions. Moreover, checking the resulting proof is much faster than performing the naïve re-execution of the computation by a verifier (the verification time scales logarithmically in the calculation size). Given their properties, they have attracted interest in many areas; among them, they can solve the scalability problems that decentralized ledgers suffer from.&lt;&#x2F;p&gt;
&lt;p&gt;There are many interesting resources to learn the basics of STARKs, such as Starkware’s &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;starkware.co&#x2F;stark-101&#x2F;&quot;&gt;STARK 101&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;aszepieniec.github.io&#x2F;stark-anatomy&#x2F;overview&quot;&gt;Anatomy of a STARK&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;andrewmilson&#x2F;ministark&quot;&gt;Ministark&lt;&#x2F;a&gt;, as well as Starkware’s blog on arithmetization (&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;starkware&#x2F;arithmetization-i-15c046390862&quot;&gt;parts I&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;starkware&#x2F;arithmetization-ii-403c3b3f4355&quot;&gt;II&lt;&#x2F;a&gt;). In this post, we will focus on how constraints are enforced and how to deal with them when applied periodically. Soon we will be posting a more in-depth version of STARKs.&lt;&#x2F;p&gt;
&lt;p&gt;The starting point for STARKs is arithmetization. We generate the execution trace of the program, obtaining a table showing how each register evolves according to the instructions being executed. The values of the execution table are related by constraints (usually low-degree polynomials). We will focus, in particular, on transition constraints and how to check that the values of the trace satisfy them.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Transition constraints&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;A transition constraint dictates the relations between different states of a computation. Suppose we have one register, which contains the elements of a Fibonacci sequence,&lt;&#x2F;p&gt;
&lt;p&gt;$a_0=a_1=1$&lt;&#x2F;p&gt;
&lt;p&gt;$a_{n+2}=a_{n+1}+a_n$&lt;&#x2F;p&gt;
&lt;p&gt;The last equation gives the transition constraint for the Fibonacci sequence; the others handle the boundary constraints for the problem, and it is easier to deal with them. To make our discussion easier, suppose that we performed $2^m$ steps of the Fibonacci sequence for some $m \geq 1$. We get, by rewriting the constraints and analyzing each index,&lt;&#x2F;p&gt;
&lt;p&gt;$a_2-a_1-a_0=0$&lt;&#x2F;p&gt;
&lt;p&gt;$a_3-a_2-a_1=0$&lt;&#x2F;p&gt;
&lt;p&gt;$a_4-a_3-a_2=0$&lt;&#x2F;p&gt;
&lt;p&gt;and so on. We can convert the trace elements into polynomials by interpolating over a suitable domain. To make things easier, we choose the $n$-th roots of unity, which enables us to perform interpolation via the fast Fourier transform. The roots are spanned by one element (a generator, $g$): by taking its powers, we get all the $n$-th roots of unity, $\left\{1,g,g^2,g^3,…,g^{n-1}\right\}$.Let us call $t(x)$ the polynomial interpolating the trace, that is, the polynomial taking the following values:&lt;&#x2F;p&gt;
&lt;p&gt;$t(1)=a_0$&lt;&#x2F;p&gt;
&lt;p&gt;$t(g)=a_1$&lt;&#x2F;p&gt;
&lt;p&gt;$t(g^2)=a_2$&lt;&#x2F;p&gt;
&lt;p&gt;$\vdots$&lt;&#x2F;p&gt;
&lt;p&gt;$t(g^{n-1})=a_{n-1}$&lt;&#x2F;p&gt;
&lt;p&gt;We can express the constraints as&lt;&#x2F;p&gt;
&lt;p&gt;$t(g^2)-t(g)-t(1)=0$&lt;&#x2F;p&gt;
&lt;p&gt;$t(g^3)-t(g^2)-t(g)=0$&lt;&#x2F;p&gt;
&lt;p&gt;$t(g^4)-t(g^3)-t(g^2)=0$&lt;&#x2F;p&gt;
&lt;p&gt;In a generic way,&lt;&#x2F;p&gt;
&lt;p&gt;$t(g^2x)-t(gx)-t(x)=0$&lt;&#x2F;p&gt;
&lt;p&gt;The way we can check that the constraints are enforced is by verifying that the polynomial $p(x)=t(g^2x)-t(gx)-t(x)$ is divisible by $(x-x_0)$, where $x_0$ is the point where we enforce the constraint. Another way to see this is that the resulting function&lt;&#x2F;p&gt;
&lt;p&gt;$$Q(x)=\frac{p(x)}{x-x_0} $$&lt;&#x2F;p&gt;
&lt;p&gt;is a polynomial. Instead of showing that $Q(x)$ is a polynomial, the STARK IOP proves that it is close to a low-degree polynomial.&lt;&#x2F;p&gt;
&lt;p&gt;In the case of the Fibonacci sequence, the constraint is valid for $x_0 \in \left\{1,g,g^2,…g^{n-3} \right\}$. Given that it is divisible by each factor, it is divisible by the product of all of them,&lt;&#x2F;p&gt;
&lt;p&gt;$Z_D(x)=\prod_{0}^{n-3} (x-g^k)$&lt;&#x2F;p&gt;
&lt;p&gt;The problem we face with this polynomial is that, to compute it, we need to perform a linear amount of multiplications, that is, as many multiplications as factors there are. Fortunately, the roots of unity have the following property:&lt;&#x2F;p&gt;
&lt;p&gt;$s(x)=\prod_{i=0}^{n-1} (x-g^k)=x^n-1$&lt;&#x2F;p&gt;
&lt;p&gt;So, instead of performing a linear amount of operations, we can calculate $Z_D(x)$ from $s(x)$ by taking out the missing factors:&lt;&#x2F;p&gt;
&lt;p&gt;$$Z_D(x)=\frac{ s(x)}{\prod_j (x-g^j)}=\frac{x^n-1}{(x-g^{n-1})(x-g^{n-2})} $$&lt;&#x2F;p&gt;
&lt;p&gt;The advantage of STARKs is that if a constraint is repeated many times, we can express that concisely. The only change goes in the vanishing polynomial $Z_D(x)$, which adds factors.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Constraints repeating after $m$ steps&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In a case such as Fibonacci’s, the constraint involves almost all points in the domain, so calculating the vanishing polynomial, $Z_D(x)$, is straightforward. But what happens when a constraint is applied only at certain points? For example, in EthStark, some transition constraints are applied only after $m$ steps.&lt;&#x2F;p&gt;
&lt;p&gt;To fix ideas, suppose that we have a transition constraint of the form&lt;&#x2F;p&gt;
&lt;p&gt;$f(x,gx,…g^d x)=0$&lt;&#x2F;p&gt;
&lt;p&gt;Our Fibonacci sequence fits this form. We will now consider that it applies every four steps; that is, the constraint is enforced at $x_0 \in \left\{1, g^4, g^8, g^{12},…\right\}$&lt;&#x2F;p&gt;
&lt;p&gt;The vanishing polynomial looks like&lt;&#x2F;p&gt;
&lt;p&gt;$Z_D(x)=\prod_k (x-g^{4k})$&lt;&#x2F;p&gt;
&lt;p&gt;If $g$ is a generator of the $n$-th roots of unity, $g^4$ is a generator of the $n&#x2F;4$-th roots of unity, $\omega=g^4$. So, we can rewrite the former as&lt;&#x2F;p&gt;
&lt;p&gt;$Z_D(x)=\prod_k (x-\omega^k)$&lt;&#x2F;p&gt;
&lt;p&gt;But since the product is over all $n&#x2F;4$-th roots of unity,$Z_D(x)=x^{n&#x2F;4}-1$. If the constraint is applied every 32 steps, as in EthStark, the vanishing polynomial is simply$Z_D(x)=x^{n&#x2F;32}-1$. If we skip some steps, we need to take those out. For example, suppose we have two constraints&lt;&#x2F;p&gt;
&lt;p&gt;$f_1(x,g x)=0$&lt;&#x2F;p&gt;
&lt;p&gt;$f_2(x, g x)=0$.&lt;&#x2F;p&gt;
&lt;p&gt;Constraint 2 is enforced every four steps, and constraint 1 is enforced every two (but not where constraint 2 is valid). To make it clear, constraint 2 is valid at $x_0 \in \left\{1,g^4,g^8,g^{12},…\right\}$ and constraint 1 is valid at $\left\{g^2, g^6, g^{10},… \right\}$. The vanishing polynomial for constraint 2 is&lt;&#x2F;p&gt;
&lt;p&gt;$Z_{D,2}(x)=\prod (x-g^{4k})$&lt;&#x2F;p&gt;
&lt;p&gt;and we have already found the solution, $Z_{D,2}(x)=(x^{n&#x2F;4}-1)$. For constraint 1, we have&lt;&#x2F;p&gt;
&lt;p&gt;$Z_{D,1} =\prod_{i \neq 0 \pmod{2} } (x-g^{2i})$&lt;&#x2F;p&gt;
&lt;p&gt;The $i \neq 0 \pmod{2}$ is just a way to say that the product only considers odd values of $i$ (so multiples of 4 are ruled out). We can apply the same trick as before:&lt;&#x2F;p&gt;
&lt;p&gt;$$Z_{D,1}=\frac{ \prod (x-g^{2i})}{\prod (x-g^{4k})}$$&lt;&#x2F;p&gt;
&lt;p&gt;This may seem weird, but we know precisely how to calculate each of them:&lt;&#x2F;p&gt;
&lt;p&gt;$$Z_{D,1}(x)=\frac{x^{n&#x2F;2}-1}{x^{n&#x2F;4}-1} $$&lt;&#x2F;p&gt;
&lt;p&gt;From here, we can remove some points where the constraint is not enforced. For example, if it is not valid at $x_0=6$,&lt;&#x2F;p&gt;
&lt;p&gt;$$Z_{D,1}(x)=\frac{x^{n&#x2F;2}-1}{(x^{n&#x2F;4}-1)(x-g^6)} $$&lt;&#x2F;p&gt;
&lt;p&gt;If we added a constraint $f_3(x,gx)$ that is enforced on steps $\left\{32,64,92,… \right\}$, we would have 3 vanishing polynomials,&lt;&#x2F;p&gt;
&lt;p&gt;$$Z_{D,3}=\frac{x^{n&#x2F;32}-1}{x-1} $$&lt;&#x2F;p&gt;
&lt;p&gt;$$Z_{D,2}=\frac{(x^{n&#x2F;4}-1)(x-1)}{x^{n&#x2F;32}-1} $$&lt;&#x2F;p&gt;
&lt;p&gt;$$Z_{D,1}(x)=\frac{x^{n&#x2F;2}-1}{x^{n&#x2F;4}-1} $$&lt;&#x2F;p&gt;
&lt;p&gt;So, by taking advantage of the properties of the roots of unity, we can enforce constraints that are applied periodically.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Summary&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;STARKs are a powerful tool that allows us to prove the integrity of a computation. To that end, STARKs start with the execution trace of a program and interpolate each column using polynomials. To see that the trace is valid, we need to check that all the constraints given by the computation are enforced. These constraints can be composed with the trace polynomials; if the constraints hold at step $T$, the resulting polynomial $P(x)$ should be divisible by $(x-g^{T-1})$ or, equivalently, there is a polynomial $Q(x)$ such that $P(x)=Q(x)(x-g^{T-1})$. If a constraint is applied multiple times, we can use the following facts to express them concisely:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The polynomial $P(x)$ is divisible by the product of factors of the form $x-x_0$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * We can easily shift the constraints thanks to the structure of the multiplicative subgroups.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The product of all elements in the multiplicative subgroups yield $x^n-1$, where $n$ is the subgroup&amp;#39;s order (number of elements).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This results in advantages in terms of performance and ease of understanding.&lt;&#x2F;p&gt;
&lt;p&gt;Finally, we will post a beginner’s version of STARKs soon, so stay tuned!&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Champagne SuperNova, incrementally verifiable computation</title>
        <published>2023-02-02T00:00:00+00:00</published>
        <updated>2023-02-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/champagne-supernova-incrementally-verifiable-computation-2/"/>
        <id>https://blog.lambdaclass.com/posts/champagne-supernova-incrementally-verifiable-computation-2/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/champagne-supernova-incrementally-verifiable-computation-2/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;In the lasts posts we’ve been writing about proving systems and incremental verifiable computation:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * [Pinocchio Virtual Machine: Nearly Practical Verifiable Computation](&#x2F;pinocchio-virtual-machine-nearly-practical-verifiable-computation&#x2F;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * [Decentralized private computation: ZEXE and VERI-ZEXE](&#x2F;decentralized-private-computations-zexe-and-veri-zexe&#x2F;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * [Incrementally verifiable computation: NOVA](&#x2F;incrementally-verifiable-computation-nova&#x2F;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Incremental proof systems offer some advantages over conventional proving systems:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * They do not require static bounds on loop iterations, making them better suited for programs with dynamic flow control.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * They require minimal memory overhead, as the prover only needs space proportional to the necessary space to perform the step instead of storing the whole computation trace.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * They are well suited for the distribution and parallelization of proof generation.  &lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The prover can run the program, keeping track of the input and output variables and state changes, and then generate the proofs in parallel using CPU or GPU for each step of the computation. Better still, the proofs can be conveniently aggregated into a single one, which the verifier can check.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;incrementally-verifiable-computation-nova&#x2F;&quot;&gt;Incrementally verifiable computation&lt;&#x2F;a&gt; (IVC) offers an approach to prove the integrity of machine executions. To use ICV, we need to design a universal circuit that can perform any machine-supported instruction. At each step, we have to call this circuit. This is inconvenient since the cost of proving a step is proportional to the size of the universal circuit, even if the program only executes one of the supported instructions at a much lower cost. One way to deal with this shortcoming is by constructing virtual machines with a minimal instruction set to bound the size of the universal circuit.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1758&quot;&gt;SuperNova&lt;&#x2F;a&gt; provides a cryptographic proof system (comprising a prover and a verifier) based on a virtual machine and a program designed to run over such a machine, satisfying the following properties:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Succinctness: the size of the proof and the time to verify said proof are at most polylogarithmic in the execution time of the program.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Zero-knowledge: The proof does not reveal anything other than the correct execution of the problem.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Convenient cost profile: The cost of proving a step of the program is proportional to the size of the circuit representing such instruction.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Incremental proof generation: the prover can generate a proof for each step of the program&amp;#39;s execution independently and later combine those proofs into a single one without increasing the size of the proofs.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;SuperNova leverages folding schemes (a cryptographic primitive used previously by &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;Nova&quot;&gt;Nova&lt;&#x2F;a&gt;), using relaxed-committed &lt;a href=&quot;&#x2F;incrementally-verifiable-computation-nova&#x2F;&quot;&gt;R1CS&lt;&#x2F;a&gt;, to realize a non-uniform IVC. SuperNova is a generalization of Nova, as it supports machines with a rich instruction set (Nova was limited to one instruction). In the following sections, we will break down the different components needed for SuperNova and how to achieve non-uniform IVC.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;commitment-scheme-for-vectors&quot;&gt;Commitment scheme for vectors&lt;&#x2F;h2&gt;
&lt;p&gt;A &lt;a href=&quot;&#x2F;the-hunting-of-the-zk-snark&#x2F;&quot;&gt;commitment scheme&lt;&#x2F;a&gt; for vectors is a collection of three efficient algorithms:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Parameter generation, $\mathrm{Gen}(1^\lambda)=pp$: given a security level parameter, $\lambda$, the algorithm outputs public parameters $pp$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Commit, $\mathrm{commit}(pp,x,r)=\mathrm{cm}$: given the public parameters, a vector, and some randomness, $r$, outputs a commitment $\mathrm{cm}$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Open, $\mathrm{open}(pp,\mathrm{cm},x,r)={0,1}$: given a commitment, the vector, randomness, and public parameters, the algorithm verifies whether the commitment given corresponds to the vector $x$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The commitment scheme has to satisfy the following properties:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Binding: given a commitment $\mathrm{cm}$, it is impossible to find two $x_1$, $x_2$ such that $\mathrm{commit}(pp,x_1,r)=\mathrm{commit}(pp,x_2,r)$. Simply put, the commitment binds us to our original value $x$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Hiding: the commitment reveals nothing from $x$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The following two properties are useful in our context and satisfied by some commitment schemes, such as Pedersen’s:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Additively homomorphic: given $x_1$, $x_2$ a commitment is additively homomorphic if $\mathrm{commit}(pp,x_1+x_2,r_1+r_2)=\mathrm{commit}(pp,x_1,r_1)+\mathrm{commit}(pp,x_2,r_2)$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Succinct: the size of the commitment is much smaller than the corresponding vector (for example, $\mathrm{commit}(pp,x,r)=\mathcal{O}(\log(x))$).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;SuperNova can be instantiated with any commitment scheme satisfying the four properties above, such as Pedersen’s, KZG, or Dory.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;computational-model-of-non-uniform-ivc-nivc&quot;&gt;Computational model of non-uniform IVC (NIVC)&lt;&#x2F;h2&gt;
&lt;p&gt;We can think of the program as a collection of $n+1$ non-deterministic, polynomial time computable functions, ${f_1,f_2,…,f_n,\phi}$, where each function receives $k$ input and $k$ output variables; each $f_j$ can also take non-deterministic input. The function $\phi$ can take non-deterministic input variables and output an element $j=\phi(z=(x,w))$, choosing one of the $f_i$. Each function is represented as a quadratic rank-one constraint system (R1CS), an NP-complete problem. In IVC, the prover takes as input at step $k$ $(k,x_0,x)$ and a proof $\Pi_k$ that proves knowledge of witnesses $(w_0,w_1,…,w_{k-1})$ such that&lt;br &#x2F;&gt;
$$ x_{j+1}=F(x_j,w_j) $$&lt;br &#x2F;&gt;
for all $j=0,1,…,k$ we have $x=x_{k+1}$. In other words, given a proof that shows that the previous step has been computed correctly and the current state $x_k$, we get the next state $x_{k+1}$ and a proof $\Pi_{k+1}$ showing that we computed step $k$ correctly. In the NIVC setting, $\phi$ selects which function we are going to use,&lt;br &#x2F;&gt;
$$ x_{j+1}=F_{\phi(x_j,w_j)} (x_j,w_j) $$&lt;&#x2F;p&gt;
&lt;p&gt;At each step, SuperNova folds an R1CS instance and its witness, representing the last step of the program’s execution into a running instance (it takes two $N$-sized NP instances into a single $N$-sized NP instance). The prover uses an augmented circuit containing a verifier circuit and the circuit corresponding to the function $f_j$ being executed. The verifier circuit comprises the non-interactive folding scheme and a circuit for computing $\phi$. We will represent the augmented functions as $f^\prime_j$.&lt;&#x2F;p&gt;
&lt;p&gt;One problem with the folding scheme is that we have multiple instructions, each having its R1CS representation. We could take the path of universal circuits, but this would make us pay a high cost for many cheap instructions. In Nova, we avoided the problem since we only had one type of instruction. To deal with multiple instructions, SuperNova works with $n$ running instances $U_i$, where $U_i(j)$ attests to all previous executions of $f^\prime_j$, up to step $i-1$. Therefore, checking all $U_i$ is equivalent to checking all $i-1$ steps. Each $f^\prime_j$ takes as input $u_i$, corresponding to step $i$, and folds it to the corresponding $U_i$ instance. We can think of it as looking at the function we want to execute and performing the instance folding with the one related to the previous executions. By doing so, we pay the cost for each instruction only when it is used, at the expense of keeping more running instances and updating accordingly.&lt;&#x2F;p&gt;
&lt;p&gt;The verifier circuit corresponding to $f_j^\prime$ does the following steps:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Checks that $U_i$ and $pc_i=\phi(x_{i-1},w_{i-1})$ (the index of the function executed previously) are contained in the public output of the instance $u_i$. This enforces that the previous step produces both $U_i$ and $pc_i$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Runs the folding scheme&amp;#39;s verifier to fold an instance and updates the running instances, $U_{i+1}$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Calls $\phi(x_i,w_i)=pc_{i+1}$ to obtain the index of the following function to invoke.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;IVC is a powerful cryptographic primitive which allows us to prove the integrity of computation in an incremental fashion. This strategy is well-suited for virtual machine executions and general programs with dynamic flow control. We could achieve this by using universal circuits, but at the expense of a considerable cost for each instruction, no matter how cheap it could be. Nova introduced folding schemes, allowing one to realize IVC for a single instruction. SuperNova generalizes Nova to multiple instructions by adding a selector function $\phi$, choosing the instruction to be executed at each step. To support several instructions, SuperNova needs to maintain separate bookkeeping for each function’s execution. This construction has many exciting applications since we could realize IVC without requiring expensive arbitrary circuits.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>ZPrize: eyes on the prize</title>
        <published>2023-01-23T00:00:00+00:00</published>
        <updated>2023-01-23T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/eyes-on-the-prize/"/>
        <id>https://blog.lambdaclass.com/posts/eyes-on-the-prize/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/eyes-on-the-prize/">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h1&gt;
&lt;p&gt;This post contains a summary of different approaches to optimize multiscalar multiplication with CUDA, as presented for &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.zprize.io&#x2F;blog&#x2F;announcing-zprize-results&quot;&gt;ZPrize&lt;&#x2F;a&gt;. This is an important calculation in certain &lt;a href=&quot;&#x2F;the-hunting-of-the-zk-snark&#x2F;&quot;&gt;proving systems&lt;&#x2F;a&gt; (zk-SNARKs), where it is necessary to add lots of points over an &lt;a href=&quot;&#x2F;multiscalar-multiplication-strategies-and-challenges&#x2F;&quot;&gt;elliptic curve&lt;&#x2F;a&gt;. These large sums can be broken down into smaller ones, each of which can be calculated in parallel by a processor, making the use of CUDA ideal to speed it up considerably. A short introduction to CUDA can be found &lt;a href=&quot;&#x2F;cuda-from-scratch&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;. The results of the ZPrize are promising, leading to more than 2x speed up; and the good news does not stop there, since each solution introduces different tricks and strategies. Below you will find an overview of some solutions and the links to each repo.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;&#x2F;h2&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Goal&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Pippenger&amp;#39;s algorithm&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Base algorithm&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Speakspeak&amp;#39;s submission&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * 6block&amp;#39;s submission&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Mike Voronov and Alex Kolganov&amp;#39;s submission&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * MatterLabs&amp;#39;s submission&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Yrrid&amp;#39;s submission&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;&#x2F;h2&gt;
&lt;p&gt;Given $n\in\mathbb{N}$, elliptic curve points $P_1, \dots, P_n$ and scalars $k_1, \dots, k_n$ in a finite field, compute $$P=\sum_{i=1}^nk_iP_i$$&lt;&#x2F;p&gt;
&lt;p&gt;where the summation is understood in terms of ordinary &lt;a href=&quot;&#x2F;what-every-developer-needs-to-know-about-elliptic-curves&#x2F;&quot;&gt;elliptic curve addition&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The number of points is related to the number of constraints needed to represent the computation (which can be larger than 100,000,000). This calculation appears, for example, when we want to compute the commitment of a polynomial \( a_0+a_1x+a_2x^2+…a_dx^d \) using the Kate-Zaverucha-Goldberg (KZG) commitment scheme.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;pippenger-algorithm&quot;&gt;Pippenger algorithm&lt;&#x2F;h2&gt;
&lt;p&gt;Let $\lambda$ be the number of bits needed to store the scalars, and let $s$ be an integer between $1$ and $\lambda$. Denote by $\lceil \lambda&#x2F;s \rceil$ the ceiling of $\lambda&#x2F;s$, that is the least integer number greater than or equal to $\lambda&#x2F;s$.&lt;&#x2F;p&gt;
&lt;p&gt;Write each scalar $k_i$ in base $2^s$&lt;br &#x2F;&gt;
$$k_i = \sum_{j=0}^{\lceil \lambda&#x2F;s \rceil-1}m_{i,j}(2^{s})^j,$$&lt;br &#x2F;&gt;
where $0 \leq m_{i,j} &amp;lt; 2^s$. Then&lt;&#x2F;p&gt;
&lt;p&gt;$$\sum_{i=1}^n k_iP_i = \sum_{i=1}^n\sum_{j}m_{i,j}2^{sj}P_i = \sum_{j=0}^{\lceil \lambda&#x2F;s \rceil-1}2^{sj}(\sum_{i=1}^n m_{i,j}P_i).$$&lt;&#x2F;p&gt;
&lt;p&gt;Let us rewrite the inner sum differently. For each $1 \leq m &amp;lt; 2^s$ we can group all the terms of the inner sum that have $m_{i,j}=m$ and write&lt;&#x2F;p&gt;
&lt;p&gt;$$\sum_{i=1}^nm_{i,j}P_i = \sum_{m=1}^{2^s-1}m(\sum_{i:,m_{i,j}=m}P_i)$$&lt;&#x2F;p&gt;
&lt;p&gt;For the elements $m$ such that there is no $i$ with $m_{i,j}=m$ we interpret the sum $\sum_{i:, m_{i,j}=m}P_i$ as $0.$ This last step is called &lt;em&gt;bucketing&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Putting it all together we obtain:&lt;br &#x2F;&gt;
$$\sum_{i=1}^nk_iP_i = \sum_{j=0}^{\lceil \lambda&#x2F;s \rceil - 1}2^{sj}\sum_{m=1}^{2^s-1}m\sum_{i:, m_{i,j}=m}P_i \tag{1}$$&lt;br &#x2F;&gt;
Pippenger’s algorithm consists of computing the sums above starting from the innermost sum:&lt;&#x2F;p&gt;
&lt;p&gt;(1) For each $j$ and $m$ compute $B_{j,m} := \sum_{i:,m_{i,j}=m}P_i$.&lt;&#x2F;p&gt;
&lt;p&gt;(2) For each $j$ compute $G_j := \sum mB_{j,m}$ as follows. For all $1 \leq m &amp;lt; 2^s$ compute all the partial sums in descending order of the indices&lt;br &#x2F;&gt;
$$S_{j,m} = B_{j,2^{s-1}} + \cdots + B_{j,m}.$$&lt;br &#x2F;&gt;
Then compute the sum of the partial sums $S_{j,1} + \cdots + S_{j,2^s-1}$. This is equal to the sum $G_j$ we want.&lt;&#x2F;p&gt;
&lt;p&gt;(3) Compute&lt;&#x2F;p&gt;
&lt;p&gt;$$\sum_{j=0}^{\lceil \lambda&#x2F;s \rceil-1}2^{sj}G_j.$$&lt;&#x2F;p&gt;
&lt;p&gt;In pseudocode (extracted from &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1321.pdf&quot;&gt;this paper&lt;&#x2F;a&gt;):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;vXGaugg.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;base-algorithm&quot;&gt;Base algorithm&lt;&#x2F;h2&gt;
&lt;p&gt;The implementation is in &lt;code&gt;algorithms&#x2F;src&#x2F;msm&#x2F;variable_base&#x2F;&lt;&#x2F;code&gt;. It is specific to the BLS12-377 curve. For this curve, we have $\lambda=253$.&lt;&#x2F;p&gt;
&lt;p&gt;Aleo uses Pippenger’s algorithm with $s=1$. Equation $(1)$ reduces to&lt;br &#x2F;&gt;
$$\sum_{i=1}^nk_iP_i = \sum_{j=0}^{\lambda - 1}2^{j}\sum_{i:, 1=m_{i,j}}P_i,$$&lt;br &#x2F;&gt;
where $m_{i,j}$ are defined as before, but in this particular case $m_{i,j}$ coincides with the $j$-th bit of $k_i$.&lt;br &#x2F;&gt;
Step 1 of Pippenger’s algorithm is trivial for this particular choice of $s$ and we get $G_j = B_{j,1}.$&lt;&#x2F;p&gt;
&lt;h4 id=&quot;parallelization-strategy&quot;&gt;Parallelization strategy&lt;&#x2F;h4&gt;
&lt;p&gt;CUDA parallelization is only used to modify step 2 as follows.&lt;&#x2F;p&gt;
&lt;p&gt;(2) The goal is to compute $G_{j} = \sum_{i:, 1=m_{i,j}}P_i$ for all $j$. For that, the following steps are performed.&lt;&#x2F;p&gt;
&lt;p&gt;(2.a) First, compute&lt;&#x2F;p&gt;
&lt;p&gt;$$G_{j, a} = \sum_{\substack{i:, 1=m_{i,j},\ 128a\leq i &amp;lt; 128(a+1)}}P_i$$&lt;&#x2F;p&gt;
&lt;p&gt;for all $0 \leq a &amp;lt; \lceil n&#x2F;128 \rceil$ and all $j$ in parallel. That is done using $\lambda * \lceil n&#x2F;128 \rceil$ threads.&lt;&#x2F;p&gt;
&lt;p&gt;(2.b) Then for each $j$ compute $G_{j}$ by adding $G_{j,a}$ for all $a$. Each $j$ gets its thread and so this step requires $\lambda$ steps. Each thread adds $\lceil n&#x2F;128\rceil$ elliptic curve points.&lt;&#x2F;p&gt;
&lt;p&gt;Once all the $G_j$ are computed the rest of the Pippenger algorithm is executed in the CPU as in the previous section.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;speakspeak&quot;&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;z-prize&#x2F;2022-entries&#x2F;tree&#x2F;main&#x2F;open-division&#x2F;prize1-msm&#x2F;prize1a-msm-gpu&#x2F;speakspeak&quot;&gt;Speakspeak&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The article is “cuZK: Accelerating Zero-Knowledge Proof with A Faster Parallel Multi-Scalar Multiplication Algorithm on GPUs” and can be found &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1321.pdf&quot;&gt;here&lt;&#x2F;a&gt;. There are differences between what the paper describes and the actual implementation in the ZPrize submission.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;parallelization-strategy-described-in-the-paper&quot;&gt;Parallelization strategy described in the paper&lt;&#x2F;h4&gt;
&lt;p&gt;The strategy here is to change steps 1 and 2 of Pippenger’s algorithm to leverage GPU parallelization.&lt;&#x2F;p&gt;
&lt;p&gt;We use the notation introduced in the &lt;strong&gt;Pippenger’s algorithm&lt;&#x2F;strong&gt; section. Let $t$ be the number of threads to be used.&lt;&#x2F;p&gt;
&lt;p&gt;(1) compute $B_{j,m}$ as follows. For each $1 \leq j &amp;lt; \lceil \lambda &#x2F; s \rceil$:&lt;&#x2F;p&gt;
&lt;p&gt;(1.a) compute $m_{i,j}$ in parallel for all $i$ using all $t$ threads.&lt;&#x2F;p&gt;
&lt;p&gt;(1.b) For each $0\leq l &amp;lt; t$ compute&lt;&#x2F;p&gt;
&lt;p&gt;$$B_{j,m,l} := \sum_{\substack{i \text{ such that} \ ,m_{i,j}=m \ i \equiv l \text{ mod } t}}P_i$$&lt;&#x2F;p&gt;
&lt;p&gt;Use all $t$ threads for it.&lt;&#x2F;p&gt;
&lt;p&gt;(1.c) Let $M^{(j)}$ be the matrix with elliptic curve point entries such that $M_{m, l}^{(j)} = B_{j,m,l}$. This is a sparse matrix. Compute $B_{j,m} = M^{(j)}\cdot 1_t$, where $1_t$ is the vector of length $t$ with all entries equal to $1$. This can be done using existing parallel algorithms for sparse matrix-vector multiplications. Use all $t$ threads for it.&lt;&#x2F;p&gt;
&lt;p&gt;(2) Compute all $G_j$ as follows. For all $0\leq j &amp;lt; \lceil \lambda &#x2F; s \rceil$ do the following in parallel using $t’ := t&#x2F;\lceil \lambda &#x2F; s \rceil$ threads for each one.&lt;&#x2F;p&gt;
&lt;p&gt;(2.a) For a given $j$, to compute $G_j = \sum mB_{j,m}$, split the sum in $t’$ even chunks and compute each one separately in its thread. That is, if we denote $\sigma=(2^s-1)&#x2F;t’$, for each $0 \leq \xi &amp;lt; \sigma$ compute&lt;&#x2F;p&gt;
&lt;p&gt;$$\sum_{m=\xi\sigma}^{(\xi+1)\sigma-1}mB_{j,m}.$$&lt;&#x2F;p&gt;
&lt;p&gt;This can be done in the same way with the partial sum trick as in step 2 of Pippenger. There is an additional step needed in this case because the sequence of coefficient in the sum above is $\xi\sigma, \xi\sigma+1, \dots,$ instead of $1, 2, 3,\dots$. But that is easily fixed by adding $(\xi\sigma-1)$ times the largest partial sum.&lt;&#x2F;p&gt;
&lt;p&gt;(2.b) Add all the chunks of the previous step. The result is $G_j$.&lt;&#x2F;p&gt;
&lt;p&gt;Finally compute step 3 as in Pippenger.&lt;&#x2F;p&gt;
&lt;p&gt;In pseudocode:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;meWCE2d.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;TdZi4gO.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;parallelization-strategy-from-the-implementation&quot;&gt;Parallelization strategy from the implementation&lt;&#x2F;h3&gt;
&lt;p&gt;The parallelization strategy in the actual code of the submission is quite simpler. There is no optimization with sparse matrix multiplications. However, there are several interesting things to note.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Only the curve BLS12-377 is supported.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The window size $s$ is chosen to be $21$. Since $\lambda = 253 = 12 * 21 + 1$, there are 13 windows, one of which has only binary scalars. This last window is treated differently from the other 12 windows. This is an odd choice given that $253 = 11 * 23$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * All the memory to store the inputs, the results, and all the partial results in between is allocated at the beginning. This needs quite a lot of memory. About 3.2GB of GPU RAM is only needed to store the scalars of all windows in the case of $2^{26}$ base points.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * For the most part (first 12 windows) kernels are launched with grids of blocks of $12$ columns and $M$ rows, where $M$ varies according to the task. Blocks on the other hand are one dimensional of size 32 and therefore warps and blocks coincide. Each column then handles computations relative to a specific window.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * There is intensive use of the `cub` [library](https:&#x2F;&#x2F;nvlabs.github.io&#x2F;cub&#x2F;index.html#sec1) for [sorting](https:&#x2F;&#x2F;nvlabs.github.io&#x2F;cub&#x2F;structcub_1_1_device_radix_sort.html), computing [run lengths](https:&#x2F;&#x2F;nvlabs.github.io&#x2F;cub&#x2F;structcub_1_1_device_run_length_encode.html), computing [cumulative sums](https:&#x2F;&#x2F;nvlabs.github.io&#x2F;cub&#x2F;structcub_1_1_device_scan.html) and [filtering](https:&#x2F;&#x2F;nvlabs.github.io&#x2F;cub&#x2F;structcub_1_1_device_select.html) lists.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Most of the code is actually inside the `sppark&#x2F;msm` directory. The original `sppark&#x2F;msm` code has been modified.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The repository includes a walkthrough of the main parts of the code. Here is a summary.&lt;&#x2F;p&gt;
&lt;p&gt;Let $n=2^{26}$ be the number of base points and let $N$ be $2 * Cores &#x2F; (12 * 32)$, where $Cores$ is the number of cores of the GPU. Kernels will be usually launched with grids of 12 x $N$ blocks of 32 threads. The factor $2$ in $N$ makes sense at least at a step where a binary reduction algorithm is used to add up all points of an array of size $12 * N *32$.&lt;&#x2F;p&gt;
&lt;p&gt;For step (1) of Pippenger.&lt;&#x2F;p&gt;
&lt;p&gt;(1.a) To compute $m_{i,j}$ for all $i,j$, a kernel with $N$ x $12$ blocks of $32$ threads are launched. All the scalars are partitioned and each thread is in charge of computing the $m_{i, j}$ for all $j$ for the coefficients $k_i$ in its partition. Partitions are of size ~ $n&#x2F;(32*N)$.&lt;&#x2F;p&gt;
&lt;p&gt;(1.b) Sequentially for each window $j$, the set of scalars $m_{i,j}$ is sorted using &lt;code&gt;cub::DeviceRadixSort::SortPairs&lt;&#x2F;code&gt;. Let us denote $m_{i, j}’$ the $i$-th scalar of window $j$ after sorting.&lt;&#x2F;p&gt;
&lt;p&gt;(1.c) Sequentially for each window $j$, the number of occurrences of each scalar $1 \leq m &amp;lt; 2^s$ in the window is computed using &lt;code&gt;cub::DeviceRunLengthEncode::Encode&lt;&#x2F;code&gt; on the previously sorted scalars $m_{i,j}’$.&lt;&#x2F;p&gt;
&lt;p&gt;(1.d) For technical reasons needed in the next step, the cumulative sum of the number of occurrences is computed using &lt;code&gt;cub::DeviceScan::InclusiveSum&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;(1.e) A kernel is launched to compute the buckets. The kernel gets a grid of size $N$ x $12$ blocks of $32$ threads. Column $j$ of the total $12$ columns handles the buckets of a window $j$. The range of indexes $1$ to $n$ is divided evenly into subranges and each thread handles the buckets corresponding to the unique scalars $m_{i,j}’$ with $i$ in its range. Ranges are slightly expanded and contracted for threads to get non-overlapping sets of scalars.&lt;&#x2F;p&gt;
&lt;p&gt;This concludes the computation of the buckets $B_{j, m}$ for all $0\leq j &amp;lt; 12$ and $1 \leq m &amp;lt; 2^s$&lt;&#x2F;p&gt;
&lt;p&gt;For step (2) of Pippenger.&lt;&#x2F;p&gt;
&lt;p&gt;(2.a) A kernel is launched on a grid of $N$ x $12$ blocks of $32$ threads. Each thread computes an even chunk of the sum $G_j = \sum mB_{j,m}$ just as described in the paper. As before, each column of the grid handles a different window.&lt;&#x2F;p&gt;
&lt;p&gt;(2.b) For each window $j$, its chunks are added up using a binary reduction algorithm. This effectively computes all $G_j$ for $0 \leq j &amp;lt;12$.&lt;&#x2F;p&gt;
&lt;p&gt;Then step (3) of Pippenger is performed in the CPU.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;6block-submission&quot;&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;z-prize&#x2F;2022-entries&#x2F;tree&#x2F;main&#x2F;open-division&#x2F;prize1-msm&#x2F;prize1a-msm-gpu&#x2F;6block&quot;&gt;6block submission&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The main contribution of this solution is a different approach to steps (1) and (2). If we forget about GPU parallelization for a moment, both steps are performed in a single step as follows.&lt;&#x2F;p&gt;
&lt;p&gt;(1’) For each window $j$, first sort all scalars $m_{i, j}$. Denote by $m_{i ,j}‘$ and $P_{i}’$ the sorted list of scalars and points respectively. For each $i$ from $n$ to $1$, where $n$ is the number of base points, compute&lt;&#x2F;p&gt;
&lt;p&gt;$$\begin{aligned} t_{i-1} &amp;amp;:= t_{i} + P_{i}’ \\ s_{i-1} &amp;amp;:= (m_{i, j}’ - m_{i-1, j}’)t_i + s_i\end{aligned}$$&lt;&#x2F;p&gt;
&lt;p&gt;with $t_n = P_n’$ and $s_n = \mathcal O$. Then $G_j$ is equal to $m_{0,j}’t_0 + s_0$. The rest of the approach is the same as in Pippenger.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;parallelization-strategy-1&quot;&gt;Parallelization strategy.&lt;&#x2F;h3&gt;
&lt;p&gt;Let $n$ be the number of base points $2^{26}$. The window size used is $21$. The $253$ bits are grouped in $11$ windows of size $21$ and an additional window of size $22$.&lt;&#x2F;p&gt;
&lt;p&gt;(1’.a) A kernel is launched on a one-dimensional grid of one-dimensional blocks of at most 128 threads. The exact size of the blocks is computed using the CUDA occupancy calculation function &lt;code&gt;cudaOccupancyMaxPotentialBlockSize&lt;&#x2F;code&gt;. The total number of threads equals the number of points. Each thread is then in charge of computing all the $m_{i, j}$ for a single scalar $k_i$.&lt;&#x2F;p&gt;
&lt;p&gt;The following steps are performed sequentially for each window $j$.&lt;&#x2F;p&gt;
&lt;p&gt;(1’.b) Having fixed $j$, the list $(m_{i, j})&lt;em&gt;{i=1}^n$ is sorted &lt;em&gt;.&lt;&#x2F;em&gt; The &lt;code&gt;cub&lt;&#x2F;code&gt; function &lt;code&gt;cub::DeviceRadixSort::SortPairs&lt;&#x2F;code&gt; is used for this. This function sorts key-value pairs. In this case, the pairs sorted are $(m&lt;&#x2F;em&gt;{i,j}, i)$, to keep track of which base point corresponds to which scalar in the sorted list. Let us denote by $m_{i, j}^\prime$ and $P_i^\prime$ the sorted list of scalars and points respectively.&lt;&#x2F;p&gt;
&lt;p&gt;(1’.c) Then a kernel is launched on a one-dimensional grid of one-dimensional blocks of at most 128 threads. Again the exact size of the blocks is computed using &lt;code&gt;cudaOccupancyMaxPotentialBlockSize&lt;&#x2F;code&gt;. The range $1$ to $n$ is split evenly and each thread is in charge of computing the sum $\sum m_{i, j}‘P_i’$ for $i$ in its range. This is done by computing $s_0$ and $t_0$ as described above. This produces a certain number of partial results (one for each thread) that we denote here by $B_{k, j}$. The sum of all these elements for all $k$ equals $G_j$.&lt;&#x2F;p&gt;
&lt;p&gt;(1’.d) The results $B_{k, j}$ for all $k$ are copied to the CPU and added sequentially to get $G_j$. Then, this is doubled $21$ times to get $2^{21}G_j$ (except for the last window where $2^{22}G_{12}$ is computed). While this happens in the CPU, steps (1’.b) and (1’.c) are handled in the GPU for the subsequent window.&lt;&#x2F;p&gt;
&lt;p&gt;Once all windows have been handled, the final sum is performed in the CPU.&lt;&#x2F;p&gt;
&lt;p&gt;A few interesting things to note about this solution.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The code is very clear. It uses OOP and many c++11 features and standard libraries.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * A naive implementation of the kernel launched in step (1&amp;#39;c) could severely suffer from warp divergence. This is because there is a lot of branching in the construction of $t_i$ and $s_i$. For example, if one $m_{i, j}&amp;#39;$ is equal to $m_{i-1,j}&amp;#39;$ then nothing has to be done to compute $s_i$. To overcome this issue, each thread fills up a buffer of max size $10$ with the non-zero differences $m_{i, j} - $m_{i-1, j}$ it encounters. All the elliptic curve operations are postponed until one of the threads in the warp fills out its buffer. At this point, all the threads in the warp flush their pending elliptic curve operations. To do this the warp vote function `__any_sync` is used (see [here](https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;cuda-c-programming-guide&#x2F;index.html#warp-vote-functions)).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;mike-voronov-and-alex-kolganov&quot;&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;z-prize&#x2F;2022-entries&#x2F;tree&#x2F;main&#x2F;open-division&#x2F;prize1-msm&#x2F;prize1a-msm-gpu&#x2F;mikevoronov&quot;&gt;Mike Voronov and Alex Kolganov&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Mainly, this submission improves over the baseline using signed scalars to reduce the number of buckets in step (1) of Pippenger (more details below). Although it also claims to use a careful tiling of threads to compute the partial sums in step (2) in Pippenger’s algorithm in parallel, there is little documentation about it.&lt;&#x2F;p&gt;
&lt;p&gt;The main contribution is in step (1). This solution uses a window size $s=23$.&lt;&#x2F;p&gt;
&lt;p&gt;(1.a) To compute $m_{i, j}$ a one-dimensional grid of one-dimensional blocks of size $256$ is launched. The total number of threads equals the number of base points, which is $2^{26}$. Each thread is in charge of computing the subscalars $m_{i, j}$ for a single $k_i$. Since the window size is $2^{23}$, all the subscalars $m_{i, j}$ satisfy $0 \leq m_{i, j} &amp;lt; 2^{23}$. If a subscalar is $m_{i, j}$ turns out to be bigger than $2^{22}$, then $2^{23}$ is substracted from it reducing it to the range $-2^{22} \leq m_{i, j} &amp;lt; 0$ and $2^{23}$ is carried over to the next window. The sign of the negative subscalar is transferred to the base point, given that it is cheap to negate elliptic curve points. As a consequence, they end up with subscalars in the range $0 \leq m_{i, j} &amp;lt; 2^{22}$ and possibly an additional window in case the last window needs to carry scalars.&lt;&#x2F;p&gt;
&lt;p&gt;Windows are then separated into two groups: The windows with odd and even indexes. Windows are handled in an asynchronous way and it is possible to handle more than two, the &lt;code&gt;config.numAsync&lt;&#x2F;code&gt; variable manages stream count. But for A40, two streams are enough to utilize all compute resources. The only exception is the last window which is treated separately in the CPU since it is the overflow window of the previous step and it is therefore much smaller. Each group gets its stream of threads and traverses its windows sequentially to compute the buckets and the final window sum $G_j$ as follows.&lt;&#x2F;p&gt;
&lt;p&gt;(1.b) For each window $j$, it sorts the subscalars $m_{i, j}$ and precomputes the starting and ending indexes of the occurrences of each subscalar in the sorted list, along with the number of occurrences of each one. This is done using the &lt;code&gt;thrust::sort_by_key&lt;&#x2F;code&gt; and &lt;code&gt;thrust::inclusive_scan&lt;&#x2F;code&gt; functions from the &lt;code&gt;thrust&lt;&#x2F;code&gt; library. It then launches a kernel with a one-dimensional grid of one-dimensional blocks of size $32$ to compute the buckets using the above pre-computed information.&lt;&#x2F;p&gt;
&lt;p&gt;(2.a) All windows are computed in parallel in different streams (two streams were used, but it is possible to use more, depending on GPU memory).&lt;br &#x2F;&gt;
(2.b) The buckets are then sorted, such that the buckets with the most points are run first. This allows the GPU warps to run convergent workloads and minimizes the tail effect. This solution writes custom algorithms to achieve this.&lt;&#x2F;p&gt;
&lt;p&gt;Then step (3) of Pippenger is performed in the CPU.&lt;&#x2F;p&gt;
&lt;p&gt;Other things to note from this solution.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The use of the `thrust` library.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * It always uses one-dimensional grids of one-dimensional blocks of sizes either $256$ or $32$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Two streams are used to even and odd windows in step (1.b) in parallel. This is because two streams are enough to utilize all computational resources.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * It looks like it was developed and run on a memory-constrained GPU card. In step (1.b) each group reuses the allocated arrays to store the buckets. It also reuses allocated arrays for different unrelated intermediate steps in the computation of the sorted lists of subscalars and the indexes associated with the first and last occurrences of each one.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Tested ideas that didn’t work. From the README of the submission:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * NAF - signed scalars are better because NAF reduces the number of buckets 2&#x2F;3 times, but signed scalars in 2 times&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * NAF + signed scalars - the main drawback of this variant is that a count of sums on the 2nd step twice more&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Karatsuba multiplication + Barrett reduction - turned out that the CIOS baseline Montgomery is better&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Affine summation + the Montgomery trick - turned out to be slower than the baseline summation&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;matterlabs&quot;&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;z-prize&#x2F;2022-entries&#x2F;tree&#x2F;main&#x2F;open-division&#x2F;prize1-msm&#x2F;prize1a-msm-gpu&#x2F;matter-labs&quot;&gt;MatterLabs&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This solution precomputes $2^{69j}P_i$ for each input point (this is different from what is stated in the documentation), $P_i$, and $j=0, 1, 2, 3$. These are all points in the EC. We can rewrite the first sum in this way:&lt;&#x2F;p&gt;
&lt;p&gt;$$ \sum_{i=1}^n \sum_{j=0}^3 k_{ij}2^{69j} P_i$$&lt;&#x2F;p&gt;
&lt;p&gt;where each $k_{ij}&amp;lt;2^{69}$.&lt;&#x2F;p&gt;
&lt;p&gt;Since each $2^{69j}P_i$ belongs to the EC and is already computed we can rewrite the sum as $\sum_{m=1}^{3n}k_m P_m$ (for other $k$s and other $P$s) where each $k_{m}&amp;lt;2^{69}$. This allows us to split each $k_m$ into three 23-bit windows for Pippenger’s algorithm.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;code&gt;Arkworks&lt;&#x2F;code&gt; library is used to represent finite fields, elliptic curves, and big integers in the tests. However, for the MSM algorithm itself, these structures are implemented by the authors. They used optimized versions of the operations to run them on the GPU when running device code.&lt;&#x2F;p&gt;
&lt;p&gt;A new library is developed called &lt;code&gt;Bellman-CUDA&lt;&#x2F;code&gt;. It’s used to make operations on finite fields, sort (using &lt;code&gt;CUB&lt;&#x2F;code&gt; utilities), run length-encoding (using &lt;code&gt;CUB&lt;&#x2F;code&gt; as well), etc. taking advantage of the GPU. The goal of this is probably to replace in the future the calls to &lt;code&gt;CUB&lt;&#x2F;code&gt; with more efficient algorithms.&lt;&#x2F;p&gt;
&lt;p&gt;The windows are processed in smaller parts. The first chunk of all the windows is processed first, then the second chunk of all windows, etc. This allows processing while other scalar parts are still in asynchronous transfer from the host to the device memory.&lt;&#x2F;p&gt;
&lt;p&gt;For each window chunk, a tuple index for each bucket is generated in parallel: $(i, j)$ where $i$ is the coefficient for the bucket and $j$ is the EC point in that bucket. These are sorted (in parallel) according to the first component so that we have the EC points that are to be summed up in each bucket. They are then length-encoded and sorted (in parallel as well) according to the number of points that the bucket has. In this way, the buckets that have more points will be processed first to enable efficient usage of the GPU hardware. After that, a list of offsets is generated (using the parallel algorithm to compute exclusive sums implemented by &lt;code&gt;CUB&lt;&#x2F;code&gt;) to know where each bucket starts and ends. For example:&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
\begin{align}&lt;br &#x2F;&gt;
2P_1+5P_2+4P_3+1P_4+2P_5 \rightarrow (2,1), (5,2), (4,3), (1,4), (2,5) \\&lt;br &#x2F;&gt;
\rightarrow [(1, 4), (2, 1), (2, 5), (4, 3), (5, 2)], [1, 2, 4, 5], [1, 2, 1, 1] \\&lt;br &#x2F;&gt;
\rightarrow [4, 1, 5, 3, 2], [0, 1, 3, 4, 5], [1, 2, 4, 5], [1, 2, 1, 1]&lt;br &#x2F;&gt;
\end{align}&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;The buckets are then aggregated in parallel.&lt;&#x2F;p&gt;
&lt;p&gt;The FF and EC routines have been optimized:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Based on Montgomery&amp;#39;s multiplication&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Minimized correction steps in the FF operations&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Use of XYZZ representation for the EC point accumulators&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Use of fast squaring&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h3 id=&quot;streams-and-memory-management&quot;&gt;Streams and memory management&lt;&#x2F;h3&gt;
&lt;p&gt;The following streams are created:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * `stream`&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * `stream_copy_scalars`&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * `stream_copy_bases`&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * `stream_copy_finished`&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * `stream_sort_a`&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * `stream_sort_b`&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The first one is the mainstream. Kernels such as &lt;code&gt;initialize_buckets&lt;&#x2F;code&gt;, &lt;code&gt;compute_bucket_indexes&lt;&#x2F;code&gt;, &lt;code&gt;run_length_encode&lt;&#x2F;code&gt;, &lt;code&gt;exclusive_sum&lt;&#x2F;code&gt;, and &lt;code&gt;sort_pairs&lt;&#x2F;code&gt; are run in that stream.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;stream_copy_scalars&lt;&#x2F;code&gt; waits for &lt;code&gt;event_scalars_free&lt;&#x2F;code&gt;.&lt;br &#x2F;&gt;
&lt;code&gt;stream_copy_scalars&lt;&#x2F;code&gt; handles the async copying of scalars and enqueues &lt;code&gt;event_scalars_loaded&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;stream_copy_bases&lt;&#x2F;code&gt; waits for &lt;code&gt;event_scalars_loaded&lt;&#x2F;code&gt; and &lt;code&gt;event_bases_free&lt;&#x2F;code&gt;. This stream also handles the async copying of bases and queues &lt;code&gt;event_bases_loaded&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;stream&lt;&#x2F;code&gt; waits for &lt;code&gt;event_scalars_loaded&lt;&#x2F;code&gt;, handles the kernel &lt;code&gt;compute_bucket_indexes&lt;&#x2F;code&gt;, and queues &lt;code&gt;event_scalars_free&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;stream&lt;&#x2F;code&gt; handles the sorting of the indexes and the asynchronous allocation of memory for the indexes and run lengths, as well as the &lt;code&gt;exclusive_sum&lt;&#x2F;code&gt; kernel and the allocation of memory for the offsets.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;stream&lt;&#x2F;code&gt; enqueues &lt;code&gt;event_sort_inputs_ready&lt;&#x2F;code&gt;. &lt;code&gt;stream_sort_a&lt;&#x2F;code&gt; and &lt;code&gt;stream_sort_b&lt;&#x2F;code&gt; wait on that event to handle the sorting of the pairs on the GPU.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;stream_sort_a&lt;&#x2F;code&gt; enqueues &lt;code&gt;event_sort_a&lt;&#x2F;code&gt; and &lt;code&gt;stream_sort_b&lt;&#x2F;code&gt; enqueues &lt;code&gt;event_sort_b&lt;&#x2F;code&gt;. &lt;code&gt;stream&lt;&#x2F;code&gt; waits on that event and also on &lt;code&gt;event_bases_loaded&lt;&#x2F;code&gt; before handling the kernel that aggregates the buckets. &lt;code&gt;stream&lt;&#x2F;code&gt; enqueues the (async) freeing of memory for the bases.&lt;&#x2F;p&gt;
&lt;p&gt;On the last loop of window chunk processing, &lt;code&gt;stream_copy_finished&lt;&#x2F;code&gt; waits for &lt;code&gt;event_scalars_loaded&lt;&#x2F;code&gt; and &lt;code&gt;event_bases_loaded&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Memory es freed and the streams (except &lt;code&gt;stream&lt;&#x2F;code&gt; that handles the bucket reduction and window splitting kernels) are destroyed.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;bucket-aggregation-algorithm&quot;&gt;Bucket aggregation algorithm&lt;&#x2F;h3&gt;
&lt;p&gt;This algorithm is used after having every bucket computed, and is the basis for a parallelization strategy to aggregate buckets. It is an alternative to the classic sum of partial sums trick in Pippenger’s. In what follows we assume every bucket has already been computed and the remaining problem is to add up all the points in every window.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;notation&quot;&gt;Notation&lt;&#x2F;h3&gt;
&lt;p&gt;Let us fix some notation. Let $W=(B_0, \dots, B_{2^{b}-1})$ be a tuple of $2^b$ elliptic curve points. Let us call such a tuple a &lt;strong&gt;$b$-bit window&lt;&#x2F;strong&gt;. To every window $W$ we associate an elliptic curve point $P_W$ defined as&lt;&#x2F;p&gt;
&lt;p&gt;$$P_W := B_1 + 2B_2 + \cdots + (2^{b}-1)B_{2^{b}-1}$$&lt;&#x2F;p&gt;
&lt;p&gt;We call a tuple of $m$ such windows $C = (W_0, \dots, W_{m-1})$ of the same length $2^{b}$ a &lt;strong&gt;window configuration&lt;&#x2F;strong&gt;. We say that the window configuration has shape $(m, b)$. Every window configuration has an associated elliptic curve point defined by&lt;&#x2F;p&gt;
&lt;p&gt;$$P_C := P_{W_0} + 2^{b}P_{W_1} + 2^{2b}P_{W_2} + \cdots + 2^{(m-1)b}P_{W_{m-1}}$$&lt;&#x2F;p&gt;
&lt;p&gt;In the context of MSM, each $B_i$ is a bucket and $P_C$ is the desired final result.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;reduction-process&quot;&gt;Reduction process&lt;&#x2F;h3&gt;
&lt;p&gt;Let us assume every bucket has already been computed and let $C$ be the corresponding window configuration. MatterLabs’ solution implements an algorithm to obtain $P_C$ by iteratively reducing a window configuration $C_i$ of shape $(m, b)$ to another window configuration $C_{i+1}$ of shape $(2m, \lceil b&#x2F;2 \rceil)$. At every step, the point $P_{C_{i}}$ is not necessarily equal to $P_{C_{i+1}}$, but it can be obtained from $C_{i+1}$ by shifting some scalars. See below for the details. The process starts with a configuration $C$ of shape $(3, 23)$ and ends with a configuration $D$ of shape $(96, 1)$. At this point, $P_C$ is computed from $D$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;window-splitting&quot;&gt;Window splitting&lt;&#x2F;h3&gt;
&lt;p&gt;The reduction consists of splitting every window of a configuration. Let us describe this splitting process for a single $b$-bit window $W$. We construct from it two new $\lceil b&#x2F;2\rceil$-bit windows $\hat W_0$ and $\hat W_1$ such that&lt;&#x2F;p&gt;
&lt;p&gt;$$P_W = P_{\hat W_0} + 2^{\lceil b&#x2F;2 \rceil}P_{\hat W_1}.$$&lt;&#x2F;p&gt;
&lt;p&gt;The idea behind this construction is the following. Every component of $W$ is of the form $B_r$, where $0\leq r &amp;lt; 2^b$. We can write $r = a + b2^{k}$ where $0\leq a,b &amp;lt; 2^k$. Then $B_r$ is put into two new buckets, namely the $a$-th component of window $\hat W_0$ and the $b$-th component of window $\hat W_1$.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;case-b-even&quot;&gt;Case $b$ even:&lt;&#x2F;h4&gt;
&lt;p&gt;Write $b=2k$. Let $W$ be a $b$-bit window. Define the new $k$-bit windows $\hat W_0$ and $\hat W_{1}$ as follows.&lt;&#x2F;p&gt;
&lt;p&gt;Denote the components of $W$ by $(B_{0}, \dots, B_{2^b-1})$. Then&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
\begin{aligned}&lt;br &#x2F;&gt;
\hat W_{0} &amp;amp;:= (\sum_{i=0}^{2^{k}-1}B_{i2^k}, \sum_{i=0}^{2^{k}-1}B_{i2^k+1},\dots,\sum_{i=0}^{2^{k}-1}B_{i2^k+2^{k}-1}), \\&lt;br &#x2F;&gt;
\hat W_{1} &amp;amp;:= (\sum_{i=0}^{2^{k}-1}B_{i}, \sum_{i=0}^{2^{k}-1}B_{i + 2^k},\dots,\sum_{i=0}^{2^{k}-1}B_{i + (2^{k}-1)2^{k}})&lt;br &#x2F;&gt;
\end{aligned}&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;h4 id=&quot;case-b-odd&quot;&gt;Case $b$ odd:&lt;&#x2F;h4&gt;
&lt;p&gt;Let us write $b = 2k-1$. This case is similar to the above. As before, let $W$ be a $b$-bit window. The definition of $\hat W_0$ and $\hat W_1$ follows the same logic as before.&lt;br &#x2F;&gt;
But there is a catch. If $r$ is such that $0\leq r &amp;lt; 2^b$ and we write $r= a + b2^k$ with $0\leq a,b &amp;lt; 2^k$, then $b$ is necessarily at most $2^{k-1}$. And so the second half of the coordinates of $\hat W_{1}$ will be empty. This is because none of the buckets $B_r$ of $W_n$ will be assigned to those coordinates. And so we obtain&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
\begin{aligned}&lt;br &#x2F;&gt;
\hat W_{0} &amp;amp;:= (\sum_{i=0}^{2^{k-1}-1}B_{i2^k}, \sum_{i=0}^{2^{k-1}-1}B_{i2^k+1},\dots,\sum_{i=0}^{2^{k-1}-1}B_{i2^k+2^{k}-1}), \\&lt;br &#x2F;&gt;
\hat W_{1} &amp;amp;:= (\sum_{i=0}^{2^{k}-1}B_{i}, \sum_{i=0}^{2^{k}-1}B_{i + 2^k},\dots,\sum_{i=0}^{2^{k}-1}B_{i + (2^{k-1}-1)2^{k}}, \mathcal O,\dots, \mathcal O).&lt;br &#x2F;&gt;
\end{aligned}&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;In the above definition, there are $2^{k-1}$ coordinates with entry $\mathcal O$, the point at infinity.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;reduction-of-window-configurations-and-coefficient-shifts&quot;&gt;Reduction of window configurations and coefficient shifts&lt;&#x2F;h3&gt;
&lt;p&gt;Performing the above process on every window of a configuration $C$ we obtain a new configuration $D$ of the desired shape. We will not always have $P_C = P_D$.&lt;&#x2F;p&gt;
&lt;p&gt;Let $C=(W_0, W_1, \dots, W_n)$ be a window configuration of shape $(m, b)$. For every window $W_n$, let $\hat W_{2n}$ and $\hat W_{2n+1}$ the two $\lceil b&#x2F;2 \rceil$-bit windows obtained from splitting $W_n$. Let $D=(\hat W_0, \hat W_1, \dots, \hat W_{2m-1})$. This is a window configuration of shape $(2m, \lceil b&#x2F;2 \rceil)$.&lt;&#x2F;p&gt;
&lt;p&gt;If $b$ is even, then it is easy to see that $P_C = P_D$.&lt;&#x2F;p&gt;
&lt;p&gt;If $b$ is odd, then $P_C$ is, in general, different from $P_D$. For example, the first 2 terms of $P_C$ are $W_0 + 2^bW_1$. On the other hand, the first four terms of $P_D$ are $\hat W_0 + 2^k\hat W_1 + 2^{2k}\hat W_2 + 2^{3k}\hat W_3$. This is equal to $W_0 + 2^{2k} W_1 = W_0 + 2^{b+1}W_1$. And so the coefficient of $W_1$ in $P_D$ has an extra factor of $2$.&lt;&#x2F;p&gt;
&lt;p&gt;Nevertheless, $P_C$ is equal to&lt;&#x2F;p&gt;
&lt;p&gt;$$P_{\hat W_0} + 2^{k-f_1}P_{\hat W_1} + 2^{2k-f_2}P_{\hat W_2} + \cdots + 2^{(2m-1)k-f_{2m-1}}P_{\hat W_{2m-1}},$$&lt;br &#x2F;&gt;
where $f_i = \lfloor i&#x2F;2\rfloor$. We call these the coefficient shifts.&lt;&#x2F;p&gt;
&lt;p&gt;In general, we can define $f_i$ to be $0$ for all $i$ if $b$ is even and $f_i = \lfloor i&#x2F;2 \rfloor$ for all $i$ if $b$ is odd.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;&#x2F;h3&gt;
&lt;p&gt;We start with a window configuration $C_0$ of shape $(m, b) = (3, 23)$. Inductively for every $i$ perform the reduction step on $C_i$ to obtain a new window configuration $C_{i+1}$ and also accumulate the coefficient shifts. After $4$ steps we obtain $C_5$ of shape $(96, 1)$ and the accumulated coefficient shifts $f_i$.&lt;&#x2F;p&gt;
&lt;p&gt;From $C_5$ and and the $f_i$ we can compute $P_{C_0}$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;parallelization-strategy-2&quot;&gt;Parallelization strategy&lt;&#x2F;h3&gt;
&lt;p&gt;When splitting a window configuration of shape $(m, b)$ into one of shape $(2m, k)$, where $k=\lceil b&#x2F;2\rceil$, each new bucket is a sum of $2^k$ elements (or $2^{k-1}$ in some cases when $b$ is odd). To compute these, the following kernels are launched.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. First a kernel with $2m2^{k+l}$ threads for some $l \leq \lfloor b&#x2F;2 \rfloor$ is launched. The $2^k$ terms of the sum of each new bucket are split into $2^{l}$ even groups. Each thread then computes the sum of the terms in a group. These partial sums are computed sequentially.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. A second kernel with $2m2^{k}$ threads is launched. Each thread is in charge of a bucket. It uses a binary reduction algorithm to compute it by adding the $2^l$ partial sums obtained by the previous kernel.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;yrrid&quot;&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;z-prize&#x2F;2022-entries&#x2F;tree&#x2F;main&#x2F;open-division&#x2F;prize1-msm&#x2F;prize1a-msm-gpu&#x2F;yrrid&quot;&gt;Yrrid&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This solution precomputes $2^{2\cdot23j}P_i$ for each input point, $P_i$ and $j=1,…,6$. These are all points in the EC. We can rewrite the first sum in this way:&lt;&#x2F;p&gt;
&lt;p&gt;$$ \sum_{i=1}^n \sum_{j=0}^5 k_{ij}2^{2\cdot23j} P_i$$&lt;&#x2F;p&gt;
&lt;p&gt;where each $k_{ij}&amp;lt;2^{2\cdot23}$.&lt;&#x2F;p&gt;
&lt;p&gt;Since each $2^{46j}P_i$ belongs to the EC and is already computed we can rewrite the sum as $\sum_{m=1}^{6n}k_m P_m$ (for other $k$s and other $P$s) where each $k_{m}&amp;lt;2^{46}$. This allows us to&lt;br &#x2F;&gt;
split each $k_m$ into two 23-bit windows for Pippenger’s algorithm.&lt;&#x2F;p&gt;
&lt;p&gt;Another optimization the algorithm uses is the following: the window value has a sign bit and a 22-bit scalar value. If the scalar is large, we can negate the point and change the scalar to $s’=m - s$ where $m$ is the order of the field. The new scalar, $s’$, will have a high bit clear. This works since $s’ (-P_i) = (m - s) (-P_i) = -s -P_i = s P_i$.&lt;&#x2F;p&gt;
&lt;p&gt;The buckets are then sorted, such that the buckets with the most points are run first. This allows the GPU warps to run convergent workloads and minimizes the tail effect. This solution writes custom algorithms to achieve bucket sorting instead of using the CUB libraries.&lt;&#x2F;p&gt;
&lt;p&gt;The bucket sums are computed in parallel (assigning a thread to each bucket) using the XYZZ EC representation. The operations for this curve representation can be found &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;hyperelliptic.org&#x2F;EFD&#x2F;g1p&#x2F;auto-shortw-xyzz.html#addition-add-2008-s&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The FF and EC routines have been optimized:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Based on Montgomery&amp;#39;s multiplication&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Minimize correction steps in the FF operations&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Use an XYZZ representation for the EC point accumulators&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Use fast squaring&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;Multiscalar multiplication (MSM) is one of the key operations in many proving systems, such as Marlin or Plonk with Kate polynomial commitment schemes. Owing to the nature of the operation, we can leverage GPUs to reduce its calculation time. The ZPrize competition sought to improve the current baseline of 5.86 seconds for an MSM with \( 2^{26} \) points for the BLS12-377 curve. There were 6 different proposals, each with its unique features, based on Pippenger’s algorithm: optimizing window size, precomputation of some points (trading memory for speed), different coordinate systems for elliptic curve addition, endomorphisms, parallel reduction algorithms, point negation, non-adjacent form for integers, better finite field arithmetic. The best solutions achieved 2.52 seconds (2.3x speedup), but we think there is still more room for further optimization. Will we get below 1 second? Maybe you have the answer…&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Incrementally verifiable computation: NOVA</title>
        <published>2023-01-20T00:00:00+00:00</published>
        <updated>2023-01-20T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/incrementally-verifiable-computation-nova/"/>
        <id>https://blog.lambdaclass.com/posts/incrementally-verifiable-computation-nova/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/incrementally-verifiable-computation-nova/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;One of the current goals is to realize, in an efficient way, incrementally verifiable computation (IVC). This cryptographic primitive allows a given party to show the integrity of a given computer program’s execution by providing proof that the result of each step is correct and that all previous ones have been appropriately executed at every step. More precisely, given step \( N \), we apply a function \( F_N \) which updates the state, taking as inputs the current state \( x_N \) and a proof that asserts the correct execution of all steps \( 1,2,…N-1 \), \( \pi_{N-1} \), and outputting the new state \( x_{N+1} \) and a proof of its correct execution \( \pi_{N+1} \). IVC has many applications, such as allowing decentralized private computation (DPC), where you can delegate the execution of your programs to untrusted third parties, succinct blockchains, and verifiable delay functions.&lt;&#x2F;p&gt;
&lt;p&gt;In a previous post, we discussed the problem of DPC and two protocols related to it, &lt;a href=&quot;&#x2F;decentralized-private-computations-zexe-and-veri-zexe&#x2F;&quot;&gt;ZEXE and VERI-ZEXE&lt;&#x2F;a&gt;. ZEXE discussed the possibility of using proof-carrying data (PCD) to be able to verify arbitrary computations, but this can be pretty expensive computationally since, at each step, we need to verify the proof of the previous step, for which we need to:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Compute expensive bilinear pairing operations.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Include the arithmetic circuit of the verifier into our program, which is not a lightweight construction.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;VERI-ZEXE leveraged accumulation schemes (AS) to provide IVC. The key idea is to delay the final proof to the ledger’s validators (where we will need to compute the expensive pairing operation). At each step of the computation, the proof \( \pi_{N-1} \) is “added” to an accumulator, which is then partially verified: the prover checks that the result of the accumulation is correct, but does not compute pairing operations. We mask the group elements in the accumulator using a randomizer to ensure zero knowledge.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2021&#x2F;370.pdf&quot;&gt;Nova&lt;&#x2F;a&gt; is a new protocol proposing an alternative to realizing IVC with lightweight construction. Instead of using &lt;a href=&quot;&#x2F;the-hunting-of-the-zk-snark&#x2F;&quot;&gt;zk-SNARKs&lt;&#x2F;a&gt;, they take advantage of folding schemes, accumulating NP instances instead of SNARKs. The authors claim it results in a weaker, simpler, and more efficient scheme than those relying on succinct arguments of knowledge:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The verifier circuit is constant in size and dominated by two group scalar multiplications.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The prover&amp;#39;s work is dominated by two multiexponentiations.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The key point is that the folding acts as a deferral of proof verification until the last point: to check the correct application of \( N \) times a given function, we only need to check the folded proof for the \( N \) steps.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;folding-schemes&quot;&gt;Folding schemes&lt;&#x2F;h2&gt;
&lt;p&gt;A folding scheme is a protocol between an untrusted prover and a verifier. Each of them has an \( N- \)sized NP instance of equal size, and the prover has, in addition, witnesses for both instances (recall, in the context of zk-SNARKs that we call witness the secret inputs&#x2F;information). The protocol enables them to output a single \( N- \) sized NP instance, known as the folded instance. The folding scheme guarantees that the folded instance is satisfiable only if the original instances are valid. We call the scheme non-trivial if the verifier’s work and communication are less than those he would have if he did not participate in the folding scheme. The folding scheme reduces the satisfiability of two NP instances to just one NP instance. Some techniques exhibiting this two-to-one reduction (or some reduction) are sum check protocols, batch proving, and bulletproofs. To realize such a construction, we have to introduce relaxed (quadratic) rank-one constraint systems (relaxed R1CS).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;r1cs-and-relaxed-r1cs&quot;&gt;R1CS and relaxed R1CS&lt;&#x2F;h2&gt;
&lt;p&gt;We saw that the correct execution of a given code could be expressed as a &lt;a href=&quot;&#x2F;how-to-transform-code-into-arithmetic-circuits&#x2F;&quot;&gt;circuit satisfiability problem&lt;&#x2F;a&gt;. Circuits are equivalent to R1CS, which are systems of equations of the form:&lt;br &#x2F;&gt;
\[ Az \times Bz = Cz \]&lt;br &#x2F;&gt;
where \( A,B,C \) are sparse matrices and \( \times \) denotes component-wise product. It is quadratic because each variable in each equation has at most degree two (we can have \( z_1^2 \) but not \( z_1^4 \)). Even though R1CS are a convenient way to express circuits, they are not fully compatible with folding schemes; in other words, it is not easy to build a folding scheme on top of R1CS.&lt;&#x2F;p&gt;
&lt;p&gt;Nova works by taking incremental computations, where each step is expressed as an R1CS; the constraint system is augmented with the verification circuit, which has to assert the correctness of the execution of the previous step. However, instead of verifying the proof \( \pi_{N-1} \), Nova treats it as an instance of R1CS and folds it into a running relaxed R1CS.&lt;&#x2F;p&gt;
&lt;p&gt;A relaxed R1CS introduces an error, \( E \), and a scalar, \( u \), such that&lt;br &#x2F;&gt;
\[ Az \times Bz = uCz+E \]&lt;br &#x2F;&gt;
Note that any R1CS is also a relaxed R1CS, where \( E \) is the zero vector and \( u=1\). Relaxed R1CS retains the property that it is NP-complete, which means that we can reduce any NP problem to it.&lt;&#x2F;p&gt;
&lt;p&gt;We want the folding scheme to merge two instances of R1CS with the same matrices \( A, B, C \) into a single one. Each R1CS has its corresponding instance-witness pairs (that is, public and private data), \( z_i=(w_i,x_i) \), and we want to create a new \( z=(w,x) \) satisfying the R1CS system of equations with \( A, B, C \), such that this also implies that each \( z_i=(w_i,x_i) \) does so. One way to do this is by having the verifier select a random \( r \) and perform the following transformation:&lt;br &#x2F;&gt;
\[ z=z_1+rz_2 \]&lt;br &#x2F;&gt;
This transformation would suffice for linear systems of equations, but since the R1CS is nonlinear, we cannot apply this simple strategy. If we replace this into the R1CS&lt;br &#x2F;&gt;
\[ Az_1\times Bz_1+r(Az_1 \times Bz_2 +Az_2\times Bz_1)+r^2(A_2z_2\times B_2z_2) = Cz_1+rCz_2 \]&lt;&#x2F;p&gt;
&lt;p&gt;In the relaxed R1CS, the error term \( E \) will absorb all the cross-terms generated by introducing the linear combination, and \( u \) will take the extra \( r \) term on the right-hand side. To do so,&lt;br &#x2F;&gt;
\[ u=u_1+ru_2 \]&lt;br &#x2F;&gt;
\[ E=E_1+r(Az_1\times Bz_2+Az_2\times Bz_1-u_1Cz_2-u_2Cz_1)+r^2E_2\]&lt;br &#x2F;&gt;
and both \( u,E \) are added to the instance-witness pair. The main problem is that the prover has to send the witnesses \( w_1,w_2 \) to the verifier so that he can compute \( E \). To do this, we treat both \( E \) and \( w \) as witnesses and hide them using polynomial commitment schemes.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;polynomial-commitment-scheme&quot;&gt;Polynomial commitment scheme&lt;&#x2F;h2&gt;
&lt;p&gt;Nova uses an inner product argument (&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;dankradfeist.de&#x2F;ethereum&#x2F;2021&#x2F;07&#x2F;27&#x2F;inner-product-arguments.html&quot;&gt;IPA&lt;&#x2F;a&gt;), which relies on Pedersen commitments. These are based on the assumption that the discrete log is hard to solve and do not require a trusted setup. IPA differs from other popular commitment schemes, such as KZG, which relies on elliptic curve pairings and needs a trusted setup. Regarding proof sizes and verification times, KZG is better since IPA with Pedersen commitments requires linear work from the verifier, with proof size depending on the input (KZG’s proof and verification time are constant). However, we can work these weaknesses around in systems such as Halo.&lt;&#x2F;p&gt;
&lt;p&gt;The lightweight construction of the verifier is tied to the polynomial commitment scheme. In this case, the highest cost is two &lt;a href=&quot;&#x2F;need-for-speed-elliptic-curves-chapter&#x2F;&quot;&gt;group scalar multiplications &lt;&#x2F;a&gt;. Nova’s verifier circuit is around 20,000 constraints.&lt;&#x2F;p&gt;
&lt;p&gt;The fundamental property that the polynomial commitment scheme must satisfy is that it is additively homomorphic: given two variables \( a, b \), we say that the commitment is additively-homomorphic if \( \mathrm{cm}(a+b)=\mathrm{cm}(a)+\mathrm{cm}(b) \), where \( \mathrm{cm}(x) \) is the commitment of \( x \). Both KZG and Pedersen’s commitments fulfill this property. Using this, both the verifier’s communication and work are constant.&lt;&#x2F;p&gt;
&lt;p&gt;The other necessary property is succinctness: the commitment size must be logarithmic in the opening size. For example, if we have a degree \( n \) polynomial, its commitment should take at most \( \log(n) \) elements.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;folding-scheme-for-committed-relaxed-r1cs&quot;&gt;Folding scheme for committed relaxed R1CS&lt;&#x2F;h2&gt;
&lt;p&gt;An instance (that is, the public variables) for a committed relaxed R1CS is given by \( x \), the public input and output variables, \( u \) and the commitments to \( E \), \( \mathrm{cm}(E) \) and \( \mathrm{cm}(w) \). We can group these in the tuple \( (x,\mathrm{cm}(w),\mathrm{cm}(E),u)\). The instance is satisfied by a witness (secret variables) \( (E,r_E,w,r_w)\) if \( \mathrm{cm}(E)=\mathrm{Commit}(E,r_E)\), \( \mathrm{cm}(w)=\mathrm{Commit}(w,r_w)\) and \( Az\times Bz = uCz+E \), where \( z=(w,x,u) \). In simple words, the witness satisfies the instance if the public variables \( \mathrm{cm}(E) \) and \( \mathrm{cm}(w) \) are indeed the commitments to the private variables \( E,w \) using randomness \( r_E,r_w \), respectively and they fulfill the relaxed R1CS equations.&lt;&#x2F;p&gt;
&lt;p&gt;The prover and verifier have access to two instances of relaxed R1CS, \( (x_1,\mathrm{cm}(w_1),\mathrm{cm}(E_1),u_1)\) and \( (x_2,\mathrm{cm}(w_2),\mathrm{cm}(E_2),u_2)\). In addition, the prover has \( (E_1,r_{E1},w_1,r_{w1})\) and \( (E_2,r_{E2},w_2,r_{w2})\). The protocol proceeds as follows:&lt;&#x2F;p&gt;
&lt;p&gt;1.The prover computes \( T=Az_1\times Bz_2+Az_2\times Bz_1-u_1Cz_2-u_2Cz_1\) and sends the commitment to it, \( \mathrm{cm}(T)=\mathrm{Commit}(T,r_T) \).&lt;br &#x2F;&gt;
2. The verifier samples the random challenge, \( r \).&lt;br &#x2F;&gt;
3. The prover and verifier output the folded instance,&lt;br &#x2F;&gt;
\( \mathrm{cm}(E)=\mathrm{cm}(E_1)+r^2\mathrm{cm}(E_2)+r\mathrm{cm}(T) \)&lt;br &#x2F;&gt;
\( u=u_1+ru_2 \)&lt;br &#x2F;&gt;
\( \mathrm{cm}(w)=\mathrm{cm}(w_1)+r\mathrm{cm}(w_2) \)&lt;br &#x2F;&gt;
\( x=x_1+rx_2 \)&lt;br &#x2F;&gt;
4. The prover updates the witness&lt;br &#x2F;&gt;
\( E=E_1+rT+r^2E_2 \)&lt;br &#x2F;&gt;
\( r_E=r_{E1}+rr_T+r^2r_{E2} \)&lt;br &#x2F;&gt;
\( w=w_1+r w_2 \)&lt;br &#x2F;&gt;
\( r_w=r_{w1}+rr_{w2} \)&lt;&#x2F;p&gt;
&lt;p&gt;The protocol can be made non-interactive by using the Fiat-Shamir transformation.&lt;&#x2F;p&gt;
&lt;p&gt;Using this strategy, we can realize IVC by successively updating the parameters after folding. The prover can then use a zk-SNARK showing that he knows the valid witness \( (E,r_E,w,r_w) \) for the committed relaxed R1CS in zero knowledge, that is, without revealing its value.&lt;&#x2F;p&gt;
&lt;p&gt;The problem with using some common SNARKs is that the prover must show that he knows valid vectors whose commitments equal given values. This implies encoding a linear number of group scalar multiplications in the SNARK’s model. Therefore, we need a new construction to deal with this problem.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;polynomial-interactive-oracle-proof-piop&quot;&gt;Polynomial interactive oracle proof (PIOP)&lt;&#x2F;h2&gt;
&lt;p&gt;The PIOP is a modified version of Spartan. It is based on the sum-check protocol and multilinear polynomials. For a given function mapping bitstrings to field elements, \( f:{0,1}^n \rightarrow \mathbb{F}\), we say that \( p:{0,1}^n \rightarrow \mathbb{F} \) is a polynomial extension of \( f \) if it is a low degree polynomial satisfying \( f(x)=p(x) \) for all \( x \) in \( {0,1}^n \). We call the extension multilinear if \( p \) is a multilinear polynomial such that \( f(x)=p(x) \). Multilinear polynomials are polynomials in several variables, such that the degree of each variable is at most 1 in every term. For example, \( p(x_1,x_2,x_3)=x_1+x_1x_2x_3+x_2x_3 \) is multilinear (in each term, we have at most \( x_i \)), but \( p(x_1,x_2)=x_1^2x_2 \) is not.&lt;&#x2F;p&gt;
&lt;p&gt;The R1CS matrices, \( A,B,C \) can be thought as functions from \( {0,1}^m \times {0,1}^m\) to some finite field \( \mathbb{F_p} \) in a natural way. Therefore, we can also make multilinear extensions of them \( A_{ML}, B_{ML}, C_{ML} \), that is, \( 2\log(m) \) multilinear polynomials. Since the R1CS matrices are sparse, the corresponding multilinear polynomials are sparse (in simple words, they have few non-zero coefficients). The vectors \( E \) and \( w \) can also be interpreted as polynomials, \( E_{ML} \) and \( w_{ML} \). The vector \( z=(w,x,u) \) and \( y=(x,u) \) have also their multilinear extensions \(z_{ML},y_{ML} \). We have the following function,&lt;br &#x2F;&gt;
\[ F(t)=(\sum_y A_{ML}(t,y)z_{ML}(y))\times (\sum_y B_{ML}(t,y)z_{ML}(y))-\ (u\sum_y C_{ML}(t,y)z(y)+E_{ML}(t)) \]&lt;br &#x2F;&gt;
where we sum over all values of \( y \) in \( {0,1}^s \). We only need to check whether the following identity holds for a randomly sampled \( \tau \)&lt;br &#x2F;&gt;
\[ \sum_x g(\tau,x)F(x)=0 \]&lt;br &#x2F;&gt;
for \( x \) in \( {0,1}^s \), with \( g(x,y)=1 \) for \( x=y \) and zero otherwise. We can check that equality by applying the sum-check protocol to the polynomial \( p(t)=g(\tau,t)F(t) \)&lt;&#x2F;p&gt;
&lt;h2 id=&quot;advantages&quot;&gt;Advantages&lt;&#x2F;h2&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The verifier circuit is lightweight, with little more than 20,000 constraints.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * It does not need to perform FFT, so no special elliptic curves are required. The only condition is that it is sufficiently secure (that is, the discrete log problem must be hard).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The verification is not based on elliptic curve pairings, so expensive operations and pairing-friendly curves are unnecessary.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;Nova is a new protocol for realizing incrementally verifiable computation based on a new cryptographic primitive called a folding scheme. The key idea is to merge two instances of a given NP statement into a single one. To be able to do so, we have to make changes to the R1CS to include an error term \( E \) and a scalar \( u \) to obtain a relaxed R1CS, over which we can build an efficient folding scheme. We also need additively-homomorphic polynomial commitment schemes, such as Pedersen commitments. The resulting construction has a small verifier circuit (around 20,000 constraints in R1CS), obtaining fast proof generation and verification. This has many applications to public ledgers, verifiable delay functions, and proof aggregation.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Arithmetization schemes for ZK-SNARKs</title>
        <published>2023-01-14T00:00:00+00:00</published>
        <updated>2023-01-14T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/arithmetization-schemes-for-zk-snarks/"/>
        <id>https://blog.lambdaclass.com/posts/arithmetization-schemes-for-zk-snarks/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/arithmetization-schemes-for-zk-snarks/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;Zero-knowledge proofs (ZKP) are gaining ground thanks to their many applications in delegating computations to untrusted servers and solving the scalability issues that decentralized ledgers suffer from. ZKP allow us to prove a given computation’s validity without revealing sensitive data. One of the key advantages is that the proof is short (succinct), and its verification time is much faster than the naïve re-execution of the computation. We can exploit this in decentralized ledgers, where each node must check the correctness of the transactions. Here, the weakest devices act as bottlenecks. If we can now verify the validity of a transaction by checking a small proof (taking a few milliseconds), then the scalability problems begin to fade away. We can also make proofs showing that we executed thousands of transactions or operations by using recursive proof composition, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;proof-aggregation-schemes-snarkpack-and-aplonk&#x2F;&quot;&gt;proof aggregation&lt;&#x2F;a&gt;, or &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;incrementally-verifiable-computation-nova&#x2F;&quot;&gt;folding schemes&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To prove the validity of the computation and avoid revealing sensitive information, ZKP rely on polynomials and their properties. Polynomials are expressions of the form \( a_0+a_1x+a_2x2+a_3x3+…a_n x^n \), where the coefficients \( a_k \) are elements of some &lt;a href=&quot;&#x2F;math-survival-kit-for-developers&#x2F;&quot;&gt;ring or field&lt;&#x2F;a&gt; (for example, integers, real numbers or members of a finite field, like \( \mathbb{Z}&#x2F;7\mathbb{Z}\), the integers modulo 7). Now, to be able to use polynomials, we have to be able to express our computations in terms of them by a process known as arithmetization.&lt;&#x2F;p&gt;
&lt;p&gt;Arithmetization reduces computed statements to algebraic statements involving polynomials of a bounded degree. Arithmetization can be divided into two categories:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Circuit computations. Most SNARKs use this.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Machines computations. STARKs use this approach.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Circuit computations are better for unstructured computations and support composability with relative ease. On the other hand, machine computations are better for uniform computations and support unbounded computations.&lt;&#x2F;p&gt;
&lt;p&gt;Some operations can be easily transformed into arithmetic operations, either because they are algebraic operations over a finite field or because we can translate them with some slight changes into those. This leads to a shift in thought about what is an expensive or straightforward computation. For example, stream ciphers are efficient encryption schemes, performing XOR operations between the plaintext (the message we want to encrypt) and a keystream (a pseudorandom string of bits), which the processor can calculate very fast. However, in terms of their arithmetization and the number of equations we need to describe them (that is, the number of constraints), they are expensive operations for SNARKs. Examples of costly operations for SNARKs are bitwise operations (AND, XOR, OR), bound checks, and comparisons (because these require breaking the variable into bits).&lt;&#x2F;p&gt;
&lt;p&gt;The arithmetization adds significant overhead to the computation time. There can be nearly two orders of magnitude increase in computation time using SNARK-friendly operations and more for non-friendly operations.&lt;&#x2F;p&gt;
&lt;p&gt;Recently, many different optimizations have been presented to reduce the overhead, such as:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Lookup tables.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * SNARK-friendly cryptographic primitives (such as [Rescue](https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2020&#x2F;1143.pdf), [SAVER](https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;1270.pdf) or [Poseidon](https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;458)).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Concurrent proof generation.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Hardware acceleration (such as using GPU or FPGA).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In general, arithmetization cannot be done manually except for elementary programs. Besides, the use of naïve arithmetization can lead to significant overhead. To deal with this, dedicated compilers accepting high-level programming languages have been developed, as well as zero-knowledge virtual machines, such as CAIRO. We will examine the most popular schemes, R1CS, AIR, and plonkish arithmetization.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;r1cs&quot;&gt;R1CS&lt;&#x2F;h2&gt;
&lt;p&gt;Arithmetic circuits can be expressed as (quadratic) rank one constraint systems (R1CS). These are systems of equations, each at most quadratic in each variable, of the form&lt;br &#x2F;&gt;
\[ (\sum_k A_{ik} z_k)(\sum_k B_{ik}z_k)-(\sum_k C_{ik}z_k)=0 \]&lt;br &#x2F;&gt;
where \( A_{ik}, B_{ik}, C_{ik} \) are elements in some finite field \( \mathbb{F} \), with many of them zero. We can write down any complex computation in this way. For example, if we want to calculate \( w=x^4 \) we can express this as&lt;br &#x2F;&gt;
\( x\times x= w_1 \)&lt;br &#x2F;&gt;
\( w_1 \times w_1=w \)&lt;br &#x2F;&gt;
where we have introduced an additional variable, \( w_1 \), which we have to decide whether it will be a public or private variable. It is important to see that R1CS for describing a given computation are not unique. For example, we could have expressed the previous computation as&lt;br &#x2F;&gt;
\( x\times x= w_1 \)&lt;br &#x2F;&gt;
\( x\times w_1= w_2 \)&lt;br &#x2F;&gt;
\( x\times w_2= w \)&lt;br &#x2F;&gt;
This system is equivalent to the previous one but has one more constraint.&lt;&#x2F;p&gt;
&lt;p&gt;To implement R1CS, programs have gadgets, allowing one to construct arithmetic circuits modularly. For example, if we want to work with a boolean variable, we can have a gadget implementing the constraints, such that the variable only takes the values 0 or 1. If we call the variable \( b \), then&lt;br &#x2F;&gt;
\( b(1-b)= 0 \)&lt;br &#x2F;&gt;
If we want to perform an OR operation between \( a \) and \( b \), then the boolean gadget implements also&lt;br &#x2F;&gt;
\( a(1-a)=0 \)&lt;br &#x2F;&gt;
while the OR gadget adds&lt;br &#x2F;&gt;
\( a+b-ab=c \)&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;arkworks-rs&#x2F;snark&#x2F;tree&#x2F;master&#x2F;relations&#x2F;src&quot;&gt;Arkworks&lt;&#x2F;a&gt; library contains gadgets for basic data types and operations. Common expressions for operators and range checks can be found in the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zips.z.cash&#x2F;protocol&#x2F;protocol.pdf&quot;&gt;Zcash protocol specification&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;algebraic-intermediate-representation-air&quot;&gt;Algebraic intermediate representation (AIR)&lt;&#x2F;h2&gt;
&lt;p&gt;Algebraic Intermediate Representation (AIR) is the arithmetization procedure used by StarkWare in their virtual machine, CAIRO (CPU AIR). The AIR consists of the three following elements:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. The execution trace of the computation. This is expressed as a trace execution matrix, \\( T \\), whose rows represent the computation state at a given time point and whose columns correspond to an algebraic register tracked over all the computation steps.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Transition constraints enforce the relations between two or more rows of the trace matrix \\( T \\).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Boundary constraints enforce equalities between some cells of the execution and constant values.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The arithmetization takes place in two stages:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Generating the execution trace and the low-degree polynomial constraints.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Transforming the previous two into a single univariate polynomial.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The set of polynomial constraints is constructed so that they are all verified if and only if the execution trace is valid (that is if the trace represents a valid computation). The constraints are low-degree polynomials but are not necessarily restricted to degree \( 2 \), as in the case of R1CS.&lt;&#x2F;p&gt;
&lt;p&gt;To see how AIR works, let us look at a few examples. Suppose that we want to add all the elements in a given vector of size \( n \), \( a=(a_1,a_2,a_3,…,a_n) \). We could introduce a variable \( t \) starting at \( 0 \) and which at each step adds the value of one of the components of \( a \). The trace matrix contains two columns; the first one is given by the elements of \( a \) and the partial sums in \( t \)&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Row&lt;&#x2F;th&gt;&lt;th&gt;\( a \)&lt;&#x2F;th&gt;&lt;th&gt;\( t \)&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;\( a_1 \)&lt;&#x2F;td&gt;&lt;td&gt;\( 0 \)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;\( a_2 \)&lt;&#x2F;td&gt;&lt;td&gt;\( a_1 \)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;3&lt;&#x2F;td&gt;&lt;td&gt;\( a_3 \)&lt;&#x2F;td&gt;&lt;td&gt;\( a_1+a_2 \)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;\( a_4 \)&lt;&#x2F;td&gt;&lt;td&gt;\( a_1+a_2+a_3 \)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;\( \vdots \)&lt;&#x2F;td&gt;&lt;td&gt;\( \vdots \)&lt;&#x2F;td&gt;&lt;td&gt;\( \vdots \)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;n&lt;&#x2F;td&gt;&lt;td&gt;\( a_n \)&lt;&#x2F;td&gt;&lt;td&gt;\( \sum_k^{n-1} a_k \)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;n+1&lt;&#x2F;td&gt;&lt;td&gt;\( \sum_k a_k \)&lt;&#x2F;td&gt;&lt;td&gt;\( \sum_k a_k \)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The following polynomial constraints can summarize the correctness of the computation:&lt;br &#x2F;&gt;
\( t_1=0 \)&lt;br &#x2F;&gt;
\( t_{j+1}-t_j-a_j=0 \) for \( j=1,2,…n \)&lt;br &#x2F;&gt;
\( a_{n+1}-t_{n+1}=0 \)&lt;&#x2F;p&gt;
&lt;p&gt;The advantage, in this case, is that the polynomial equations are not constrained to degree two or less. Multiplicative inverses, \( x^{-1} \), such that \( x \times x^{-1}=1 \) can be written down in two equivalent forms:&lt;br &#x2F;&gt;
\( x^{p-2}=y \)&lt;br &#x2F;&gt;
\( x\times y -1 =0 \)&lt;br &#x2F;&gt;
The first expression uses Fermat’s little theorem and involves a gate of degree \( p-2 \), while the second has degree 2.&lt;&#x2F;p&gt;
&lt;p&gt;The procedure for AIR as used in STARKs follows these steps:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Get the execution trace.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Perform low-degree extension.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Evaluate constraints.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. Compose constraints into the compositional polynomial.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The low-degree extension works as follows:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Take each register (each column of the execution trace matrix) as evaluations of some polynomial, \\( f \\).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Interpolate the \\( f \\) over the trace domain to find its coefficients.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Evaluate \\( f \\) over a larger domain.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The easiest way to work our way around is by using the number theoretic transform (this is the finite field version of the fast Fourier transform). We need to select a finite field such that it contains the n-th roots of unity, \( w_k \), such that \( w_k^n=1 \) and \( n \) is a power of 2 (\( n=2^m \)) larger than the number of rows. To obtain all the n-th roots, we can take powers of a generator, \( \omega \), \( \omega^0=1, \omega=w_1, \omega^2=w_2,…\), etc. To perform a low-degree extension, we can increase the domain by adding the 2n-th roots of unity and take advantage of our previous evaluations (or 4n-th roots of unity, leading to a 4x blowup).&lt;&#x2F;p&gt;
&lt;p&gt;To evaluate the constraints,&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Define the algebraic relations between rows.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Reinterpret these relations as polynomials with roots at the points where the conditions hold.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Divide out roots from constraint polynomials to convert them into rational constraints.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;For example, if we have some relation between the rows, such as&lt;br &#x2F;&gt;
\( r_{k+2} = r_{k+1}^2+2r_{k} \)&lt;br &#x2F;&gt;
we can interpret this as some polynomial \( f \) and associate each step with \( x_k=\omega^k \), so&lt;br &#x2F;&gt;
\( f(x \omega^2)=(f(x \omega)^2+2 f(x)) \)&lt;br &#x2F;&gt;
The polynomial&lt;br &#x2F;&gt;
\( p(x)=f(x \omega^2)-(f(x \omega)^2+2 f(x)) \)&lt;br &#x2F;&gt;
has roots at the points \( x \) where the relation \( f \) holds.&lt;br &#x2F;&gt;
We can then take out the roots by dividing them by&lt;br &#x2F;&gt;
\( d(x)=\prod_k (x-\omega_k) \)&lt;br &#x2F;&gt;
where the product is carried out only over the values of \( k \) where the constraint holds. We get the required polynomial,&lt;br &#x2F;&gt;
\[ g(x)=\frac{p(x)}{d(x)} \]&lt;br &#x2F;&gt;
The following identity gives a practical result,&lt;br &#x2F;&gt;
\[ \prod_{k=0}^{n-1} (x-\omega^k) = x^n-1 \]&lt;br &#x2F;&gt;
So, if we know that the constraint holds on most \( \omega^k \), \( d(x) \) can be computed efficiently using that identity. For example, if \(n=256 \) and it holds for all rows, except \( k=128, 194 \), then&lt;br &#x2F;&gt;
\[ d(x) = \frac{x^n-1 }{(x-\omega^{128}) (x-\omega^{194}) }\]&lt;&#x2F;p&gt;
&lt;p&gt;For our previous relationship&lt;br &#x2F;&gt;
\( r_{k+2} = r_{k+1}^2+2r_{k} \)&lt;br &#x2F;&gt;
say that \( r_1=1, r_2=5 \) and we want to calculate until \( r=1000 \). We will use \( n=1024 \) because this is the smallest power of \( 2 \) larger than \( 1000 \). In addition to the constraints being valid for all points from \( 3 \) to \( 1000 \), we also have constraints for the two initial values:&lt;br &#x2F;&gt;
\( f(\omega^0)=1=f(1) \)&lt;br &#x2F;&gt;
\( f(\omega)=5 \)&lt;br &#x2F;&gt;
Therefore, we get some additional polynomials (if the conditions do hold):&lt;br &#x2F;&gt;
\[ p_1(x)=\frac{f(x)-1}{x-1} \]&lt;br &#x2F;&gt;
\[ p_2(x)=\frac{f(x)-5}{x-\omega} \]&lt;br &#x2F;&gt;
\[ p_3(x)=\frac{f(x \omega^2)-(f(x \omega)^2+2 f(x))}{d_3(x)} \]&lt;br &#x2F;&gt;
where&lt;br &#x2F;&gt;
\[ d_3(x) = \frac{x^{1024}-1 }{(x-1 )(x-\omega) \prod_{k = 1001}{1023}(x-\omegak) } \]&lt;br &#x2F;&gt;
We can finally obtain the compositional polynomial by taking a random linear combination of \( p_1,p_2,p_3 \):&lt;br &#x2F;&gt;
\[ P(x)=\alpha_1 p_1+\alpha_2 p_2+\alpha_3 p_3\]&lt;&#x2F;p&gt;
&lt;h2 id=&quot;plonkish-arithmetization&quot;&gt;Plonkish arithmetization&lt;&#x2F;h2&gt;
&lt;p&gt;The arithmetization used by Plonk is known as randomized algebraic intermediate representation with preprocessing (RAP, for short). TurboPlonk and UltraPlonk are restricted cases of RAP. As before, our starting point is the execution trace matrix, \( T \), consisting of \( n \) rows and \( w \) columns.&lt;&#x2F;p&gt;
&lt;p&gt;Plonk’s constraint system is written (considering two fan-in gates) as&lt;br &#x2F;&gt;
\[ q_L x_a+q_R x_b+q_O x_C+q_M x_a x_b +q_C=0 \]&lt;&#x2F;p&gt;
&lt;p&gt;This can represent the operations found in R1CS and allows for the implementation of custom gates. Plonk’s original arithmetization scheme consisted in encoding the computation trace into polynomials, for which we had to check the correctness of the wiring, that the polynomial encodes the inputs correctly, that every gate is evaluated correctly, and the output of the last gate.&lt;&#x2F;p&gt;
&lt;p&gt;A preprocessed AIR (PAIR) extends the execution trace by adding new columns, \( c_1,c_2,…c_m \) so that the new columns will participate in the constraints. These variables allow us to change the relationship between different rows in the trace matrix. For example, we could alternate the relationship between even and odd rows, performing different operations. For example, we might want to have the following:&lt;br &#x2F;&gt;
\( x_{2n} = x_{2n-1}^2 \)&lt;br &#x2F;&gt;
\( x_{2n+1} = 2\times x_{2n} \)&lt;br &#x2F;&gt;
We can encode this relationship by doing&lt;br &#x2F;&gt;
\( c_1(x_n-x_{n-1}^2)+(1-c_1)(x_n-2x_{n-1}) \)&lt;br &#x2F;&gt;
where \( c_1=1 \) in even rows and \( c_1=0 \) in odd rows. Because we can use them to choose the operation we want to perform, they are called selectors. We can use more selectors to describe complex operations, such as elliptic curve addition.&lt;&#x2F;p&gt;
&lt;p&gt;We can use the grand product check to check that two vectors \( a,b \) are permutations of each other. Given a random \( \gamma \) in the finite field \( \mathbb{F} \), the following equality should hold:&lt;br &#x2F;&gt;
\[ \prod (a_i+\gamma) = \prod (b_i+\gamma)\]&lt;br &#x2F;&gt;
Due to the Schartz-Zippel lemma, we know that the probability that two polynomials are equal at a randomly sampled value is less than \( 1-d&#x2F;\vert \mathbb{F} \vert \), where \( d \) is the degree of the polynomial and \( \vert \mathbb{F} \vert \) is the number of elements of the finite field.&lt;&#x2F;p&gt;
&lt;p&gt;To check that this operation has been carried out correctly, we can introduce one additional variable, \( v \), such that&lt;br &#x2F;&gt;
\( v_1=1 \)&lt;br &#x2F;&gt;
\( v_k=v_{k-1}\times (a_{k-1}+\gamma)&#x2F;(b_{k-1}+\gamma) \) for \( k={2,n+1} \)&lt;br &#x2F;&gt;
If the last value, \( v_{n+1} \), is equal to one, then we know, with very high probability, that the columns \( a,b \) are permutations of each other.&lt;&#x2F;p&gt;
&lt;p&gt;Plonk allows one to include lookup arguments. These help us check that a given operation between two variables \( a,b \) yielding output \( c \) is correct by looking inside a table with precomputed valid \( (a,b,c) \). To do so, we need to incorporate a table \( t \) where the rows give all possible input&#x2F;output combinations. For example, we can take \( a,b \) be 8-bit strings and provide the results of the XOR operation, \( c=a\oplus b \). This gives a total of \( 2^{16} \) combinations. To check that the result is correct, we can use a random variable, \( \beta \), and compute \( f_k=a_k+\beta b_k+\beta^2 c_k \) and \( g_k=t_{k1}+\beta t_{k2}+\beta^2 t_{k3} \), where \( t_{ki} \) are the elements of the table. We will cover these kinds of arguments in an upcoming post.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;One of the critical steps in the generation of zk-SNARKs for verifiable computation is transforming a given computer program into polynomials. This process is known as arithmetization, and we have some schemes to do it efficiently, such as R1CS, AIR, and Plonkish arithmetization. In the first one, we need gadgets to implement the data types (such as boolean, u8, and i64 variables) and their associated operations. In the case of AIR and Plonkish, we need to get the execution trace of the program, establish the relationship between the rows and interpolate polynomials. Both approaches need to be carefully implemented, as naïve ways to do so can lead to a greater number of constraints and significant overhead. Fortunately, the development of new SNARK-friendly primitives, lookup arguments, custom gates, and hardware acceleration (such as the use of GPU and FPGA) can reduce either the arithmetic complexity or increase the speed at which calculations are performed and enable shorter proving and verifying times, open the doors for many new and exciting applications in the real world.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>How to transform code into arithmetic circuits</title>
        <published>2023-01-14T00:00:00+00:00</published>
        <updated>2023-01-14T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/how-to-transform-code-into-arithmetic-circuits/"/>
        <id>https://blog.lambdaclass.com/posts/how-to-transform-code-into-arithmetic-circuits/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/how-to-transform-code-into-arithmetic-circuits/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;The use of efficient &lt;a href=&quot;&#x2F;the-hunting-of-the-zk-snark&#x2F;&quot;&gt;zk-SNARKs&lt;&#x2F;a&gt; (zero-knowledge succinct non-interactive arguments of knowledge) has given rise to many new and vital applications. For example, we can &lt;a href=&quot;&#x2F;decentralized-private-computations-zexe-and-veri-zexe&#x2F;&quot;&gt;delegate expensive computations&lt;&#x2F;a&gt; to untrusted servers and receive proof showing the integrity of the computations. This proof is short and can be verified much faster than the naïve approach of re-executing the whole calculation. How can this be possible? The key idea is that the integrity of the computation can be expressed as the solution or satisfiability of a non-deterministic polynomial (NP)-complete problem. Before we explain what NP-complete means, let’s look at an example. When you write down code in a high-level language, the compiler transforms it into machine code. It is then executed in the processor, which has dedicated circuits for performing the necessary operations. We can express any complex computation in the form of some circuit. The idea with SNARKs is that we can transform the code into an arithmetic circuit made of operations such as the addition and multiplication of integers and prove the correctness of the execution by checking that the values involved in the calculation satisfy the circuit.&lt;&#x2F;p&gt;
&lt;p&gt;An NP-complete problem is such that:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * We can verify its solution in polynomial time. We can always find the answer by executing a brute-force search over all possibilities. These conditions correspond to the class NP.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * We can use the problem to simulate any other in the NP class.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Examples of NP-complete problems are circuit satisfiability, the graph coloring problem, and the traveling salesman problem.&lt;&#x2F;p&gt;
&lt;p&gt;We don’t want to write down the circuit corresponding to a program every time we want to code something. Doing this would be like writing code in assembly language or machine code instead of using a higher-level language. To do so, we need to construct a dedicated compiler, which reads our code and transforms it into an arithmetic circuit. We will see that some operations lead to a straightforward representation as arithmetic circuits (such as the addition or multiplication of integers). In contrast, other simple functions, such as XOR, AND, or equality checks, have a more complex structure.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;arithmetic-circuits&quot;&gt;Arithmetic circuits&lt;&#x2F;h2&gt;
&lt;p&gt;An arithmetic circuit is a directed acyclic graph involving the multiplication and addition of numbers. We can think of it as evaluating some polynomial over those numbers. For example, the following circuit expresses the calculation of the following polynomial, \( p(x) = x^3 + x^2 + 1 \)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;ruVa3AS.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;br &#x2F;&gt;
We can also have circuits taking different values and representing a multivariate polynomial, such as \( p(x_1,x_2) = x_1 x_2 + x_1 + x_2^2\).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;Ky9wLuo.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Arithmetic circuits can also be expressed as rank one constraint system, such that there is a one-to-one correspondence between them.&lt;&#x2F;p&gt;
&lt;p&gt;As we mentioned, the only operations we have are addition and multiplication; operations such as division have to be simulated. For example, if we want to perform&lt;br &#x2F;&gt;
\[ a&#x2F;b=c\]&lt;br &#x2F;&gt;
we can introduce an additional variable (the multiplicative inverse of \( b \), that is, \( b^{-1}\)),&lt;br &#x2F;&gt;
\(x\times b=1 \)&lt;br &#x2F;&gt;
\(a\times x=c \)&lt;br &#x2F;&gt;
The first condition ensures that \( x \) is \( b^{-1} \), and the second performs the calculation we wanted. The arithmetic circuit would look like&lt;br &#x2F;&gt;
&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;TrjZGXD.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;br &#x2F;&gt;
We could have also worked this by remembering that the multiplicative inverse of an integer (using modular arithmetic) is \( b^{-1 } = b^{p-2} \) . However, this leads to a more complex circuit since we would have to evaluate, in general, a large power, which needs many multiplication gates, even if done efficiently (of the order of \( \log(p) \)). Therefore, when trying to express a non-native operation over arithmetic circuits, we must think about the most efficient way.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;r1cs&quot;&gt;R1CS&lt;&#x2F;h2&gt;
&lt;p&gt;A (quadratic) rank-one constrain system is a system of equations of the form:&lt;br &#x2F;&gt;
\( \left(a_{01}+\sum a_{k1} x_k\right)\left(b_{01}+\sum b_{k1} x_k\right)=\left(c_{01}+\sum c_{k1} x_k\right) \)&lt;br &#x2F;&gt;
\( \left(a_{02}+\sum a_{k2} x_k\right)\left(b_{02}+\sum b_{k2} x_k\right)=\left(c_{02}+\sum c_{k2} x_k\right) \)&lt;br &#x2F;&gt;
\( \left(a_{0n}+\sum a_{kn} x_k\right)\left(b_{0n}+\sum b_{kn} x_k\right)=\left(c_{0n}+\sum c_{kn} x_k\right) \)&lt;&#x2F;p&gt;
&lt;p&gt;The number \( n \) gives the total number of constraints in the system. We can show that any bounded computation can be expressed as an R1CS. What happens if we want to perform computations involving something like \( y^5 \)? We can use a simple approach known as flattening. We introduce new variables for the intermediate computations:&lt;br &#x2F;&gt;
\( y\times y=y_1=y^2\)&lt;br &#x2F;&gt;
\( y\times y_1=y_2=y^3 \)&lt;br &#x2F;&gt;
\( y_1 \times y_2= y_3=y^5 \)&lt;br &#x2F;&gt;
For this simple calculation, the vector \( x \) is simply \( x=(y,y_1,y_2,y_3) \). Most of the elements \( a_{ij},b_{ij},c_{ij} \) are zero. The non-zero elements are \( a_{11},b_{11},c_{11},a_{12},b_{22},c_{32},a_{23},b_{33},c_{34}\), which are all equal to one. We could also express the R1CS as&lt;br &#x2F;&gt;
\(y\times y=y_1 \)&lt;br &#x2F;&gt;
\(y_1\times y_1=y_2 \)&lt;br &#x2F;&gt;
\(y\times y_2=y_3 \)&lt;br &#x2F;&gt;
Both represent the same calculation, but the constraints look a bit different. Therefore, there can be multiple representations for a given problem.&lt;&#x2F;p&gt;
&lt;p&gt;R1CS keeps track of the values involved in the calculation and the relationships between the variables. We have a deciding function to check whether or not a given assignment of the variables \( x \) satisfies the R1CS. We have to replace the values of \( x \) into the system of equations and see that the right and left-hand sides are equal. Equivalently,&lt;br &#x2F;&gt;
\( \left(a_{01}+\sum a_{k1} x_k\right)\left(b_{01}+\sum b_{k1} x_k\right)-\left(c_{01}+\sum c_{k1} x_k\right)=0 \)&lt;br &#x2F;&gt;
\( \left(a_{02}+\sum a_{k2} x_k\right)\left(b_{02}+\sum b_{k2} x_k\right)-\left(c_{02}+\sum c_{k2} x_k\right)=0 \)&lt;br &#x2F;&gt;
\( \left(a_{0n}+\sum a_{kn} x_k\right)\left(b_{0n}+\sum b_{kn} x_k\right)-\left(c_{0n}+\sum c_{kn} x_k\right)=0 \)&lt;&#x2F;p&gt;
&lt;p&gt;One advantage of R1CS stems from its modularity. If we have two systems of constraints, \( CS_1, CS_2 \), we can obtain a new one \( CS_3 \) which has to satisfy both systems.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;compilers&quot;&gt;Compilers&lt;&#x2F;h2&gt;
&lt;p&gt;We have seen that circuits and R1CS have a modularity property, allowing us to derive more complex circuits or systems of equations by combining simpler ones. We can leverage this by developing a compiler that generates the circuits&#x2F;constraints associated with each data type and associated operations.&lt;&#x2F;p&gt;
&lt;p&gt;The native elements for arithmetic circuits are the &lt;a href=&quot;&#x2F;math-survival-kit-for-developers&#x2F;&quot;&gt;field elements&lt;&#x2F;a&gt;, that is, \( 0,1,2,3,…p \), which we can also interpret as \( -p&#x2F;2+1,-p&#x2F;2+2,…,0,1,2,…p&#x2F;2 \) and the operations \( + \) and \( \times \). Data types such as &lt;code&gt;u8&lt;&#x2F;code&gt;, &lt;code&gt;u16&lt;&#x2F;code&gt;, &lt;code&gt;u64&lt;&#x2F;code&gt;, and &lt;code&gt;i128&lt;&#x2F;code&gt; are not and have to satisfy specific properties. Likewise, we have to express their operations in terms of arithmetic circuits. For example, &lt;code&gt;u16&lt;&#x2F;code&gt; is an integer value between 0 and 65535, much smaller than the field elements’ range. If we want such a data type, we must perform a range check to ensure that the value is between 0 and 65535. This condition adds overhead since we have to add constraints to the circuit associated with the range check.&lt;&#x2F;p&gt;
&lt;p&gt;Boolean variables also face similar problems. In ordinary circuits, a boolean is directly associated with one bit, and operations between bits have been optimized for performance. If we want to represent a boolean variable, which takes as values only 0 and 1, we have to add constraints to enforce these values. One simple way to ensure this is by having the variable \( b \) satisfy the following equation&lt;br &#x2F;&gt;
\( b(1-b)=0\)&lt;br &#x2F;&gt;
The arithmetic circuit associated with this equation is shown below and displays three gates: two multiplications and one addition.&lt;br &#x2F;&gt;
&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;qGxf87H.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;If we want to calculate \( c= \neg b \), we need to know how to represent NOT in circuit form first. The following equation can represent this&lt;br &#x2F;&gt;
\[ c=1-b \]&lt;br &#x2F;&gt;
The circuit representation is,&lt;br &#x2F;&gt;
&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;CeoYeMi.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;br &#x2F;&gt;
If we do a naïve pasting of both circuits, we get&lt;br &#x2F;&gt;
&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;4z3zqbU.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;br &#x2F;&gt;
We see that there are a lot of repeated elements (such as \(1, -1, -b\). In a later stage, we could optimize the circuit not to introduce redundant elements or computations, as these only increase the proving time.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose we want to represent an integer \( k \) in its bit representation (say &lt;code&gt;u16&lt;&#x2F;code&gt;). In that case, we have 16 bits, \( b_k \), each of which has the same circuit (meaning we have 32 multiplication and 16 addition gates), plus additional checks showing the following:&lt;br &#x2F;&gt;
\[ k=\sum_{j=0}^{15} 2^jb_j \]&lt;br &#x2F;&gt;
A simple gate does not represent bitwise operations, such as AND, XOR, and NOT. If we want to perform in a naïve way \(a \oplus b \) (performing an XOR operation between two bitstrings, which is something you would typically do in a &lt;a href=&quot;&#x2F;symmetric-encryption&#x2F;&quot;&gt;stream cipher&lt;&#x2F;a&gt; such as ChaCha20), we need to represent the following:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Each bitstring.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The check that those bits represent \\( a,b \\)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The circuits for each XOR operation.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can use two solutions to avoid this shortcoming. First, instead of trying to represent each non-arithmetic operation by a combination of field operations, we can create tables that show the relations between input and outputs and check the validity of the computation by looking that the combination is in the table. For example, we could store the results of XORing all 8-bit strings in a table and then use a lookup argument to check. This way, we can reduce the number of constraints, reducing the degree of the resulting polynomials and leading to faster proof generation times.&lt;&#x2F;p&gt;
&lt;p&gt;The second solution is to use new cryptographic functions which are SNARK-friendly. We can say that SNARK-friendly primitives have a simple representation as arithmetic circuits (few constraints can represent them); they usually try to use the native operations in the field. Examples of SNARK-friendly hash functions are Poseidon and Rescue.&lt;&#x2F;p&gt;
&lt;p&gt;Circuit compilers work in phases. In the first phase, the compiler starts with the main function. It begins by replacing functions with their corresponding circuits and adding the necessary variables and the circuits associated with their data types. In the second phase, the input variables are replaced by their actual values and all the intermediate results, getting a solution to the system of constraints.&lt;&#x2F;p&gt;
&lt;p&gt;To translate code into arithmetic circuits, we can implement gadgets. These are simply elements that give the behavior of one of the building blocks of a computational problem. For example, we can implement a gadget to test the equality of two integers or one which performs the concatenation of two strings. Given the modularity property, we can glue everything together and obtain the large circuit. For example, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;arkworks-rs&quot;&gt;Arkworks&lt;&#x2F;a&gt; gives tools to transform code into R1CS using gadgets.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;The integrity of a given computation can be expressed as the satisfiability or solution of an NP-complete problem, such as arithmetic circuit satisfiability. To that end, we transform the entire computation into an arithmetic circuit, where the native elements are field elements (instead of bits), and the addition and multiplication of field elements are the natural operations in the circuit. We can equivalently express circuits as constraint systems, such as R1CS. Given the modularity property of circuits and R1CS, we can leave the transformation of code into circuits to a dedicated compiler, which takes every data type and its operations and transforms it into circuit form. All non-native data types and their operations have to be defined in terms of the native elements and operations, which makes certain operations, such as bitwise AND, XOR, NOT expensive. This translation, in turn, makes well-established cryptographic primitives expensive for zk-SNARKs, as each function adds many constraints. The development of new, SNARK-friendly primitives and lookup tables can help reduce the complexity of the circuit representation and speed up proof generation.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Decentralized private computation: ZEXE and VERI-ZEXE</title>
        <published>2023-01-13T00:00:00+00:00</published>
        <updated>2023-01-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/decentralized-private-computations-zexe-and-veri-zexe/"/>
        <id>https://blog.lambdaclass.com/posts/decentralized-private-computations-zexe-and-veri-zexe/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/decentralized-private-computations-zexe-and-veri-zexe/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;The &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2018&#x2F;962.pdf&quot;&gt;ZEXE&lt;&#x2F;a&gt; (Zero-knowledge EXEcution) protocol appeared in 2018, introducing the cryptographic primitive of decentralized private computation (DPC). It aims to solve two main drawbacks that decentralized ledgers suffer: privacy and scalability.&lt;&#x2F;p&gt;
&lt;p&gt;Let’s take the examples of Bitcoin and Ethereum. We see that the history of all transactions is public (which could leak sensitive information on your company’s suppliers, acquaintances, or the services you hire). Ethereum offers programmability but requires each node to execute every operation, where the least powerful device acts as a bottleneck. ZCash tackles the privacy problem but does not offer programmability, just private transactions. ZEXE tries to get the best of both worlds:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Privately running arbitrary programs.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Being able to run computations offline.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Providing proof of the integrity of the computations, which nodes can verify quickly.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;For an overview of the protocol, we recommend our previous post on &lt;a href=&quot;&#x2F;fully-private-applications-a-zexe-protocol&#x2F;&quot;&gt;ZEXE&lt;&#x2F;a&gt;. As a quick reminder, the protocol offers the following features:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Programmability: we can run arbitrary programs.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Fast verification: we can prove the validity of our computations by using zk-SNARKs (zero-knowledge Succinct Non-interactive ARguments of Knowledge), which offer short (succinct) proofs that verifiers can check on-chain in a few milliseconds.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Data and function privacy: the protocol hides relevant input information and functions when we execute transitions in the ledger.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The ZEXE protocol has seen several improvements since its introduction to enhance its performance. This post will analyze the differences between the original protocol and a recent proposal, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;802.pdf&quot;&gt;VERI-ZEXE&lt;&#x2F;a&gt;. The authors of VERI-ZEXE compared their protocol’s performance with the original proposal of ZEXE and its early modifications. There are no comparisons between the current improved versions of the ZEXE protocol and VERI-ZEXE.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;building-blocks&quot;&gt;Building blocks&lt;&#x2F;h2&gt;
&lt;p&gt;We mentioned that the ZEXE protocol uses zk-SNARKs, which allow us to provide proofs of integrity for given computations, which anyone can verify much faster than the naïve approach of re-execution. The highest cost of the system is related to the generation of the proof, which relies on elliptic curve operations. You can look at the basics of some SNARK systems in &lt;a href=&quot;&#x2F;the-hunting-of-the-zk-snark&#x2F;&quot;&gt;our previous post&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Modern proof systems have two main building blocks: a polynomial interactive oracle proof -PIOP- (which transforms a given computation into polynomial equations) and a polynomial commitment scheme -PCS-. We get different proving systems depending on our choices, each of which has advantages and disadvantages. Some examples of PIOPs are Marlin, PLONK (Permutations over Lagrange-bases for Oecumenical Noninteractive arguments of Knowledge) -and all its derivates- and Spartan. Among the PCS, we have KZG (Kate-Zarevucha-Goldberg), FRI (Fast Reed-Solomon Interactive Oracle Proofs of Proximity), Bulletproofs, and DARK (Diophantine ARgument of Knowledge), to name a few.&lt;&#x2F;p&gt;
&lt;p&gt;To be able to perform proofs in a fast and efficient way, we need “SNARK-friendly” cryptographic primitives and operations. A function is “SNARK-friendly” if its representation as an arithmetic circuit is small. For example, intuitive and straightforward bitwise operations such as AND and XOR have a complex circuit representation. Therefore, the cost of functions in the context of SNARKs must consider the complexity of the arithmetic circuit used to represent the operation and its variables.&lt;&#x2F;p&gt;
&lt;p&gt;A great deal of the cost in SNARK systems comes from:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Multiscalar multiplication ([MSM](&#x2F;multiscalar-multiplication-strategies-and-challenges&#x2F;)). These are operations of the form \\( Q= \sum_k a_k P_k \\), where \\( a_k \\) are numbers and \\( P_k \\) are points belonging to an elliptic curve.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Elliptic curve pairings. These are used in the verification of some systems. They involve field extensions and operations between different groups of elliptic curves.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Polynomial evaluations over non-native fields.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Fiat-Shamir transform: a hash function is needed to generate the challenges. Many well-established cryptographic primitives have complicated representations as arithmetic circuits, which makes their evaluation costly.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Research efforts are attempting to solve all of these problems. GPUs or FPGA can &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.zprize.io&#x2F;prizes&#x2F;accelerating-msm-operations-on-gpu-fpga&quot;&gt;speed up the calculation of MSM&lt;&#x2F;a&gt;. New hash functions and encryption schemes with nicer arithmetic circuits can further reduce the complexity of frequently used cryptographic primitives (for example, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;458.pdf&quot;&gt;Poseidon&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;tosc.iacr.org&#x2F;index.php&#x2F;ToSC&#x2F;article&#x2F;view&#x2F;8695&#x2F;8287&quot;&gt;Vision and Rescue&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;veri-zexe-s-choices&quot;&gt;VERI-ZEXE’s choices&lt;&#x2F;h2&gt;
&lt;p&gt;To tackle these problems, VERI-ZEXE changes the proving system and cryptographic primitives. Here are some of the main modifications:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * PLONK as PIOP. Over the last years, PLONK has seen several significant improvements, such as high-degree custom gates, the use of lookup tables, and the use of multilinear polynomials (which avoids using fast Fourier transform) (such as turboPLONK, ultraPLONK, and [hyperPLONK](https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2022&#x2F;1355.pdf)).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Lightweight verifier circuit via [accumulation scheme](https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2020&#x2F;499.pdf). The protocol moves out the pairing check from the SNARK circuit and delays the verification to the ledger&amp;#39;s validators.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Instance merging. When performing transactions, birth and death predicates of records have to be checked. Instead of verifying each predicate separately, the protocol leverages that the predicates can be taken in birth&#x2F;death pairs, resulting in a larger predicate. However, since the verification of the combined predicate has a simpler circuit representation (this means that the number of operations does not scale linearly), the overall cost is reduced.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Proof batching. We can generate and verify proofs in batches by exploiting the properties of some PCS, such as KZG. These allow the opening of \\( N \\) different commitments simultaneously, with a cost that does not scale linearly in the number of commitments (that is, you can open \\( N \\) commitments for less than the cost of \\( N \\) separate openings).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Variable base MSM via a lookup table. The MSM is carried out by combining Pippenger&amp;#39;s algorithm (which splits the scalars into blocks) with a [lookup table](https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2020&#x2F;315.pdf), reducing the cost of elliptic curve additions.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Polynomial evaluation over non-native fields. The circuits of the prover and the verifier lie in different finite fields. One way to deal with this was using two pairs of elliptic curves. VERI-ZEXE uses modular addition and multiplication with range check with lookup, resulting in a slightly more complicated circuit.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * SNARK-friendly symmetric primitives. Using collision-resistant hash functions, pseudorandom generators, and commitment schemes with a smaller circuit representation (which reduces the number of operations), resulting in less memory and time use. For example, the Fiat-Shamir transformation uses the sponge construction of the Rescue permutation.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The use of PLONK and its additions, together with simpler constructions for cryptographic primitives, results in a reduction of more than one order of magnitude in the total number of constraints, which in turn decreases the scale of the MSM multiplications and overall proving time.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;accumulation-schemes-as-and-incrementally-verifiable-computation-ivc&quot;&gt;Accumulation schemes (AS) and Incrementally verifiable computation (IVC)&lt;&#x2F;h2&gt;
&lt;p&gt;The verification of proofs requires the calculation of costly pairing operations. The original ZEXE protocol used incrementally verifiable computation to prove the satisfiability of user-defined predicates using SNARK recursion: given a computation at step \( N \), the prover would receive the state \( z_{N} \) and a proof \( \pi_{N-1} \) attesting to the correct execution of the previous step. The prover would then execute step \( N \) and generate a proof \( \pi_N \) which certifies that “the new state \( z^\prime \) is the result of the correct execution and that \( \pi_{N-1} \) is true (in other words, that the prover did the \( N-1 \) previous steps correctly)”. In this last step, the computational burden comes in: to check the proof, the verifier’s computation is embedded inside the prover’s circuit, which slows down the proof’s generation.&lt;&#x2F;p&gt;
&lt;p&gt;An accumulation scheme proceeds differently by delaying the verification of the final proof to the ledger’s validators. At each step of the calculation, the prover receives the current state and an accumulator, which is partially verified (the prover checks that the accumulation results are correct but does not calculate the elliptic curve pairing operation). The group elements in the accumulator must be masked using a randomizer, which acts as an additional witness (secret input) for the accumulator’s verifier. This masking ensures that the accumulator does not leak information on the computations being carried out,&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lookup-tables-and-efficient-modular-operations&quot;&gt;Lookup tables and efficient modular operations&lt;&#x2F;h2&gt;
&lt;p&gt;Using lookup tables for elliptic curve addition in the Pippenger algorithm and efficient operations for modular arithmetic reduces the number of PLONK constraints by a factor of 6.&lt;&#x2F;p&gt;
&lt;p&gt;The idea behind lookup tables for MSM is as follows:&lt;br &#x2F;&gt;
\[ Q= \sum_i a_i P_i \]&lt;br &#x2F;&gt;
The Pippenger algorithm splits the scalars \( a_i \) into \( m \) windows of length \( c \) (For example, a scalar is a 256-bit number, and we choose a window of 8-bits). We can write each scalar as&lt;br &#x2F;&gt;
\[ a_i= \sum_j a_{ij}2^{mj}\]&lt;br &#x2F;&gt;
where each \( a_{ij} \) is in the range \( {0,1,…,2^c-1} \). We can compute, for each point \( P_i \) all possible combinations of scalars values \( 2P_i,3P_i,4P_i,…,(2^c-1)P_i\).&lt;&#x2F;p&gt;
&lt;p&gt;We can now calculate the result \( Q_{ij}=a_{ij}P_i\) by looking at the table (which has a more straightforward description than pure elliptic curve operations) and get the results of the j-th bucket,&lt;br &#x2F;&gt;
\[ B_j = \sum_i Q_{ij} \]&lt;br &#x2F;&gt;
We can get the final result by finally adding over the \( m \) buckets,&lt;br &#x2F;&gt;
\[ Q=\sum_j B_j 2^{cj}\]&lt;&#x2F;p&gt;
&lt;h2 id=&quot;further-improvements-from-hyperplonk&quot;&gt;Further improvements from HyperPlonk?&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;xymqID7.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;VERI-ZEXE uses PLONK with lookup tables, resulting in fewer constraints and shorter proving times. Two weeks ago, HyperPLONK came out, providing linear time prover and high-degree custom gates. One of the key changes is the shift from univariate polynomials (polynomials in one variable, \(x \), such as \(a_0+a_1x+a_2x2+…a_dxd \)) to multivariate linear polynomials (polynomials in several variables, where the degree of each \( x_k \) is at most one, such as \( a_0 +a_1x_1+a_2x_2+a_{12}x_1x_2+a_{145}x_1x_4x_5 \)). This change avoids using the fast Fourier transform (FFT) for very large systems (with over \(2^{20} \) constraints), which has a superlinear cost (roughly speaking, the FFT for \( n \) points needs \( n\log(n) \) operations). Preliminary studies have shown that this new PLONK version performs better for circuits with more than 16000 constraints compared to optimized versions of the original proposal. We will cover this topic in an upcoming post.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;ZK proofs are the key to many new applications, such as decentralized finances, governance, etc. The ZEXE protocol introduced the concept of decentralized private computation, allowing users to run private applications over public ledgers. The original proposal was based on non-universal proving systems, which have efficient performance but require a new trusted setup for each program we want to run. Since then, several significant improvements in proving systems (such as Marlin and PLONK) and new “SNARK-friendly” cryptographic primitives (such as symmetric ciphers and hash functions) have been introduced, resulting in increased performance and lower computational costs. These changes allow less powerful devices to act as provers and run more complex programs.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Pinocchio Virtual Machine: Nearly Practical Verifiable Computation</title>
        <published>2023-01-13T00:00:00+00:00</published>
        <updated>2023-01-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/pinocchio-virtual-machine-nearly-practical-verifiable-computation/"/>
        <id>https://blog.lambdaclass.com/posts/pinocchio-virtual-machine-nearly-practical-verifiable-computation/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/pinocchio-virtual-machine-nearly-practical-verifiable-computation/">&lt;p&gt;At &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;lambdaclass.com&#x2F;&quot;&gt;LambdaClass&lt;&#x2F;a&gt; we set up a small research team to work on Zero Knowledge Proofs and Fully Homomorphic Encryption, who in the past few weeks implemented a virtual machine implementing the Pinocchio protocol in Rust.&lt;&#x2F;p&gt;
&lt;p&gt;You can check out the repository at &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;pinocchio_lambda_vm&quot;&gt;lambdaclass&#x2F;pinocchio_lambda_vm&lt;&#x2F;a&gt;. It was built by Mauro Toscano, Sergio Chouhy, Agustin Garassino and Diego Kingston.&lt;&#x2F;p&gt;
&lt;p&gt;If you’re in need of a team of engineers and researchers who’ve been working together for a decade in areas like distributed systems, machine learning, compilers, and cryptography, we’re your guys. Wanna chat more about it? Book a meeting with us through &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;calendly.com&#x2F;federicocarrone&quot;&gt;calendly&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;zk-SNARKs protocols can be hard to understand for newcomers. One of the first practical implementations is called Pinocchio and it’s a very good starting point for anyone trying to get their head around them. Pinocchio’s paper can be found &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2013&#x2F;279.pdf&quot;&gt;here&lt;&#x2F;a&gt;. Having a thorough understanding of its main ideas is of great value to be able to get through the newer and more sofisticated protocols.&lt;&#x2F;p&gt;
&lt;p&gt;In this post we discuss the intuition behind its inner workings and the way it is able to provide succint proofs of circuit executions.&lt;&#x2F;p&gt;
&lt;p&gt;We know that sometimes math seems more complicated and scary than it actually is. And, very often, being able to take a look at some actual code sheds light on what’s happening. So we created a companion for this blogpost: a zero-dependency rust implementation of Pinocchio for learning purposes. You can find it at &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;pinocchio_lambda_vm&quot;&gt;lambdaclass&#x2F;pinocchio_lambda_vm&lt;&#x2F;a&gt;. We encourage you to go there if you are searching for details not covered here.&lt;&#x2F;p&gt;
&lt;p&gt;So, let’s get started!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-problem-to-solve&quot;&gt;The problem to solve&lt;&#x2F;h2&gt;
&lt;p&gt;The problem that this is trying to solve is the following: someone runs some code with a number of input values. She gets the final result of the program and wants to convince others that this value is actually the output of the execution of that program for the given inputs.&lt;&#x2F;p&gt;
&lt;p&gt;The starting point of this is a process where the program of interest is translated to “arithmetic circuits”. In the context of Pinocchio these are directed graphs where the nodes represent arithmetic operations. We’ll see now with an example.&lt;&#x2F;p&gt;
&lt;p&gt;By the way, we are not discussing here how to convert the programs to circuits. Our context is the following: someone called a “prover” wants to convince others, the “verifiers”, that she executed an arithmetic circuit and got some output values out of it. Let’s see what this all means.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;circuit-execution&quot;&gt;Circuit execution&lt;&#x2F;h2&gt;
&lt;p&gt;Throughout this post, let’s consider a variant of the example circuit from the paper as a running example. The circuit is the following:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;2023&#x2F;01&#x2F;imagen.png&quot; alt=&quot;imagen&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The little squares on the top represent the input values. Choosing input values determines the output values of all the other gates. Executing the circuit means choosing some values for the input gates and filling out the rest of the values corresponding to the output of the multiplication gates. If you are wondering why we are only labeling the output of the multiplication gates, that’s normal. The answer is: it’s enough to do it this way. We’ll come back to this in a moment.&lt;&#x2F;p&gt;
&lt;p&gt;For example, suppose we are working in $\mathbb F_{p}$, for some large prime $p$. The following are two evaluations of the circuit.&lt;&#x2F;p&gt;
&lt;p&gt;Here the input values are $2$ and $3$. The output is $30$. The unique intermediate value is $6$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;2023&#x2F;01&#x2F;imagen-2.png&quot; alt=&quot;imagen-2&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Another evaluation is the following: the input values are $6$ and $4$, the output value is $240$, and the intermediate value is $24$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;2023&#x2F;01&#x2F;imagen-3.png&quot; alt=&quot;imagen-3&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;equations-that-satisfy-circuit-execution-instances&quot;&gt;Equations that satisfy circuit execution instances&lt;&#x2F;h4&gt;
&lt;p&gt;Let’s name the input values  $c_1, c_2$, the ouput value $c_3$ and the intermediate one $c_4$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;2023&#x2F;01&#x2F;imagen-5.png&quot; alt=&quot;imagen-5&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Here is a simple but important observation. It doesn’t matter which input values we choose, the resulting values will satisfy the following system of equations:&lt;br &#x2F;&gt;
$c_1c_2 = c_4$&lt;br &#x2F;&gt;
$(c_1+c_2)c_4 = c_3$&lt;br &#x2F;&gt;
Here we have one equation for every multiplication gate. Since we are not labeling the output of the addition gates, we expand them in the left and right operands of the multiplication gate. In this example, that happens only in the left operand of the second equation.&lt;br &#x2F;&gt;
Every set of values $c_1,c_2,c_3,c_4$ that satisfy those two equalities are the values corresponding to an execution instance of the program. They are the unique values corresponding to the execution of the program with input values $c_1$ and $c_2$.&lt;br &#x2F;&gt;
These equations test whether a set of values $c_1,c_2,c_3,c_4$ correspond to an execution instance.&lt;&#x2F;p&gt;
&lt;p&gt;We could have named $c_5$ the output of the addition gate and have the following larger system of equations.&lt;br &#x2F;&gt;
$c_1c_2 = c_4$&lt;br &#x2F;&gt;
$c_1+c_2 = c_5$&lt;br &#x2F;&gt;
$c_5c_4 = c_3$&lt;br &#x2F;&gt;
But it is unnecessary. And not doing so has the advantage that every equation in the resulting system of equations has the form&lt;br &#x2F;&gt;
$$(c_{i_1} + \cdots + c_{i_\alpha})(c_{j_1} + \cdots + c_{j_\beta}) = c_k$$&lt;br &#x2F;&gt;
Note that the second equation in the last system is not of this form. Having all equations of the same shape will be very convenient for the protocol. Moreover, having all the equations of that specific form will be very important, as we’ll later see. And we achieve this simply by giving variable names to the input and output values of the multiplication gates, but not addition gates. A system of equations of this form is called a Rank-1 Constraint System (R1CS).&lt;&#x2F;p&gt;
&lt;h4 id=&quot;recap&quot;&gt;Recap&lt;&#x2F;h4&gt;
&lt;p&gt;So, we started from a circuit. By choosing some input values and tracking the output values of every multiplication gate, we obtain a tuple of values $(c_1,\dots,c_N)$ that satisfy a system of equations of a specific form, called R1CS. Moreover, any solution to that system of equations corresponds to some execution of the circuit.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;naive-proof-of-execution&quot;&gt;Naive proof of execution&lt;&#x2F;h2&gt;
&lt;p&gt;In our example, if we would like to prove that we executed the circuit, we could show the values we got $c_1,c_2,c_3,c_4$. A verifier could then check that the equations hold and be sure that we executed the circuit with input values $c_1$ and $c_2$, and we got $c_3$ as a result. The problem with this is evident: the verification work is the same as executing the circuit. This is useless since we want to delegate heavy computations to untrusted servers and then have concise proof of the execution. So redoing all the work is off the table.&lt;&#x2F;p&gt;
&lt;p&gt;The idea of QAP and Pinocchio is expressing the system of equations in a more compact form as a single polynomial identity. This, together with some fundamental algebra results, will allow us to give a succinct proof of the execution. The amount of work that the verifier will have to do is always the same, regardless of the complexity of the circuit.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;pinocchio-s-idea-of-proof-of-execution&quot;&gt;Pinocchio’s idea of proof of execution&lt;&#x2F;h2&gt;
&lt;p&gt;As we will see shortly, there is a very particular and circuit dependant way of constructing a polynomial $p$ out of the values $c_1, c_2, c_3, c_4$ that encodes correct executions. Meaning that $c_1,c_2,c_3,c_4$ are valid execution instance values if and only if their associated polynomial $p$ satisfies a special property. For the circuit of the example, this property is $p$ being equal to $X(X-1)h$ for some polynomial $h$. We’ll see shortly where this property comes from. So, the protocol’s idea is that the prover constructs the polynomial $p$ and convinces the verifier that there exists $h$ such that $p=X(X-1)h$.&lt;&#x2F;p&gt;
&lt;p&gt;The protocol will be something like this.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. In a setup phase a random point $s$ in $\mathbb F_p$ is chosen and the value $s(s-1)$ is precomputed.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. The prover executes the circuit with public input values $c_1$ and $c_2$ and obtains $c_3,c_4$. She constructs the polynomial $p$ out of these values and computes $h$ such that $p = X(X-1)h$. She evaluates $a = p(s)$ and $b = h(s)$ and sends $a$ and $b$ to the verifier.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. The verifier checks that $a = s(s-1)b$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We will have to address many disturbing things for this to work. For example:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. How is $p$ constructed?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Why is it enough to check that $a = s(s-1)b$ to be convinced that $p = X(X-1)h$?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. How does the verifier know that the prover followed the correct recipe to construct $p$ from the values of the circuit&amp;#39;s execution?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. How does the verifier know that $a$ and $b$ are values that come from evaluating polynomials $p$ and $h$ at $s$?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    5. How does the verifier know which public input values were used to execute the circuit?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    6. What&amp;#39;s stopping the prover from simply choosing $b=1$ and $a = s(s-1)$?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    7. How is this ever going to work?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To address all these questions, the protocol gets more convoluted. But the essence of it is the steps above. The rest comes into play to guarantee that no one is cheating. We’ll cover all of these questions, and in the end, we’ll obtain the actual Pinocchio protocol.&lt;&#x2F;p&gt;
&lt;p&gt;For more complex circuits, more complex polynomials $p$ are involved, and the condition $p = X(X-1)h$ is replaced with $p = X(X-1)(X-2)\cdots(X-k)h$ for a number $k$ that depends on the number of multiplication gates of the circuit. The idea is still the same. More importantly, the number of checks the verifier has to perform is independent of the circuit!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;1-polynomials-to-express-correct-circuit-executions&quot;&gt;1. Polynomials to express correct circuit executions&lt;&#x2F;h2&gt;
&lt;p&gt;Let’s start by showing how we construct the polynomial $p$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;from-families-of-polynomials-to-systems-of-equations&quot;&gt;From families of polynomials to systems of equations&lt;&#x2F;h3&gt;
&lt;p&gt;Let’s forget about our examples and circuits and start by playing around with random polynomials. Let’s take $v_1$ as&lt;br &#x2F;&gt;
$$v_1 = X, v_2 = X + 1, v_3 = X + 2$$&lt;&#x2F;p&gt;
&lt;p&gt;$w_1$ as&lt;br &#x2F;&gt;
$$w_1 = 2X + 1, w_2 = 2X + 2, w_3 = 2X + 3$$&lt;&#x2F;p&gt;
&lt;p&gt;and $y_1$ as&lt;br &#x2F;&gt;
$$y_1 = X, y_2 = -X + 1, y_3 = 0$$&lt;&#x2F;p&gt;
&lt;p&gt;Let $c_1,c_2,c_3$ be three elements of $\mathbb F_p$. Out of them, construct the following polynomial&lt;br &#x2F;&gt;
$$p = (c_1v_1 + c_2v_2 + c_3v_3)(c_1w_1 + c_2w_2 + c_3w_3) - (c_1y_1 + c_2y_2 + c_3y_3)$$&lt;br &#x2F;&gt;
We could ask ourselves the following (seemingly unrelated to anything) question: which values $c_1, c_2, c_3$ are such that $p$ has roots at $0$ and $1$? To figure it out, we can evaluate at $0$ and $1$. Precisely, $p(0) = 0$ and $p(1) = 0$ mean&lt;&#x2F;p&gt;
&lt;p&gt;$0 = p(0) = (c_2+2c_3)(c_1 + 2c_2 + 3c_3) - c_2$&lt;br &#x2F;&gt;
$0 = p(1) = (c_1 + 2c_2 + 3c_3)(3c_1 + 4c_2 + 5c_3) - c_3$&lt;&#x2F;p&gt;
&lt;p&gt;$c_1,c_2$, and $c_3$ are such that the polynomial $p$ satisfies $p(0) = 0$ and $p(1) = 0$ if and only if they solve the following system of equations&lt;&#x2F;p&gt;
&lt;p&gt;$(c_2+2c_3)(c_1 + 2c_2 + 3c_3) = c_2$&lt;br &#x2F;&gt;
$(c_1 + 2c_2 + 3c_3)(3c_1 + 4c_2 + 5c_3) = c_3$&lt;&#x2F;p&gt;
&lt;p&gt;On the other hand, the basic theory of polynomials says that $0$ and $1$ are roots of a polynomial $p$ if and only if $X(X-1)$ divides $p$. That is, if and only if there exists a polynomial $h$ such that&lt;br &#x2F;&gt;
$$p = X(X-1)h$$&lt;&#x2F;p&gt;
&lt;h4 id=&quot;takeaway&quot;&gt;Takeaway&lt;&#x2F;h4&gt;
&lt;p&gt;Wrapping up, we started from sets of polynomials $v_i, w_i, y_i$. These give a way to construct a polynomial $p$ out of any tuple $c_1,c_2,c_3$. The polynomial $p$ is divisible by $X(X-1)$ if and only if the values $c_1,c_2,c_3$ satisfy a system of equations.&lt;&#x2F;p&gt;
&lt;p&gt;This way of encoding systems of equations in a single polynomial identity is the fundamental trick to constructing succinct proofs.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;going-back-to-our-circuit&quot;&gt;Going back to our circuit&lt;&#x2F;h3&gt;
&lt;p&gt;In the previous section, we chose random polynomials. Therefore we got associated with them a system of equations that has nothing to do with our circuit. What we are going to do now is to carefully choose polynomials $v_i, w_i, y_i$, such that the system of equations associated with them is the R1CS of our circuit:&lt;&#x2F;p&gt;
&lt;p&gt;$c_1c_2 = c_4$&lt;br &#x2F;&gt;
$(c_1+c_2)c_4 = c_3$&lt;&#x2F;p&gt;
&lt;p&gt;Since we have four variables $c_1,c_2,c_3$, and $c_4$, we need four polynomials in each family. In other blogposts we will explain how to construct them using polynomial interpolation, but for now, here are the polynomials we want:&lt;&#x2F;p&gt;
&lt;p&gt;$v_1 = 1, v_2 = X, v_3 = 0, v_4 = 0$&lt;br &#x2F;&gt;
$w_1 = 0, w_2 = -X + 1, w_3 = 0, w_4 = X$&lt;br &#x2F;&gt;
$y_1 = 0, y_2 = 0, y_3 = X y_4 = -X + 1$&lt;&#x2F;p&gt;
&lt;p&gt;And for every $(c_1,c_2,c_3,c_4)$ we construct $p$ following this recipe:&lt;&#x2F;p&gt;
&lt;p&gt;$p = (c_1v_1 + c_2v_2 + c_3v_3 + c_4v_4)(c_1w_1 + c_2w_2 + c_3w_3 + c_4w_4) - (c_1y_1 + c_2y_2 + c_3y_3 + c_4y_4)$&lt;br &#x2F;&gt;
$= (c_1 + c_2X)(c_2(-X+1) + c_4X) - (c_3X + c_4(-X + 1))$&lt;&#x2F;p&gt;
&lt;p&gt;We have $p(0) = c_1c_2 - c4$ and $p(1) = (c_1+c_2)c4 - c3$. So $p$ is divisible by $X(X-1)$ if and only if&lt;&#x2F;p&gt;
&lt;p&gt;$c_1c_2 = c_4$&lt;br &#x2F;&gt;
$(c_1+c_2)c_4 = c_3$&lt;&#x2F;p&gt;
&lt;p&gt;Let’s plug in some actual values and see what $p$ looks like. Let’s start with the tuples of our execution examples. For our first example, we have $(c_1,c_2,c_3,c_4) = (2, 3, 30, 6)$. We get&lt;&#x2F;p&gt;
&lt;p&gt;$p = (2 + 3X)(3(-X+1) + 6X) - (30X + 6(-X+1))$&lt;br &#x2F;&gt;
$ = (2 + 3X)(3X + 3) - (24X + 6)$&lt;br &#x2F;&gt;
$ = 6X + 6 + 9X^2 + 9X - 24X - 6$&lt;br &#x2F;&gt;
$ = 9X^2 -9X$&lt;br &#x2F;&gt;
$ = 9X(X-1)$&lt;&#x2F;p&gt;
&lt;p&gt;For our second example, $(c_1,c_2,c_3,c_4) = (6, 4, 240, 24)$. We obtain&lt;&#x2F;p&gt;
&lt;p&gt;$p = (6 + 4X)(4(-X+1) + 24X) - (240X + 24(-X+1))$&lt;br &#x2F;&gt;
$ = (6 + 4X)(20X + 4) - (216X + 24)$&lt;br &#x2F;&gt;
$ = 120X + 24 + 80X^2 + 16X - 216X - 24$&lt;br &#x2F;&gt;
$ = 80X^2-80X$&lt;br &#x2F;&gt;
$ = 80X(X-1)$&lt;&#x2F;p&gt;
&lt;p&gt;So in both cases $p(0)=0, p(1)=0$, and consequently $p$ is divisible by $X(X-1)$.&lt;&#x2F;p&gt;
&lt;p&gt;Let us see what happens when we choose $(c_1, c_2, c_3, c_4)$ that do not correspond to an execution instance. For example, consider the polynomial $p$ associated with $c_1=1, c_2=1, c_3=0, c_4=0$. These values do not conform to an execution instance of the circuit. We get&lt;&#x2F;p&gt;
&lt;p&gt;$p = (1 + X)((-X+1) + 0X) - (0X + 0(-X+1))$&lt;br &#x2F;&gt;
$= (1 + X)(-X + 1)$&lt;&#x2F;p&gt;
&lt;p&gt;and this polynomial satisfies $p(0) = 1$, so it’s not divisible by $X(X-1)$.&lt;&#x2F;p&gt;
&lt;p&gt;Families of polynomials $v_i, w_i, y_i$ that encode circuit execution instances like this exist for any circuit.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;recap-1&quot;&gt;Recap&lt;&#x2F;h3&gt;
&lt;p&gt;The main goal is to prove the correct execution of a circuit. Showing the values we got $c_1,\dots,c_N$ is not great because the verifier has to do a lot of work to validate them. Polynomials enter here to give an alternative way of showing that information. Out of the values $c_1,\dots,c_N$ we can construct a polynomial $p$. That polynomial has a special property when the values $c_i$ correspond to a valid execution of the circuit. In our example that property is $p$ being divisible by $X(X-1)$. So this gives another way to prove correct executions:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Show that we properly constructed $p$ following the recipe for the circuit in question.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Show that $p = X(X-1)h$ for some polynomial $h$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;2-schwartz-zippel-lemma&quot;&gt;2. Schwartz-Zippel lemma&lt;&#x2F;h2&gt;
&lt;p&gt;A naive way of showing that $p = X(X-1)h$ would be to give the coefficients of $p$ to the verifier and let him divide it by $X(X-1)$ to find that such a polynomial $h$ exists. This isn’t good since the amount of work required to do that division scales with the complexity of the circuit. There is a very cheap alternative that appears in every SNARK: show that the equality $p(s) = s(s-1)h(s)$ holds for some random element $s$ in $\mathbb F_p$. The Schwarz-Zippel lemma states that this is enough to be convinced that $p = X(X-1)h$ with high probability. Let’s see why.&lt;&#x2F;p&gt;
&lt;p&gt;The key concept here is that of a &lt;em&gt;root&lt;&#x2F;em&gt; of a polynomial. A root of a polynomial $f$ is an element $r \in \mathbb F_p$ such that $f(r)=0$. A fundamental algebra theorem states that a non-zero polynomial of degree $d$ can have, at most, $d$ roots. For example, the polynomial $f = X^5 -3X + 8$ has at most $5$ roots. So if $\mathbb F_p$ has a vast number of elements and we choose a random $s$ in it, the chance of it being one of the $5$ roots of that polynomial is meager. On the other hand, the polynomial $f=0$ is the only one that satisfies $f(s)=0$ for all $s$.&lt;&#x2F;p&gt;
&lt;p&gt;Putting this all together, if $f$ is a polynomial and $f(s)=0$ for a random $s$, then with high probability, we can be sure that $f=0$.&lt;&#x2F;p&gt;
&lt;p&gt;In our case, we have $p$ and $h$ and want to convince a verifier that $p = X(X-1)h$. In other words, we have the polynomial $f = p - X(X-1)h$ and we want to convince the verifier that $f=0$. So using this approach, it’s enough to show that $0 = f(s) = p(s) - s(s-1)h(s)$ for a random $s$. This is the same as showing that $p(s) = s(s-1)h(s)$ for a random $s$.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-hidings&quot;&gt;3. Hidings&lt;&#x2F;h2&gt;
&lt;p&gt;So far things look a bit silly because we are working with raw elements in $\mathbb F_p$. If the prover sends the verifier $p(s)$ and $h(s)$, she sends two elements of $\mathbb F_p$. So the verifier receives two elements $a, b$ in $\mathbb F_p$ and has no idea how they were produced. Are they random? Are they actually $p(s)$ and $h(s)$? There’s no way to tell.&lt;&#x2F;p&gt;
&lt;p&gt;The solution is to work with obfuscated data that allows parties to do a minimal set of operations. The way to obfuscate is pretty simple. We will have a group $G$ and a way to associate to every element of $\mathbb F_p$ its element in $G$ in a way that’s hard to invert.&lt;&#x2F;p&gt;
&lt;p&gt;$$\mathbb F_p \longrightarrow G$$&lt;&#x2F;p&gt;
&lt;h4 id=&quot;example&quot;&gt;Example&lt;&#x2F;h4&gt;
&lt;p&gt;Let’s give an example. The number $113$ is prime, and here is a fact we’ll use: in $\mathbb Z_{454}$ the element $3$ has the property that $3^{x} \equiv 1$ modulo $454$ if and only if $n \equiv 0$ modulo $113$. This implies that $3^i \equiv 3^j$ modulo $454$ if and only if $i \equiv j$ modulo $113$.&lt;&#x2F;p&gt;
&lt;p&gt;So, we have a field $\mathbb F_{113}$, a group $G=\mathbb Z_{454}$ and the element $3\in\mathbb Z_{454}$. We can use all of this to &lt;strong&gt;hide&lt;&#x2F;strong&gt; elements of $\mathbb F_{113}$ in $\mathbb Z_{454}$ as follows: the hiding of an element $x\in\mathbb F_{113}$ is $3^x$ modulo $454$.&lt;&#x2F;p&gt;
&lt;p&gt;$\mathbb F_{113} \longrightarrow \mathbb Z_{454}$&lt;br &#x2F;&gt;
$x \mapsto 3^x$&lt;&#x2F;p&gt;
&lt;p&gt;Here are some examples:&lt;&#x2F;p&gt;
&lt;p&gt;$1 \mapsto 3$&lt;br &#x2F;&gt;
$9 \mapsto 161$&lt;br &#x2F;&gt;
$10 \mapsto 29$&lt;br &#x2F;&gt;
$90 \mapsto 65$&lt;&#x2F;p&gt;
&lt;p&gt;Suppose someone chooses a random $s \in \mathbb F_{113}$, computes $3^s$ modulo $454$, and publishes the result. Say the result is $225$. So we know that $3^s \equiv 225$ modulo $454$. It’s tough to find out what $s$ is without going through all the possibilities. The brute force attack works here because numbers are small. For larger fields and groups, this becomes infeasible. Assume that’s the case for this toy example too. Say there’s no way for us to obtain the value of $s$.&lt;&#x2F;p&gt;
&lt;p&gt;Now, even though we don’t know $s$, we can compute other hidings off the hiding $3^s$. For example, we know that $3^s = 225$, so&lt;&#x2F;p&gt;
&lt;p&gt;$3^{10s} = 225^{10} = 343$&lt;&#x2F;p&gt;
&lt;p&gt;So, even if we only know the hiding of $s$, we can compute the hiding of $10s$. Moreover we can compute the hiding of $as + b$ for any $a,b$ in $\mathbb F_{113}$: $3^{as+b} = 3^{s a \cdot3^b} = 225^{a \cdot3^b}$. For example&lt;&#x2F;p&gt;
&lt;p&gt;$s \mapsto 225$&lt;br &#x2F;&gt;
$10s \mapsto 343$&lt;br &#x2F;&gt;
$3s+2 \mapsto 155$&lt;&#x2F;p&gt;
&lt;h4 id=&quot;in-general&quot;&gt;In general&lt;&#x2F;h4&gt;
&lt;p&gt;The actual group $G$ where hidings live is not that important now. The example above should give a feel of what they look like. But, from now on, we’ll assume we have a way to compute hidings. And we’ll denote the hiding of an element $x\in\mathbb F_p$ by $E(x)$. In the examples $E(s) = 225$, $E(10s) = 343$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;checking-equations-on-hidings&quot;&gt;Checking equations on hidings&lt;&#x2F;h3&gt;
&lt;p&gt;We just saw that if we have the hiding $E(s)$ of an unknown element $s$, we can compute the hiding of $as+b$ for any $a$ and $b$. More generally if we have hidings $E(s)$ and $E(t)$, we can compute the hiding of $as + bt$ as&lt;br &#x2F;&gt;
$$E(as+bt) = E(s)^a E(t)^b.$$&lt;br &#x2F;&gt;
This is because we have $E(s)$, $E(t)$, and the rest involves group operations with those two elements. What we can’t do is compute $E(st)$, the hiding of $st$, without knowing the raw values $s$ and $t$.&lt;&#x2F;p&gt;
&lt;p&gt;That’s sad, but it’s not the end of the world. We can get away with it without that. We will need an algorithm that helps us check relations on the original values when we have only their hidings. Precisely, we’ll need an algorithm $\mathcal A$ that takes $5$ hidings $E(a), E(b), E(c), E(t), E(d)$ and outputs $1$ if $ab - c = td$, and outputs $0$ otherwise. And the beauty of it is that it won’t reveal what the raw values $a,b,c,t,d$ are.&lt;&#x2F;p&gt;
&lt;p&gt;The actual algorithm $\mathcal A$ is something we need to sweep under the rug. The important thing is that such algorithms exist for some groups $G$, especially for elliptic curves. We suggest leaving this as a black box for now.&lt;&#x2F;p&gt;
&lt;p&gt;Note that $\mathcal A(E(a), E(b), E(c), E(0), E(0))$ outputs $1$ if and only if $ab=c$. This will also be useful! Since we’ll use this version a lot, we’ll call it the algorithm $\mathcal B$. So $\mathcal B$ takes $3$ hidings $E(a)$, $E(b)$ and $E(c)$ and outputs 1 if and only if $c = ab$.&lt;&#x2F;p&gt;
&lt;p&gt;For example, using the hidings from the previous section,&lt;br &#x2F;&gt;
$$\mathcal B(161, 29, 65) = 1.$$&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ok-but-why&quot;&gt;Ok, but why?&lt;&#x2F;h4&gt;
&lt;p&gt;Recall that the formula to construct $p$ from values $c_1,c_2,c_3,c_4$ is&lt;&#x2F;p&gt;
&lt;p&gt;$$p = vw - y,$$&lt;&#x2F;p&gt;
&lt;p&gt;where $v = c_1v_1 + c_2v_2 + c_3v_3 + c_4v_4$, $w = c_1w_1 + c_2w_2 + c_3w_3 + c_4w_4$ and $y=c_1y_1 + c_2y_2 + c_3y_3 + c_4y_4$. And if the prover did everything right, she’ll be able to find a polynomial $h$ such that $p = X(X-1)h$.&lt;&#x2F;p&gt;
&lt;p&gt;So the idea is that the prover sends $E(v(s))$, $E(w(s))$, $E(y(s))$ and $E(h(s))$ for some unknown value $s$. Then the verifier, who is going to have $E(s(s-1))$, can use algorithm $\mathcal A$ to check that $v(s)w(s) - y(s) = s(s-1)h(s)$. This is exactly $p(s) = s(s-1)h(s)$, the thing we wanted.&lt;&#x2F;p&gt;
&lt;p&gt;Sending the hidings of $v(s)$, $w(s)$, and $y(s)$ instead of the hiding of $p(s)$ has the advantage that recipes for constructing $v, w$ and $y$ are way much easier than the recipe to build $p$. They are linear combinations of the base polynomials $v_i, w_i, y_i$. And, as we’ll see now, there’s a way to prove we correctly constructed those polynomials in the land of hidings.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;recap-2&quot;&gt;Recap&lt;&#x2F;h4&gt;
&lt;p&gt;Instead of raw values, the prover and verifier will have to work with hidings. The setup phase will produce enough hidings so the prover can compute the values it needs to show to the verifier. And that will be enough for him to be convinced that the prover properly executed the circuit. And with some tricks, we can guarantee that the prover can’t cheat.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;4-intuition-on-how-to-prove-correct-constructions-of-polynomials&quot;&gt;4. Intuition on how to prove correct constructions of polynomials&lt;&#x2F;h2&gt;
&lt;p&gt;Let’s start addressing the third question: how does the verifier know that the prover followed the correct recipe to construct a polynomial?&lt;&#x2F;p&gt;
&lt;p&gt;What follows is an intuition on how the protocol solves this issue. Why do we say it is an intuition? Well, it has gaps. We’ll discuss the actual security proofs in other blogposts.&lt;&#x2F;p&gt;
&lt;p&gt;See for yourself if you can spot the gaps!&lt;&#x2F;p&gt;
&lt;h4 id=&quot;proving-correct-construction-of-v&quot;&gt;Proving correct construction of $v$&lt;&#x2F;h4&gt;
&lt;p&gt;Let’s start with $v$. What we want is to somehow send $E(v(s))$ to the verifier along with some redundant information that proves that $v(s) = c_1v_1(s) + c_2v_2(s) + c_3v_3(s) + c_4v_4(s)$. But without giving away the values $c_1,c_2,c_3,c_4$.&lt;&#x2F;p&gt;
&lt;p&gt;Let’s go back to our example. We have $v_1,v_2,v_3,v_4$. And assume there was a setup phase in which random $s$ and $\alpha$ were sampled from $\mathbb F_p$, and the following evaluation and verification keys were publicly published&lt;br &#x2F;&gt;
Evaluation key:&lt;&#x2F;p&gt;
&lt;p&gt;$E(v_1(s)), E(v_2(s)), E(v_3(s)), E(v_4(s)),$&lt;br &#x2F;&gt;
$E(\alpha v_1(s)), E(\alpha v_2(s)), E(\alpha v_3(s)), E(\alpha v_4(s)).$&lt;&#x2F;p&gt;
&lt;p&gt;Verification Key:&lt;&#x2F;p&gt;
&lt;p&gt;$$E(\alpha)$$&lt;&#x2F;p&gt;
&lt;p&gt;The values $s$ and $\alpha$ are not known to anyone and were discarded. Suppose the prover has already executed the circuit, obtained $c_1,c_2,c_3,c_4$, and constructed $v$. She wants to send the verifier $v(s)$, but she doesn’t know $s$. She can then use the evaluation key to send its hiding: $E(v(s))$. So the prover constructs the following elements and sends them to the verifier:&lt;&#x2F;p&gt;
&lt;p&gt;$E(v(s)) = E(v_1(s))^{c_1} E(v_2(s))^{c_2} E(v_3(s))^{c_3} E(v_4(s))^{c_4} ,$&lt;&#x2F;p&gt;
&lt;p&gt;$E(\alpha v(s)) = E(\alpha v_1(s))^{c_1}E(\alpha v_2(s))^{c_2}E(\alpha v_3(s))^{c_3}E(\alpha v_4(s))^{c_4}.$&lt;&#x2F;p&gt;
&lt;p&gt;Note that the right-hand sides depend on elements known to the prover.&lt;&#x2F;p&gt;
&lt;p&gt;The verifier receives these two elements. Let’s call them $V=E(v(s))$ and $V’=E(\alpha v(s))$. But the verifier doesn’t trust the prover, so for the moment, he knows that he received two hidings $V=E(x)$ and $V’=E(y)$ of some elements $x$ and $y$. And he wants to be convinced that $x = c_1v_1(s) + c_2v_2(s) + c_3v_3(s) + c_4v_4(s)$ for some $c_1,c_2,c_3,c_4$. If those values correspond to a correct evaluation of the circuit is something we are not worrying about right now, and another check will cover that. What we are doing right now is just checking that $x$ is an evaluation at $s$ of a polynomial constructed in a very particular way.&lt;br &#x2F;&gt;
The verifier performs the following check using the verification key&lt;br &#x2F;&gt;
$$\text{Check that }\mathcal B(V, E(\alpha), V’)\text{ equals }1$$&lt;br &#x2F;&gt;
If the prover did everything right, the check would pass. From the verifier’s end, if the check passes, let’s see what he can be sure about. If the check passes, since $V=E(x)$ and $V’=E(y)$, he knows that $y=\alpha x$. Looking at the evaluation key, he sees that the prover doesn’t know the raw value of $\alpha$. The only thing the prover knows related to $\alpha$ is the hidings $E(\alpha v_i(s))$ in the evaluation key. So the only way the prover could have ever constructed the hiding of a multiple of $\alpha$ is from the values $E(\alpha v_i(s))$. And the only thing that can be constructed using them and the group operation is&lt;&#x2F;p&gt;
&lt;p&gt;$E(\alpha v_1(s))^{c_1}E(\alpha v_2(s))^{c_2}E(\alpha v_3(s))^{c_3}E(\alpha v_4(s))^{c_4} = E(c_1\alpha v_1(s) + c_2\alpha v_2(s) + c_3\alpha v_3(s) + c_4\alpha v_4(s)))$&lt;br &#x2F;&gt;
$= E(\alpha(c_1v_1(s) + c_2v_2(s) + c_3v_3(s) + c_4v_4(s)))$&lt;&#x2F;p&gt;
&lt;p&gt;for some $c_1,c_2,c_3,c_4$. So $V’=E(\alpha x)$ has to be of the form above. This implies that $\alpha x = \alpha(c_1v_1(s) + c_2v_2(s) + c_3v_3(s) + c_4v_4(s))$, and therefore the verifier is in possession of&lt;br &#x2F;&gt;
$$V = E(x) = E(c_1v_1(s) + c_2v_2(s) + c_3v_3(s) + c_4v_4(s)).$$&lt;br &#x2F;&gt;
And that’s how the verifier gets convinced that the prover sent him $V=E(v(s))$ where $v$ is a polynomial constructed using the correct recipe!&lt;&#x2F;p&gt;
&lt;h4 id=&quot;a-gap&quot;&gt;A gap&lt;&#x2F;h4&gt;
&lt;p&gt;Did you spot it? The problem lies in the phrase, “The only thing the prover knows related to $\alpha$ are the hidings $E(\alpha v_i(s))$ in the evaluation key.” That’s false because the verification key is also public and potentially known to the prover. The prover also has $E(\alpha)$. She could use it. For example, she could choose any $z$ and send the verifier $V=E(z)$ and $V’=E(\alpha)^z = E(\alpha z)$. And that would pass the verifier’s check without being $z = v(s)$ for any linear combination $v$ of the polynomials $v_i$.&lt;br &#x2F;&gt;
To this point, this is what we have and the prover could sneak in that $z$ if she wants. It’s unclear how the prover could use that flexibility to construct fake proofs. And she actually can’t without breaking some cryptographic assumptions first. But that proof takes another path. For now, let’s continue with our intuition ignoring this fact.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;intuition-continues&quot;&gt;Intuition continues&lt;&#x2F;h4&gt;
&lt;p&gt;Even though there’s this glitch we just discussed, this idea gives an obvious use case of hidings to convince a verifier that a value was produced in a very particular way.&lt;&#x2F;p&gt;
&lt;p&gt;Since the prover needs to send also $E(w(s))$ and $E(y(s))$, there must be a trusted setup publishing enough hidings to construct them. This is what it would look like.&lt;&#x2F;p&gt;
&lt;p&gt;So to send all $E(v(s))$, $E(w(s))$ and $E(y(s))$ we need a trusted setup that samples random $s$, $\alpha_v$, $\alpha_w$, $\alpha_y$ and publishes&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Evaluation key: for all $i$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;$E(v_i(s)), E(\alpha_v v_i(s)),$&lt;br &#x2F;&gt;
$E(w_i(s)), E(\alpha_w w_i(s)),$&lt;br &#x2F;&gt;
$E(y_i(s)), E(\alpha_y y_i(s)),$&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Verification Key: $E(\alpha_v), E(\alpha_w), E(\alpha_y)$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;With all this, the prover can construct all the hidings and send them to the verifier. He follows all the checks and gets convinced that he received $E(v(s)))$, $E(w(s))$ and $E(y(s))$, where $v, w$ and $y$ are linear combinations of the respective $v_i, w_i$ and $y_i$.&lt;&#x2F;p&gt;
&lt;p&gt;But there’s a problem with that. Let’s repeat what we just said in more detail to see it.&lt;br &#x2F;&gt;
The verifier gets convinced that he received $E(v(s))$, where $v(s)$ is &lt;strong&gt;some&lt;&#x2F;strong&gt; linear combination of the $v_i(s)$, say $v(s) = av_1(s) + bv_2(s) + cv_3(s) + dv_4(s)$ for some values $a,b,c,d$.&lt;br &#x2F;&gt;
On the other hand he also received $E(w(s))$, where $w(s)$ is &lt;strong&gt;some&lt;&#x2F;strong&gt; linear combination of the elements $w_i(s)$, say $w(s) = a’v_1(s) + b’v_2(s) + c’v_3(s) + d’v_4(s)$ for some values $a’,b’,c’,d’$. And that’s the problem. The verifier has no guarantee that $a=a’$, $b=b’$, $c=c’$, and $d=d’$. And that’s important! Because $v, w$ and $y$ need to be constructed using the same values $c_1,c_2,c_3,c_4$. That’s part of the recipe to construct $p$.&lt;&#x2F;p&gt;
&lt;p&gt;So we need something that connects the three hidings sent by the prover. Because so far, they are independent, and so are the three checks of the verifier.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;proving-construction-of-all-v-w-y&quot;&gt;Proving construction of all $v,w,y$&lt;&#x2F;h4&gt;
&lt;p&gt;The solution is to use the same trick cleverly yet again. And things start to get weird.&lt;br &#x2F;&gt;
The setup phase will now sample random $s, \alpha_v, \alpha_w, \alpha_y, \beta$ and publishes the following keys&lt;&#x2F;p&gt;
&lt;p&gt;Evaluation key:&lt;&#x2F;p&gt;
&lt;p&gt;$E(v_i(s)), E(\alpha_v v_i(s)),$&lt;br &#x2F;&gt;
$E(w_i(s)), E(\alpha_w w_i(s)),$&lt;br &#x2F;&gt;
$E(y_i(s)), E(\alpha_y y_i(s)),$&lt;br &#x2F;&gt;
$E(\beta(v_i(s) + w_i(s) + y_i(s))).$&lt;&#x2F;p&gt;
&lt;p&gt;Verification Key:&lt;&#x2F;p&gt;
&lt;p&gt;$$E(\alpha_v), E(\alpha_w), E(\alpha_y), E(\beta)$$&lt;&#x2F;p&gt;
&lt;p&gt;As before the prover obtains $c_1,c_2,c_3,c_4$ and uses the evaluation key to compute $E(v(s))$, $E(\alpha_v v(s))$, $E(w(s))$, $E(\alpha_w w(s))$, $E(y(s))$, $E(\alpha_y y(s))$. But now she can also use the new elements of the evaluation key to compute $E(\beta (v(s) + w(s) + y(s)))$. She sends these seven elements to the verifier. The verifier receives seven hidings. Let’s denote them $V,V’, W, W’, Y, Y’, Z$ in the order the prover sent them.&lt;br &#x2F;&gt;
The verifier performs the following checks:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Check that $\mathcal B(V, E(\alpha_v), V&amp;#39;)$, $\mathcal B(W, E(\alpha_w), W&amp;#39;)$, and $\mathcal B(Y, E(\alpha_y), Y&amp;#39;)$ all equal $1$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. He computes $VWY$ using the group operation and checks that $\mathcal B(VWY, E(\beta), Z)$ equals $1$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If all these tests pass, the prover will convince the verifier that&lt;&#x2F;p&gt;
&lt;p&gt;$V = E(av_1(s) + bv_1(s) + cv_3(s) + dv_4(s)),$&lt;br &#x2F;&gt;
$W = E(aw_1(s) + bw_2(s) + cw_3(s) + dw_4(s))$&lt;br &#x2F;&gt;
$Y = E(ay_1(s) + by_2(s) + cy_3(s) + dy_4(s))$&lt;&#x2F;p&gt;
&lt;p&gt;for some elements $a,b,c,d$. And the reason is very similar to the previous case. The second check guarantees that $Z$ is the hiding of some multiple of $\beta$. The only way that it could have been produced is using the new hidings $E(\beta(v_i(s) + w_i(s) + y_i(s)))$. With such hidings the prover can only compute $E(\beta(v(s) + w(s) + y(s)))$ where $v,w$ and $y$ are linear combinations of the $v_i$, $y_i$ and $w_i$, respectively, with the &lt;strong&gt;same&lt;&#x2F;strong&gt; coefficients. That, and again using the second check passed, implies that the coefficients in the raw values of $V, W$, and $Y$ are the same.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-about-h&quot;&gt;What about $h$?&lt;&#x2F;h4&gt;
&lt;p&gt;As we said previously, the prover is sending $E(v(s))$, $E(w(s))$ and $E(y(s))$ because the verifier can use them to check that $\mathcal A(E(v), E(w), E(y), E(s(s-1)), E(h(s)))$ equals $1$. For that the verifier needs both $E(s(s-1))$ and $E(h(s))$. The value $E(s(s-1))$ is independent of the particular execution of the circuit and can be added to the verifying key. Concerning $E(h(s))$, the prover needs to provide that. So she needs to be able to construct it. The hidings on the evaluation key are insufficient now since $h$ won’t be, in general, any linear combination of the polynomials $v_i, w_i, y_i$. The solution is to add $E(s^i)$ to the evaluation key for as many values $i$ as are needed to construct $h$ in the worst case.&lt;&#x2F;p&gt;
&lt;p&gt;To keep this short, we will wait to add this to the protocol. We’ll cover the remaining issue and then write down the final protocol.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;recap-3&quot;&gt;Recap&lt;&#x2F;h4&gt;
&lt;p&gt;With all this, the prover can construct hidings of $v(s)$, $w(s)$,$y(s), h(s)$ and convince the verifier that she followed the correct recipe to build the polynomials $v,w,y$. And moreover that $p = vw-y=X(X-1)h$. This answers the third and fourth questions of the beginning!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;5-inputs-and-outputs&quot;&gt;5. Inputs and outputs&lt;&#x2F;h2&gt;
&lt;p&gt;Let’s answer question 5. How does the verifier know that the prover used the input values $c_1$ and $c_2$? This is very simple. Recall that the verifier expects a proof of execution of the circuit on inputs $c_1,c_2$. So the prover executes the circuit, obtains the output $c_3$, and communicates $c_3$ to the verifier. So the verifier knows $c_1,c_2$, and $c_3$. In general, the verifier knows all public input and output values. Therefore we can modify the protocol slightly to allow the verifier to check inputs and outputs.&lt;&#x2F;p&gt;
&lt;p&gt;The polynomial $p$ is equal to $vw - y$ where $v = c_1v_1 + c_2v_2 + c_3v_3 + c_4v_4$ and similarly for $w$ and $y$. And the verifier is expecting to receive, for instance, the hiding $E(v(s))$ as part of the proof. But the input&#x2F;output part of $E(v(s))$ is something the verifier can compute by himself. More precisely, he knows $c_1,c_2,c_3$ and $E(v_1(s))$, $E(v_2(s))$ and $E(v_3(s))$ since they are part of the public evaluation key so far. So he can compute&lt;&#x2F;p&gt;
&lt;p&gt;$$E(c_1v_1(s) + c_1v_2(s) + c_3v_3(s)) = E(v_1(s))^{c_1} E(v_2(s))^{c_2} E(v_3(s))^{c_3}$$&lt;&#x2F;p&gt;
&lt;p&gt;This means that if the prover only sends him $V = E(c_4v_4(s))$, then the verifier can complete it with the above to obtain $E(v(s))$:&lt;br &#x2F;&gt;
$$E(v(s)) = E(c_1v_1(s) + c_1v_2(s) + c_3v_3(s))V$$&lt;br &#x2F;&gt;
And doing so, he gets the guarantee that the terms in $v(s)$ corresponding to the input&#x2F;output are the ones he expects.&lt;&#x2F;p&gt;
&lt;p&gt;Note that if we do this, the prover won’t need the hidings $E(v_1(s))$, $E(v_2)$, and $E(v_3(s))$ anymore. So we are moving them to the verifying key.&lt;&#x2F;p&gt;
&lt;p&gt;Let’s write down what we have so far!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;6-final-protocol&quot;&gt;6. Final protocol&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;almost-the-final-protocol-for-the-example&quot;&gt;Almost the final protocol for the example&lt;&#x2F;h3&gt;
&lt;p&gt;Recall that we assume there is a hiding function $E: \mathbb F_p \to G$ for some group $G$ and that there exists an algorithm $\mathcal A$ such that $\mathcal A(E(a), E(b), E(c), E(t), E(h)) = 1$ if and only if $ab-c=th$. We define $\mathcal B(E(a), E(b), E(c)) := \mathcal A(E(a), E(b), E(c), E(0), E(0))$ which outputs $1$ if and only if $ab=c$. Also recall that $E(a)E(b) = E(a+b)$.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;1-setup&quot;&gt;1. Setup&lt;&#x2F;h4&gt;
&lt;p&gt;Five random elements $s, \alpha_v, \alpha_w, \alpha_y, \beta$ are sampled from $\mathbb F_p$. Two public &lt;em&gt;keys&lt;&#x2F;em&gt; are generated from them: the evaluation key and the verification key&lt;br &#x2F;&gt;
3.1 Evaluation Key:&lt;&#x2F;p&gt;
&lt;p&gt;$E(v_4(s)), E(w_4(s)), E(y_4(s)),$&lt;br &#x2F;&gt;
$E(\alpha_vv_4(s)), E(\alpha_ww_4(s)), E(\alpha_yy_4(s)),$&lt;br &#x2F;&gt;
$E(\beta(v_4(s) + w_4(s) + y_4(s))),$&lt;br &#x2F;&gt;
$E(1)$&lt;&#x2F;p&gt;
&lt;p&gt;3.2 Evaluation Key:&lt;&#x2F;p&gt;
&lt;p&gt;$E(\alpha_v), E(\alpha_w), E(\alpha_y), E(\beta),$&lt;br &#x2F;&gt;
$E(v_1(s)), E(v_2(s)), E(v_3(s)),$&lt;br &#x2F;&gt;
$E(w_1(s)), E(w_2(s)), E(w_3(s)),$&lt;br &#x2F;&gt;
$E(y_1(s)), E(y_2(s)), E(y_3(s)),$&lt;br &#x2F;&gt;
$E(s(s-1))$&lt;&#x2F;p&gt;
&lt;h4 id=&quot;2-proof-generation&quot;&gt;2. Proof generation&lt;&#x2F;h4&gt;
&lt;p&gt;The prover executes the circuit with public input values $c_1, c_2$, obtains the output value $c_3$, and the intermediate value $c_4$. She computes&lt;&#x2F;p&gt;
&lt;p&gt;$v = c_1v_1 + c_2v_2 + c_3v_3 + c_4v_4$&lt;br &#x2F;&gt;
$w = c_1w_1 + c_2w_2 + c_3w_3 + c_4w_4$&lt;br &#x2F;&gt;
$y = c_1y_1 + c_2y_2 + c_3y_3 + c_4y_4$&lt;br &#x2F;&gt;
$p = vw - y$&lt;&#x2F;p&gt;
&lt;p&gt;She also computes $h$ such that $p = X(X-1)h$. The polynomial $h$ must be of degree $0$ for this particular circuit, so it is a constant value in $\mathbb F_p$.&lt;br &#x2F;&gt;
The prover computes the following hidings from the evaluation key.&lt;&#x2F;p&gt;
&lt;p&gt;$\pi = (E(c_4v_4(s)), E(\alpha_vc_4v_4(s)),$&lt;br &#x2F;&gt;
$E(c_4w_4(s)), E(\alpha_wc_4w_4(s)),$&lt;br &#x2F;&gt;
$E(c_4y_4(s)), E(\alpha_yc_4y_4(s)),$&lt;br &#x2F;&gt;
$E(\beta c_4(v_4(s) + w_4(s) + y_4(s)),$&lt;br &#x2F;&gt;
$E(h))$&lt;&#x2F;p&gt;
&lt;p&gt;The prover sends the output value $c_3$ to the verifier and the proof $\pi$.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;3-verification&quot;&gt;3. Verification&lt;&#x2F;h4&gt;
&lt;p&gt;The verifier receives the output value $c_3$ and the proof $\pi = (V, V’, W, W’, Y, Y’, Z, H)$. He computes the input&#x2F;output parts from the verification key.&lt;&#x2F;p&gt;
&lt;p&gt;$V_{IO} = E(c_1v_1(s) + c_2v_2(s) + c_3v_3(s))$&lt;br &#x2F;&gt;
$W_{IO} = E(c_1w_1(s) + c_2w_2(s) + c_3w_3(s))$&lt;br &#x2F;&gt;
$Y_{IO} = E(c_1y_1(s) + c_2y_2(s) + c_3y_3(s))$&lt;&#x2F;p&gt;
&lt;p&gt;He performs the following checks&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. $\mathcal B(V&amp;#39;, V, E(\alpha_v)) = 1$, $\mathcal B(W&amp;#39;, W, E(\alpha_w)) = 1$, $\mathcal B(Y&amp;#39;, Y, E(\alpha_y)) = 1$,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. $\mathcal B(VWY, E(\beta), Z)=1$,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. $\mathcal A(V_{IO}V, W_{IO}W, Y_{IO}Y, E(s(s-1)), H) = 1$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If all checks pass, the verifier gets convinced that the prover executed the circuit with input values $c_1,c_2$ and obtained the output value $c_3$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;pinocchio-s-protocol&quot;&gt;Pinocchio’s protocol&lt;&#x2F;h3&gt;
&lt;p&gt;Now we give the actual protocol for a general circuit. Suppose the circuit has $n_I$ input values and $n_O$ output values.&lt;&#x2F;p&gt;
&lt;p&gt;As before, let $E$ be the hiding function and $\mathcal A$ and $\mathcal B$ the algorithms to check equations on hidings.&lt;&#x2F;p&gt;
&lt;p&gt;Let $N=n_I + n_O$. Executing the circuit with input values $(c_1,\dots,c_{n_I})$ outputs the values $(c_{n_I+1},\dots, c_{N})$ and all the intermediate values $(c_{N+1}, \dots, c_m)$. Putting these tuples together, we say that $(c_1,\dots,c_m)$ are the values of the execution of the circuit.&lt;&#x2F;p&gt;
&lt;p&gt;In the example, we had $n_I=2, n_O=1$, $N=3$ and therefore executing the circuit with input values $(c_1,c_2)$ produces the output value $c_3$ and the intermediate value $c_4$.&lt;&#x2F;p&gt;
&lt;p&gt;Let $d$ be the number of multiplication gates in the circuit. Let $t = X(X-1)(X-2)\cdots (X-d)$. There exist families of polynomials $v_i,w_i,y_i$, with $i=1,\dots, m$, such that $(c_1,\dots,c_m)$ are the values of the execution of the circuit if and only if $p = vw-y$ is divisible by $t$, where $v=\sum_i c_iv_i$, $w=\sum_ic_iw_i$ and $y=\sum_ic_iy_i$.&lt;&#x2F;p&gt;
&lt;p&gt;In our example $t = X(X-1)$.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;1-setup-1&quot;&gt;1. Setup&lt;&#x2F;h4&gt;
&lt;p&gt;Eight random elements $s, r_v, r_w, \alpha_v, \alpha_w, \alpha_y, \beta, \gamma$ are sampled from $\mathbb F_p$. Let $r_y=r_vr_w$. Two public &lt;em&gt;keys&lt;&#x2F;em&gt; are generated from them: the evaluation key and the verification key&lt;br &#x2F;&gt;
3.1 Evaluation Key: For all $i=N+1,\dots,m$, and for all $j=1,\dots,d-2$&lt;&#x2F;p&gt;
&lt;p&gt;$E(r_vv_i(s)), E(r_ww_i(s)), E(r_yy_i(s)),$&lt;br &#x2F;&gt;
$E(\alpha_vr_vv_i(s)), E(\alpha_wr_ww_i(s)), E(\alpha_yr_yy_i(s)),$&lt;br &#x2F;&gt;
$E(\beta(r_vv_i(s) + r_ww_i(s) + r_yy_i(s))),$&lt;br &#x2F;&gt;
$E(s^j)$&lt;&#x2F;p&gt;
&lt;p&gt;3.2 Evaluation Key: For all $i=1,\dots, N$&lt;&#x2F;p&gt;
&lt;p&gt;$E(\alpha_v), E(\alpha_w), E(\alpha_y), E(\beta\gamma), E(\gamma)$&lt;br &#x2F;&gt;
$E(r_vv_i(s)),$&lt;br &#x2F;&gt;
$E(r_ww_i(s)),$&lt;br &#x2F;&gt;
$E(r_yy_i(s)),$&lt;br &#x2F;&gt;
$E(t(s))$&lt;&#x2F;p&gt;
&lt;h4 id=&quot;2-proof-generation-1&quot;&gt;2. Proof generation&lt;&#x2F;h4&gt;
&lt;p&gt;The prover executes the circuit with input values $(c_1,\dots,c_{n_I})$ and obtains the values $(c_1,\dots,c_m)$. She computes:&lt;&#x2F;p&gt;
&lt;p&gt;$v = \sum_{i=1}^m c_iv_i$&lt;br &#x2F;&gt;
$w = \sum_{i=1}^m c_iw_i$&lt;br &#x2F;&gt;
$y = \sum_{i=1}^m c_iy_i$&lt;br &#x2F;&gt;
$p = vw - y$&lt;&#x2F;p&gt;
&lt;p&gt;She also computes $h$ such that $p = th$. The polynomial $h$ is of degree at most $d$.&lt;br &#x2F;&gt;
The prover computes the following hidings from the evaluation key. Note that all the sums here have $i$ ranging from $N+1$ to $m$. That corresponds to the indices of the intermediate values.&lt;&#x2F;p&gt;
&lt;p&gt;$\pi = (E(\sum_{i=N+1}^mc_ir_vv_i(s)), E(\alpha_v\sum_{i=N+1}^mr_vc_iv_i(s)),$&lt;br &#x2F;&gt;
$E(\sum_{i=N+1}^mc_ir_ww_i(s)), E(\alpha_w\sum_{i=N+1}^mr_wc_iw_i(s)),$&lt;br &#x2F;&gt;
$E(\sum_{i=N+1}^mc_ir_yy_i(s)), E(\alpha_y\sum_{i=N+1}^mr_yc_iy_i(s)),$&lt;br &#x2F;&gt;
$E(\beta \sum_{i=N+1}^mr_vc_iv_i(s) + r_wc_iw_i(s) + r_ic_iy_i(s)),$&lt;br &#x2F;&gt;
$E(h))$&lt;&#x2F;p&gt;
&lt;p&gt;The prover sends the output values $(c_{n_O+1},\dots,c_N)$ to the verifier and the proof $\pi$.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;3-verification-1&quot;&gt;3. Verification&lt;&#x2F;h4&gt;
&lt;p&gt;The verifier receives the output values $(c_{n_O},\dots, c_N)$ and the proof $\pi = (V, V’, W, W’, Y, Y’, Z, H)$. He computes the input&#x2F;output parts from the verification key. Note that all sums here have $i$ ranging from $1$ to $N$. That corresponds to the input&#x2F;output indices.&lt;&#x2F;p&gt;
&lt;p&gt;$V_{IO} = E(\sum_{i=1}^Nc_ir_vv_i(s))$&lt;br &#x2F;&gt;
$W_{IO} = E(\sum_{i=1}^Nc_ir_ww_i(s))$&lt;br &#x2F;&gt;
$Y_{IO} = E(\sum_{i=1}^Nc_ir_yy_i(s))$&lt;&#x2F;p&gt;
&lt;p&gt;He performs the following checks&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. $\mathcal B(V&amp;#39;, V, E(\alpha_v)) = 1$, $\mathcal B(W&amp;#39;, W, E(\alpha_w)) = 1$, $\mathcal B(Y&amp;#39;, Y, E(\alpha_y)) = 1$,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. $\mathcal A(Z, E(\gamma), E(0), VWY, E(\beta\gamma))=1$,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. $\mathcal A(V_{IO}V, W_{IO}W, Y_{IO}Y, E(t(s)), H) = 1$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * If all checks pass, the verifier gets convinced that the prover executed the circuit with input values $(c_1,\dots,c_{n_I})$ and obtained the output values $(c_{n_O+1}, \dots, c_N)$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;</content>
        
    </entry>
    <entry xml:lang="en">
        <title>The hunting of the (zk)-SNARK</title>
        <published>2023-01-11T00:00:00+00:00</published>
        <updated>2023-01-11T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/the-hunting-of-the-zk-snark/"/>
        <id>https://blog.lambdaclass.com/posts/the-hunting-of-the-zk-snark/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/the-hunting-of-the-zk-snark/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;Succinct Non-Interactive Arguments of Knowledge (SNARKs) are powerful cryptographic primitives with decentralized finances, governance, and computation applications. There are many different SNARKs, such as Marlin (the one Aleo uses), Plonk, STARKs, Groth16, etc., depending on the tools they are built on and with differences in performance, proof sizes, verification times, etc. However, they are all based on some common principles and properties. Among SNARKs, the most important ones for private applications are zero-knowledge SNARKs (zk-SNARKs, for short). They allow us to prove that we know certain variables, called witness, $w$, such that the output of a function $F$, evaluated at the witness and instance (public variables), $x$, is $F(x,w)=y$, without revealing anything about $w$.&lt;&#x2F;p&gt;
&lt;p&gt;We can convert computer programs to functions receiving input (some of which we may want to conceal) and prove their correct execution with SNARKs. For example, we can transform the program into an arithmetic circuit, $C$, and, given the public input and output, $x$, and confidential data, $w$, we can prove that the program was correctly executed by showing the satisfiability of the circuit, $C(x,w)=0$. Since arithmetic circuit satisfiability is an NP-complete problem, we can reduce any NP problem to an arithmetic circuit (this is not the only alternative, though, and several constructions rely on different techniques).&lt;&#x2F;p&gt;
&lt;p&gt;Before jumping into the details, let us first explain the main properties of a SNARK and the precise meaning of each term in the name. A zk-SNARK involves two parties, the prover and the verifier, where the first one tries to convince the latter of a given statement, such as the prover knows $w$ such that $C(w,x)=0$. The SNARK must fulfill the following:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Completeness: If the prover knows $w$, he will always be able to convince an honest verifier of the statement&amp;#39;s validity.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Soundness: if the statement is false, then no cheating prover could convince the verifier to accept, except with very low probability.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Zero-knowledge: the proof does not reveal anything about the witness.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;As for the name, we have the following:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Succinctness: the proofs must be short and quick to verify. This property would enable us to delegate expensive computations to untrusted parties and check their validity without running the program ourselves.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Non-interactive: does not require interaction between prover and verifier, nor to generate the proof or verify it. We will see that the construction will rely first on interactive proofs, but we can turn it into non-interactive by employing the [Fiat-Shamir](https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Fiat%E2%80%93Shamir_heuristic) transform.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Argument of knowledge: we can prove with a high probability that we know the witness.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;&#x2F;h2&gt;
&lt;p&gt;SNARKs require trusted setups. Among them, we can distinguish:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Uniform reference string or transparent setups (URS).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Structured reference string or private setup (SRS).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In the case of SRS, we can find two instances:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Universal (for example, MARLIN)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Specific (Groth 16)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In practice, the trusted setup is carried out as a multiparty computation; the construction will be secure if there is at least one honest party. The problem with specific SRS is that the string depends on the program, and we must carry out a new setup for each one (this is an undesirable property).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;probabilistic-proofs-and-capabilities-of-the-verifier&quot;&gt;Probabilistic proofs and capabilities of the verifier&lt;&#x2F;h2&gt;
&lt;p&gt;The conciseness of the argument relies on probabilistic proofs. To do that, we first have to establish the things that the “powers” or capabilities of the verifier. We have:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Interaction: the verifier can interact with the prover, sending challenges and receiving responses.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Multiprover: there are several provers, but they are isolated.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Randomness: the verifier can select random elements or queries.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Ability to perform queries to an oracle: the verifier may query the prover&amp;#39;s messages.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;When the verifier has access to more than one of these powers, we get different types of proofs:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Interactivity + Randomness: Interactive proofs (IP).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Randomness + Oracle: Probabilistically checkable proofs (PCP).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Interactivity + Randomness + Oracle: Interactive Oracle Proofs (IOP)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There are other possibilities, such as MIOP, but we will focus on the previous 3. IOPs give the most efficient constructions for SNARKs: quasilinear time verification, linear size proof lengths, linear time prover, and efficient implementations. PCP are interesting from an educational point of view but are inefficient in practice (it does not result in succinct proofs, except with linear queries). Finally, IP give some powerful building blocks in the form of subroutines.&lt;&#x2F;p&gt;
&lt;p&gt;In an IOP, the prover and verifier exchange messages. The prover sends arbitrary messages (in a polynomial IOP, the prover sends commitments - see next section- to polynomials), and the verifier sends random challenges. After some rounds, the verifier queries some values and decides whether to accept or reject the proof.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;commitment-schemes&quot;&gt;Commitment schemes&lt;&#x2F;h2&gt;
&lt;p&gt;The performance of SNARKs depends on the type of commitment used; there have been many advances over the last years to improve them.&lt;&#x2F;p&gt;
&lt;p&gt;We can think of a commitment as a safe box. We make some choice for a bet, store it in a secure container, and hand it to someone else. Once the result is known, we can reveal our selection by opening the safe box.&lt;&#x2F;p&gt;
&lt;p&gt;A commitment has to verify two properties:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Binding: we cannot produce two valid openings for the same commitment. In other words, if we committed to some value $a$, it should be impossible to find $b$ such that the $cm(a)=cm(b)$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Hiding: the commitment reveals nothing about the committed data.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;One way to commit to a message is by using a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cryptographic_hash_function&quot;&gt;collision-resistant hash function&lt;&#x2F;a&gt;. If we have a message $m$ and some random value $r$,&lt;br &#x2F;&gt;
$\mathrm{cm}(m,r)=hash(m\mid r)$&lt;br &#x2F;&gt;
Given that it is collision-resistant, we have the binding property.&lt;br &#x2F;&gt;
We can afterward open the commitment and verify the following:&lt;br &#x2F;&gt;
$\mathrm{Verify}(m,r,\mathrm{cm})\rightarrow$ accept or reject&lt;br &#x2F;&gt;
One advantage of commitments is that they tend to be short. For example, if we use SHA-256, the digest length will be 32 bytes.&lt;&#x2F;p&gt;
&lt;p&gt;One relevant group of commitments is the polynomial schemes. Here are some constructions and the math that they rely on:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Basic elliptic curves: bulletproofs&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Bilinear groups: Kate-Zaverucha-Goldberg (KZG) polynomial commitments (pairings, trusted setup) DORY (no trusted setup)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Groups of unknown order: DARK&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Hash functions only: FRI&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;anatomy-of-a-snark&quot;&gt;Anatomy of a SNARK&lt;&#x2F;h2&gt;
&lt;p&gt;A SNARK can be constructed by selecting the following two elements:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. A type of probabilistic proof: for example, probabilistically checkable proof (PCP) or interactive oracle proof (IOP). A particular kind of IOP is polynomial IOP (PIOP).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. A commitment scheme (cryptography). For example, polynomial&#x2F;functional commitments, vector commitments, and linear encoding.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The probabilistic proof determines the type of computation. It can be either a machine computation (such as vmTinyRam) or a circuit.&lt;&#x2F;p&gt;
&lt;p&gt;The cryptographic element determines the cost to generate the proofs, whether it will be postquantum secure, and the type of setup (transparent or structured). The math tools we need to work with each of them are:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Linear encoding: Elliptic curve pairings (ECP) + Lattices&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Vector commitment: Collision resistant hash (CRH) functions + ECP.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Polynomial commitment: CRH, ECP, PO groups, UO groups&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Some SNARK recipes are:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Linear PCP + Linear encoding: Groth16, Groth-Maller 17&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. PCP&#x2F;IOP + vector commitment: STARKs&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Polynomial PCP&#x2F;IOP + polynomial commitment: MARLIN, SONIC, Plonk, Spartan.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Bulletproofs take different combinations of the elements above and are based on cryptographic sum checks.&lt;&#x2F;p&gt;
&lt;p&gt;The proof’s size depends strongly on the type of construction. For example, for PIOP with KZG polynomial commitments, proofs take less than one kB (two elliptic curve elements). In contrast, IOP with &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;vitalik.ca&#x2F;general&#x2F;2017&#x2F;11&#x2F;22&#x2F;starks_part_2.html&quot;&gt;FRI&lt;&#x2F;a&gt; (Fast Reed-Solomon Interactive Oracle Proofs of Proximity) needs around 100 kB, more than two orders of magnitude more! This difference is because FRI uses Merkle trees; the verification requires an authentication path, needing several hashes.&lt;&#x2F;p&gt;
&lt;p&gt;One problem we face with circuits is that the verifier should read it, resulting in a linear verification time that depends linearly on the size (which would make it non-succinct). To avoid this, we can preprocess it and attain sublinear verification times.&lt;&#x2F;p&gt;
&lt;p&gt;We will now focus on the KZG polynomial commitment, which is the basis of Marlin and Plonk.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;kzg-commitment-scheme&quot;&gt;KZG commitment scheme&lt;&#x2F;h2&gt;
&lt;p&gt;The polynomial commitment scheme has the following algorithms:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Setup.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Commit.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Open.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To commit to a polynomial, we will evaluate the polynomial at a given but unknown point $\alpha$.&lt;&#x2F;p&gt;
&lt;p&gt;The setup takes the maximum degree of the polynomial (which depends on the number of gates of the arithmetic circuit) and outputs the public parameters: proving key and verifying key. To be able to evaluate polynomials, we will use an elliptic curve (we will need it to be &lt;a href=&quot;&#x2F;multiscalar-multiplication-strategies-and-challenges&#x2F;&quot;&gt;pairing friendly&lt;&#x2F;a&gt;, such as BLS 12-377) to hide $\alpha$ and its powers ($\alpha$ is chosen during the setup ceremony and discarded as toxic waste!). To do that, we pick a generator of a group of large prime order $d+1$, $g$ and calculate&lt;br &#x2F;&gt;
$H={g,\alpha g, \alpha^2 g, \alpha^3 g,…, \alpha^{d} g}={h_0,h_1,h_2,…h_{d}}$&lt;br &#x2F;&gt;
This calculation will give us a vast collection of elliptic curve points (we will save them as a string), which will work as the proving key. In the case of a universal setup, the number of elements will coincide with the maximum size of the circuit. Since elliptic curve points take in the order of $100$ B, if we want to deal with $10^{8}$ gates, we will need more than 1 GB to store it. For a given circuit, which could be much smaller than the maximum, MARLIN trims the key so that it is much simpler and faster to work. The setup also depends on a security parameter $\lambda$, but we will consider it to be fixed in our analysis.&lt;br &#x2F;&gt;
We therefore have $\mathrm{setup}(\lambda,N)\rightarrow \mathrm{pp(pk,vk)}$.&lt;&#x2F;p&gt;
&lt;p&gt;The prover generates the polynomial $P(x)=p_0+p_1x+…p_dx^d$ and commits to it by evaluating it at $\alpha$. Since we do not know $\alpha$, only the scalar multiples of group elements of powers of $\alpha$,&lt;br &#x2F;&gt;
$\mathrm{cm}(P)=p_0g+p_1\alpha g+…+p_d\alpha^d g=p_0h_0+p_1h_1+…p_dh_d$&lt;br &#x2F;&gt;
This calculation is the problem of multiscalar multiplication (&lt;a href=&quot;&#x2F;multiscalar-multiplication-strategies-and-challenges&#x2F;&quot;&gt;MSM&lt;&#x2F;a&gt;). We see that the polynomial commitment corresponds to one group element of the elliptic curve.&lt;&#x2F;p&gt;
&lt;p&gt;We could also use a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Merkle_tree&quot;&gt;Merkle tree&lt;&#x2F;a&gt; to commit to a polynomial. The problem with Merkle trees is that the size of the tree would be dependent on the degree of the polynomial. In the case of KZG, the commitment is just one group element independent of the size. Besides, when we want to evaluate the polynomial in a proof, we need to send all the coefficients in the clear (revealing the polynomial), with the verifier having to do linear work on the size of the polynomial. On the other hand, we will see that KZG mostly hides the polynomial (unless there are lots of queries), and the verification is independent of the degree of the polynomial. Additionally, KZG allows for batch proofs: if we have several commitments $\mathrm{cm}_1$, $\mathrm{cm}_2$, …, $\mathrm{cm}_k$, we can generate a single proof showing that all commitments are valid.&lt;&#x2F;p&gt;
&lt;p&gt;Once the prover commits to the polynomial, the verifier (in an interactive scheme) can send random points $r_k$ to the prover, and the latter gives the polynomial evaluated at $r_k$, $P(r_k)$. To make it non-interactive, we use the Fiat-Shamir transform to generate random challenges from a cryptographic hash function.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose the prover wants to convince the verifier that $P(u)=v$. He can transform that equation into a polynomial, $g(x)=P(x)-v$, which has a root at $x=u$. This fact means that $g(x)$ is divisible by $x-u$, which we can write as $g(x)=P(x)-v=Q(x)(x-u)$, where $Q$ is the quotient polynomial. The prover can commit to $Q(x)$ doing the same as before, that is&lt;br &#x2F;&gt;
$\mathrm{cm}(Q)=q_0g+q_1\alpha d+…+q_{d-1}\alpha^{d-1} g=q_0h_0+q_1h_1+…q_{d-1}h_{d-1}$&lt;br &#x2F;&gt;
which is another MSM. The proof $\pi$ contains this group element: constant size! The proof will show that $P(u)=v$ and $P$ is indeed a polynomial of at most degree $d$ and that the commitment of $P$ is $\mathrm{cm}(P)$.&lt;&#x2F;p&gt;
&lt;p&gt;The verifier accepts the proof if $(\alpha-u)\mathrm{cm}(Q)=\mathrm{cm}(P)-vg$. The problem is that we do not know $\alpha$. This is where &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;vitalik.ca&#x2F;general&#x2F;2017&#x2F;01&#x2F;14&#x2F;exploring_ecp.html&quot;&gt;pairings&lt;&#x2F;a&gt; come to our aid, and we only need the elements $h_0$ and $h_1$ to be able to compute. Roughly speaking, an elliptic curve pairing is a function&lt;br &#x2F;&gt;
$f: \mathcal{G}_1 \times \mathcal{G}_2\rightarrow \mathcal{G}_t$&lt;br &#x2F;&gt;
which takes two elements, $P$ from $\mathcal{G}_1$ and $Q$ from $\mathcal{G}_2$ and outputs an element $R$ from $\mathcal{G}_t$. All the groups have the same order $r$ and correspond to groups in pairing-friendly elliptic curves. In the case of MARLIN, we use the &lt;a href=&quot;&#x2F;multiscalar-multiplication-strategies-and-challenges&#x2F;&quot;&gt;curve BLS 12-377&lt;&#x2F;a&gt;. The pairing satisfies the following:&lt;br &#x2F;&gt;
$f(P,Q)=f(g,g_2)^{pq}$&lt;br &#x2F;&gt;
where $g$ and $g_2$ are generators for the groups $\mathcal{G}_1$ and $\mathcal{G}_2$ (and $P=pg$ and $Q=qg_2$). The form of the verification equation in terms of pairings is&lt;br &#x2F;&gt;
$f(\mathrm{cm}(Q),(\alpha-u)g_2)=f(\mathrm{cm}(P)-vg,g_2)$&lt;br &#x2F;&gt;
We do the check over $\mathcal{G}_t$. We only need to know $\alpha g_2$ from the trusted setup.&lt;&#x2F;p&gt;
&lt;p&gt;How can we be convinced that if we evaluated the polynomials at a single point and they coincide, then it is likely that the argument is true? The answer lies with the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Schwartz%E2%80%93Zippel_lemma&quot;&gt;Schwartz-Zippel lemma&lt;&#x2F;a&gt; (we will state it for finite fields): for a polynomial $P$ of degree $d$ over a finite field of order $p$, the probability that the polynomial is zero at a point sampled at random $r$ is&lt;br &#x2F;&gt;
$\mathrm{Pr}(P(r)=0)=d&#x2F;p$&lt;br &#x2F;&gt;
Given that the maximum size of circuits (which gives the maximum degree of a polynomial) is around $2^{26}\approx 10^8$ and the size of the field is larger than $2^{256}$, the probability is around $2^{-230}\approx 10^{-70}$. If we say $P_1$ and $P_2$ coincide at one point $r$, we can construct the polynomial $P(x)=P_1(x)-P_2(x)$ (since polynomial addition is closed) and $P(r)=0$. Given how unlikely it is to hit a zero at random, we are confident that $P(x)$ is the zero polynomial.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;Zk-SNARKs have started gaining attention due to their use in developing fully private applications. They provide succinct proofs that a specific computation was carried out correctly without revealing sensitive information. There are many possible constructions, most based on probabilistic proofs and a commitment scheme. Depending on the different choices, more efficient versions are possible and determine the type of computation (machine or circuit computation). We explored the KZG commitment scheme, which shows the idea behind systems such as MARLIN and Plonk and the calculations we need to generate and verify the proofs.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Multiscalar Multiplication: Strategies and Challenges</title>
        <published>2023-01-09T00:00:00+00:00</published>
        <updated>2023-01-09T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/multiscalar-multiplication-strategies-and-challenges/"/>
        <id>https://blog.lambdaclass.com/posts/multiscalar-multiplication-strategies-and-challenges/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/multiscalar-multiplication-strategies-and-challenges/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;Generating a zk-SNARK (zero-knowledge succinct non-interactive argument of knowledge), as the one Aleo uses, involves a lot of cryptographic calculations, almost all of which happen inside an elliptic curve over a finite field.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;elliptic-curves&quot;&gt;Elliptic Curves&lt;&#x2F;h2&gt;
&lt;p&gt;The following is a short version of &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;cryptographyinrustforhackers.com&#x2F;chapter_2.html#elliptic-curves&quot;&gt;this&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;An elliptic curve point is a pair of numbers $(x,y)$ in a finite field (you can think of a finite field as $Z_p$, the integers modulo some huge prime number, though sometimes they’re more than that), which satisfy an equation of the form:&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
y^2 = x^3 + ax + b&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;for some $a$ and $b$ in the finite field.&lt;&#x2F;p&gt;
&lt;p&gt;You can sum elliptic curve points, but not in the traditional way. Instead of doing&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
(x_1, y_1) + (x_2, y_2) = (x_1 + x_2, y_1 + y_2)&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;We do this:&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
(x_1, y_1) + (x_2, y_2) = (x_3, -y_3)&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;where&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
x_3 = s^2 - x_1 - x_2 \&lt;br &#x2F;&gt;
y_3 = s(x_1 - x_3) - y_1 \&lt;br &#x2F;&gt;
s = \frac{y_2 - y_1}{x_2 - x_1} x_3&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;There are two exceptions to this:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. When $(x_1, y_1) = (x_2, y_2)$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. When $x_1 = x_2$ but $y_1 \neq y_2$. In this case, as no other solution exists, $y_1 = - y_2$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In both 1. and 2. the calculation above is not defined (we would be dividing by zero), so what we do instead is:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. The sum of a point with itself is, as before, a point $(x_3, y_3)$, only in this case  &lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
x_3 = s^2 - 2 x_1 \&lt;br &#x2F;&gt;
y_3 = (x_1 - x_3) - y_1 \&lt;br &#x2F;&gt;
s = \dfrac{3 x_1^2 + a}{2 y_1}&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
(Here, $a$ is the coefficient defining the curve equation above).
2. In this case, the result of the sum is a unique point we arbitrarily add to the curve, called the &lt;em&gt;point at infinity&lt;&#x2F;em&gt; and noted $\mathcal{O}$. This point works as the zero for our sum, i.e.,&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
\mathcal{O} + (x, y) = (x, y)&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
for every $(x,y)$.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;primitive-operations&quot;&gt;Primitive operations&lt;&#x2F;h2&gt;
&lt;p&gt;Elliptic curve cryptography ultimately relies on two primitive operations, point addition (adding two different points) and point doubling (adding a point to itself), which we call &lt;code&gt;ECADD&lt;&#x2F;code&gt; and &lt;code&gt;ECDBL&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We can apply the double-and-add algorithm if we try to add a point many times to itself. As already explained &lt;a href=&quot;&#x2F;what-every-developer-needs-to-know-about-elliptic-curves&#x2F;&quot;&gt;in one of our posts&lt;&#x2F;a&gt;, the idea is simple: if we want to calculate, say, $9P$ for some curve point $P$, instead of performing nine additions we can do&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
P + P = 2P \&lt;br &#x2F;&gt;
2P + 2P = 4P \&lt;br &#x2F;&gt;
4P + 4P = 8P \&lt;br &#x2F;&gt;
4P + P = 9P&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;which is only four addition operations.&lt;&#x2F;p&gt;
&lt;p&gt;When we have to add many different points $k_1P_1+k_2P_2+…+k_nP_n$, most techniques assume these primitives are given and focus on how to perform the scalar multiplications $k_i P_i$ and the additions, minimizing the amount of &lt;code&gt;ECADD&lt;&#x2F;code&gt;s and &lt;code&gt;ECDBL&lt;&#x2F;code&gt;s.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;msm&quot;&gt;MSM&lt;&#x2F;h2&gt;
&lt;p&gt;The &lt;em&gt;Multi-Scalar Multiplication&lt;&#x2F;em&gt; problem consists of, given an elliptic curve, calculating&lt;&#x2F;p&gt;
&lt;p&gt;$$&lt;br &#x2F;&gt;
\sum_{i=1}^{n} k_i P_i&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;for some scalars (a.k.a. integers modulo a certain prime) $k_i$, some elliptic curve points $P_i = (x_i, y_i)$ and some $n$ (in Aleo’s challenge it is $2^{26}$).&lt;&#x2F;p&gt;
&lt;p&gt;The sum operation here is the one discussed in the previous section. Similarly, $k_i P_i$ means “$P_i$ summed to itself $k_i$ times,” the sum once again being the one defined above.&lt;&#x2F;p&gt;
&lt;p&gt;Around 80% of the time to produce a zk-SNARK proof is spent doing MSM, so optimizing it is crucial for performance.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;bucketing-method&quot;&gt;Bucketing method&lt;&#x2F;h3&gt;
&lt;p&gt;We can break the MSM into smaller sums and reduce the number of operations by repeatedly using the windowing technique. If we want to compute each $k_iP_i$, we can break it into windows of size $c$&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
k_iP_i=k_{i0}P_i+k_{i1}2^{c} P_i+k_{i2}2^{2c} P_i+…+k_{i,m-1}2^{c(m-1)} P_i&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
Using this, we can rewrite the MSM problem as&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
P=\sum_{i} k_iP_i=\sum_{i}\sum_{j} k_{ij}2^{cj}P_i&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
We can now change the order of the summations,&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
P=\sum_{i} k_iP_i=\sum_{j}2^{cj}\left(\sum_{i} k_{ij}P_i\right)=\sum_j 2^{cj} B_j&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
In other words, we first divide the scalars into windows and then combine all the points in each window. Now we can focus on how to calculate each $B_j$ efficiently:&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
B_j=\sum_{i} k_{ij}P_i=\sum_{\lambda=0}{2c-1} \lambda \sum_{u(\lambda)} P_u&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
where the summation over $u(\lambda)$ is done only over points whose coefficient is $\lambda$. For example, if $c=3$ and we have $15$ points,&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
B_1=4P_1+3P_2+5P_3+1P_4+4P_5+6P_7+6P_8+3P_{14}+5P_{15}&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
We can split the summation by the coefficients $\lambda$, taking values from $1$ to $7$. For $\lambda=1$, $\sum_u P_u=P_4$ (because $P_4$ is the only one with coefficient $1$), for $\lambda=4$, $\sum_u P_u=P_1+P_5$, etc. We place all points with a common coefficient $\lambda$ into the $\ lambda$ bucket. Thus,&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
B_j=\sum_\lambda \lambda S_{j\lambda}=S_{j1}+2S_{j2}+3S_{j3}+4S_{4j}+5S_{5j}+6S_{j6}+7S_{j7}&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
We can calculate this with a minimum number of point additions using partial sums.&lt;br &#x2F;&gt;
$T_{j1}=S_{j7}$&lt;br &#x2F;&gt;
$T_{j2}=T_{j1}+S_{j6}$&lt;br &#x2F;&gt;
$T_{j3}=T_{j2}+S_{j5}$&lt;br &#x2F;&gt;
$T_{j4}=T_{j3}+S_{j4}$&lt;br &#x2F;&gt;
$T_{j5}=T_{j4}+S_{j3}$&lt;br &#x2F;&gt;
$T_{j6}=T_{j5}+S_{j2}$&lt;br &#x2F;&gt;
$T_{j7}=T_{j6}+S_{j1}$&lt;br &#x2F;&gt;
Each of these operations involves doing just one elliptic point addition. We can obtain the final result by summing these partial sums:&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
B_j=\sum T_{jk}&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;We can improve the calculations by changing the expansion of the coefficients $k_i$. In binary representation, the Hamming weight is the number of non-zero bits. Ideally, we would like this weight to be as small as possible to reduce the number of additions (For example, 65537, which is $2^{16}+1$, is used as the public key for the RSA cryptosystem in many implementations. The square and multiply algorithm requires only two multiplications). The average Hamming weight in a binary representation is $1&#x2F;2$; if we introduce a signed binary representation ($-1,0,1$), the average weight is reduced to $1&#x2F;3$, with the consequent decrease in the number of operations (on average).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bls-12-377&quot;&gt;BLS 12-377&lt;&#x2F;h2&gt;
&lt;p&gt;The curve which Aleo uses is called BLS 12-377. The base field (finite field) has order $q$ (a 377-bit prime) and has an embedding degree of 12. Both the order of the elliptic curve group $G_1$, $r$, and finite field are highly 2-adic (that is, both $q$ and $r$ can be written as $2^\alpha r+1$, where $r$ is an odd number and $\alpha$ is greater than $40$). The orders $q$ and $r$ are related by the embedding degree: $r \mid q^{12}-1$. The equation for the elliptic curve is&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
y2=x3+1&lt;br &#x2F;&gt;
$$&lt;&#x2F;p&gt;
&lt;p&gt;Additionally, we can build a second group $G_2$ over a quadratic field extension of $\mathbb{F}_q$; the equation of the curve is&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
y2=x3+B&lt;br &#x2F;&gt;
$$&lt;br &#x2F;&gt;
where $B$ is a parameter. For more information on the curve’s parameters, see &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;ark-bls12-377&#x2F;latest&#x2F;ark_bls12_377&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;BLS 12-377 is birrationally equivalent to &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;draft-irtf-cfrg-hash-to-curve-05#appendix-B&quot;&gt;Montgomery and twisted Edwards’s curves&lt;&#x2F;a&gt;. This allows us to perform point addition and scalar multiplication faster by avoiding costly field inversions. In the case of Montgomery curves, it is possible to perform scalar multiplication in constant time, making the operation resistant to timing attacks.&lt;&#x2F;p&gt;
&lt;p&gt;Implementations of the BLS 12-377 curve and the (birrationally equivalent) twisted Edwards curve are given in this &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;arkworks-rs&#x2F;curves&quot;&gt;repository&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;BLS 12-377 is one of the pairing-friendly elliptic curves; these have applications such as short digital signatures that are efficiently aggregatable, polynomial commitment schemes, and single-round multi-key exchanges.&lt;br &#x2F;&gt;
The reason why we have two equations for the BLS curve and two groups is related to pairings. A pairing is a bilinear map: it takes two points, each from a group of prime order $r$. For technical reasons, these groups need to be different. As the original curve has only one group of order $r$, we need to extend the field to find other groups of order $r$. The embedding degree gives how much we have to extend the field to find those other groups. As a bonus, the extended field contains all the $r$-th roots of unity.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;a-note-on-field-extensions&quot;&gt;A note on Field Extensions&lt;&#x2F;h3&gt;
&lt;p&gt;The embedding degree is also the degree of the field extension we need to use. Familiar examples of field extensions are the real numbers, $\mathbb{R}$, (extending the field of rational numbers $\mathbb{Q}$, which is a transcendental extension) and the complex numbers, \( \mathbb{C} \) (extending \( \mathbb{R} \)). The latter cannot be further extended since there are no irreducible polynomials in \( \mathbb{C} \) (we say that the complex numbers are algebraically closed).&lt;&#x2F;p&gt;
&lt;p&gt;If we want to build a quadratic extension of \( \mathbb{F_{q}} \), \( \mathbb{F_{q^2}}\) we can think of it as a polynomial \( a_0+a_1x \), where \( a_0\) and \( a_1 \) are elements in \( \mathbb{F_q} \). The addition is straightforward since we add the independent and linear terms separately. For multiplication, given elements $a$ and $b$&lt;br &#x2F;&gt;
\[ a\times b = a_0b_0 +(a_0b_1+a_1b_0)x+a_1b_1 x^2\]&lt;&#x2F;p&gt;
&lt;p&gt;To avoid the problem of going outside linear polynomials, we can reduce the degree by using an irreducible polynomial, such as \( x^2+1 \) and setting \( x^2+1=0 \). If we replace the above equation,&lt;br &#x2F;&gt;
\[ a\times b = a_0b_0 - a_1b_1 +(a_0b_1+a_1b_0)x\]&lt;br &#x2F;&gt;
which resembles multiplication in complex numbers.&lt;&#x2F;p&gt;
&lt;p&gt;The conditions for choosing the polynomial are:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. It must have the same degree as the extension field (quadratic in our case).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. It must be irreducible in the field we are extending, meaning that we cannot factor it into smaller degree polynomials.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Arithmetic in \( \mathbb{F_{q^{12}}} \) is complicated and expensive. Luckily, we can perform a sextic twist so that the group \( G_2 \) is defined over \( \mathbb{F_{q^{2}}} \).&lt;&#x2F;p&gt;
&lt;p&gt;In practice, when we want to build a field extension such as \( \mathbb{F_{q^{12}}} \), we can proceed by extending smaller fields in a sequential form: a tower of extensions (such as \( \mathbb{Q}\rightarrow \mathbb{R}\rightarrow \mathbb{C} \)).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;faster-with-fft&quot;&gt;Faster with FFT&lt;&#x2F;h2&gt;
&lt;p&gt;The potential use for FFTs comes when implementing the primitives &lt;code&gt;ECADD&lt;&#x2F;code&gt; and &lt;code&gt;ECDBL&lt;&#x2F;code&gt;. We can do these operations in different coordinate systems. As stated &lt;a href=&quot;&#x2F;need-for-speed-elliptic-curves-chapter&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;, projective coordinates are typically much faster because they avoid doing a finite field inversion, which is much more expensive than multiplications and additions.&lt;&#x2F;p&gt;
&lt;p&gt;When using projective coordinates, the calculations are faster because we trade divisions for multiplications. This means there are a lot of multiplications to be done, which is why we need efficient methods to multiply integers, where Karatsuba’s, Toom’s, or FFT algorithms might become appealing since we are dealing with &lt;a href=&quot;&#x2F;weird-ways-to-multiply-really-fast-with-karatsuba-toom-cook-and-fourier&#x2F;&quot;&gt;multiplication of large integers&lt;&#x2F;a&gt;. The optimal algorithm will depend on the size of the integers.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Verifiable AES: encryption using zero-knowledge proofs</title>
        <published>2023-01-07T00:00:00+00:00</published>
        <updated>2023-01-07T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/verifiable-encryption-using-zero-knowledge-proofs/"/>
        <id>https://blog.lambdaclass.com/posts/verifiable-encryption-using-zero-knowledge-proofs/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/verifiable-encryption-using-zero-knowledge-proofs/">&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;2023&#x2F;01&#x2F;download-1.jpeg&quot; alt=&quot;download-1&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;scope&quot;&gt;Scope&lt;&#x2F;h2&gt;
&lt;p&gt;At Lambdaclass, we want to try and test different cryptographic primitives that we can use to develop new products and applications to empower individuals and organizations, with an increased focus on security and requiring minimal trust between parties. In a series of posts, we will cover powerful primitives such as zero-knowledge proofs and fully homomorphic encryption, their applications, and use cases.&lt;&#x2F;p&gt;
&lt;p&gt;Encryption is transforming messages into random-looking texts to ensure confidentiality between two parties.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What is our objective here?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
We want to generate proof allowing us to verify an encryption algorithm, ensuring it does what it was designed for.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why do we need this?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
We need this so that the user does not need to trust that the other party performed the encryption correctly; we replace trust with cryptographic proofs. There a few use cases where the receiver doesn’t want to decrypt the message unless it’s an emergency. But at the same time the receiver needs to make sure the encryption was correctly done so that the message is there waiting for him to open it.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;When is this useful?&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
Whenever we want to receive unknown data from untrusted parties in a secure way and be sure that we are not being cheated.&lt;&#x2F;p&gt;
&lt;p&gt;The repository of the project can be found &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;AES_zero_knowledge_proof_circuit&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;general-introduction&quot;&gt;General Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;Two parties (we will call them Alice and Bob) can communicate securely by using encryption schemes. We can broadly classify encryption schemes into the following categories:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Private (symmetric) key encryption. Commonly used methods are AES and ChaCha20.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Public (asymmetric) key encryption. Commonly used methods are RSA and ElGamal.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In symmetric encryption, Alice and Bob must agree on a shared key before sending messages. The problem is how they can agree on something if they can’t send messages to each other securely? Luckily, key agreement schemes, such as Diffie-Hellman, allow them to choose a secret key. We will focus here on the Elliptic Curve Diffie-Hellman protocol. The key ingredients are a finite field $\mathbb{F_p}$ (where $p$ is a large prime) and an elliptic curve $\mathcal{C}$ defined over $\mathbb{F_p}$ (which contains a subgroup of prime order $r$ with a generator $g$). It consists of the following steps:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Alice chooses an element $s_A$ in $\mathbb{F_p}$ and computes her public key, $g_A=s_A g$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Alice sends her public key $g_A$ to Bob.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Bob chooses an ephemeral key, $s_B$ in $\mathbb{F_p}$, and computes his public key $g_B=s_B g$ and the shared secret $g_{AB}=s_B g_A=s_As_B g$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. Bob sends $g_B$ to Alice; Alice can also derive the shared secret by doing $g_{AB}=s_A g_B$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    5. They can calculate the symmetric key, $sk$, from the same key derivation function, $sk=KDF(g_{AB})$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Given a message $m$, Bob can encrypt it and send it to Alice by using a scheme such as AES and the key,&lt;br &#x2F;&gt;
$$c=E(m,sk)$$&lt;br &#x2F;&gt;
Any encryption scheme must satisfy the following consistency check:&lt;br &#x2F;&gt;
$$ m=D(E(m,sk),sk)=D(c,sk)$$&lt;&#x2F;p&gt;
&lt;h3 id=&quot;goal&quot;&gt;Goal&lt;&#x2F;h3&gt;
&lt;p&gt;Alice needs Bob to send her some secret, $sc$, which she does not know directly (otherwise, she would not need to communicate with Bob). She only knows a hash of $sc$ (for example, a Pedersen commitment). To send the secret, they need to agree on the key first. Then, Bob has to use that key to encrypt $sc$ and send it to Alice. The biggest problem is that Alice does not fully trust Bob. He could encrypt another message or use a different key. We want to develop a scheme where Bob can prove to Alice that he encrypted the message $sc$ using the key $sk$ without obviously leaking information about the key or the message.&lt;&#x2F;p&gt;
&lt;p&gt;Concisely, we can say that the goal is the following: Bob has to prove that the ciphertext $c$ is the result of the encryption of $m$ (whose commitment is $cm_m$) under a scheme (AES) using the symmetric key $sk$.&lt;&#x2F;p&gt;
&lt;p&gt;The following list shows all the input variables, indicating whether they are sensitive (should be secret) or not:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Secret: Bob&amp;#39;s ephemeral key, $s_B$, the message, $sc$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Public&#x2F;Not secret: ciphertext, $c$, Alice and Bob&amp;#39;s public keys, $g_A$ and $g_B$, the commitment to the message $cm_{sc}$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;All other elements, such as the curve and finite field, have been previously agreed on and are publicly known.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;steps&quot;&gt;Steps&lt;&#x2F;h3&gt;
&lt;p&gt;The following calculations would allow Bob to achieve his goal:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Using his ephemeral key, show that $g_B==s_B g$. If he succeeds, he gets a boolean variable, $b_1=1$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Using $s_B$ and $g_A$, he derives $sk$, encrypts $m$ and shows that $c==E(sc,sk)$. If this passes, he gets $b_2=1$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Using $sc$, he computes $cm_m$ and compares whether $cm_{sc}=\mathrm{commit}(sc)$. If this is correct, he gets $b_3=1$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. If $b_1 \wedge b_2 \wedge b_3=1$.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The big question is how can he prove all these conditions without revealing sensitive information? Here is where zero-knowledge proofs come into play.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-are-zero-knowledge-proofs&quot;&gt;What are zero-knowledge proofs?&lt;&#x2F;h2&gt;
&lt;p&gt;Zero-knowledge proofs (ZKP) are powerful cryptographic primitives which allow us to prove the validity of a statement or computation without revealing information other than the truth of the statement. We can represent any bounded computation as an arithmetic circuit, $C$. ZKP allow us to prove that we know some secret $w$ and public known values, $x$, such that $C(z=(x,w))=0$. In our case, the circuit is given by the computation performing checks 1-4. The variable $w$ contains $s_B$ and $sc$, $w=(s_B,sc)$. The public instance $x$ contains $g_A,g_B,c,cm_{sc}$ and the intended output, $1$, $x=(g_A,g_B,c,cm_{sc},1)$.&lt;&#x2F;p&gt;
&lt;p&gt;ZKP use polynomials and their properties to prove statements. Zk-SNARKs are ZKP with the following additional properties: succinctness (proofs are brief and faster to verify than naïve re-execution of the calculation) and non-interactive (prover and verifier do not need to exchange messages). There are two building blocks to most SNARKs: an information-theoretic device (most commonly, polynomial interactive oracle proofs, PIOP) and a cryptographic commitment scheme (in particular, polynomial commitment schemes, PCS). In this case, we will work with Marlin (PIOP) and the Kate-Zaverucha-Goldberg (KZG) commitment scheme.&lt;&#x2F;p&gt;
&lt;p&gt;The first step is to transform our code into arithmetic circuits or, equivalently, as a (quadratic) rank-one constraint system (R1CS). The latter is a system of equations of the form:&lt;br &#x2F;&gt;
$$ Az\cdot Bz=Cz$$&lt;br &#x2F;&gt;
where $A,B,C$ are matrices of the same size and $\cdot$ indicates the componentwise product.&lt;&#x2F;p&gt;
&lt;p&gt;Then, we will express these constraints as polynomials and generate the proof. Polynomial commitments come into play to ensure the prover does not cheat and make the protocol zero-knowledge. We will now focus on the proof generation and verification of step 2 (AES encryption).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;encryption-using-the-advanced-encryption-standard-aes&quot;&gt;Encryption using the Advanced Encryption Standard (AES)&lt;&#x2F;h2&gt;
&lt;p&gt;AES is a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Block_cipher&quot;&gt;block cipher&lt;&#x2F;a&gt;: it takes a 128-bit message (interpreted as a $4\times 4$ matrix of bytes) and a secret key, $sk$, and performs a pseudorandom permutation. AES has a round function, which is applied a fixed number of times, each using a different key, to encrypt the message. We use the key scheduling function to derive all the round keys from the master key, $sk$. The round function consists of the following operations:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Add a round key.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Substitute bytes (S-boxes).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Shift rows.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. Mix columns.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Each of these operations is necessary to guarantee that AES is secure. Repeating the operations in multiple rounds guarantees that elements are sufficiently shuffled and mixed, leading to semantic security (that is, we cannot learn anything about the plaintext just by looking at the ciphertext).&lt;&#x2F;p&gt;
&lt;p&gt;AES is described in the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;nvlpubs.nist.gov&#x2F;nistpubs&#x2F;FIPS&#x2F;NIST.FIPS.197.pdf&quot;&gt;NIST standard&lt;&#x2F;a&gt;. AES needs to use a mode to deal with messages of size greater than 128 bits. Some standard modes are AES-CBC (cipher block chaining) and AES-GCM (Galois counter mode, which provides authenticated encryption).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;add-round-key&quot;&gt;Add round key&lt;&#x2F;h3&gt;
&lt;p&gt;This is the step that makes the encryption depend on the key. For each round, a round key is derived from the master key ($sk$). The function is straightforward: it consists of an XOR operation between the round key and the state (the message or its transformations). To make it consistent with the code,&lt;br &#x2F;&gt;
$$ \mathrm{ret}=\mathrm{input_text}\oplus \mathrm{key} $$&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn add_round_key(input_text: &amp;amp;[u8], key: &amp;amp;[u8; 16]) -&amp;gt; [u8; 16] {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut ret = [0_u8; 16];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let _ = zip(input_text, key)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        .map(|(cell_i, key_i)| cell_i ^ key_i)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        .collect_slice(&amp;amp;mut ret[..]);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ret&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The XOR operation appears frequently in cryptography. Unless we know the key, we have a 50 % chance of guessing the correct value for each bit, which is as good as flipping a fair coin and guessing.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;substitute-bytes-s-boxes&quot;&gt;Substitute bytes &#x2F; S-boxes&lt;&#x2F;h3&gt;
&lt;p&gt;The S-boxes add the non-linear component to the block cipher. Here, each byte of the matrix is one-to-one mapped onto another byte. Here we present the complete code for the S-boxes, but this is done via a lookup table in practice. This will prove helpful in generating the proof since looking at the table requires fewer constraints than the whole operation.&lt;&#x2F;p&gt;
&lt;p&gt;In AES, we interpret bytes as polynomials of degree at most $7$, with coefficients in ${0,1}$. For example, the byte $10010110$ is interpreted as the polynomial $x7+x4+x^2+x$, and $00100001$ is $x^5+1$. We can multiply polynomials, but if the degree is larger than 7, we have to take the remainder of the product and the irreducible polynomial $m(x)=x8+x4+x^3+x+1$.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;fn rotate_left(byte: u8, n: u8) -&amp;gt; u8 {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    (byte &amp;lt;&amp;lt; n) | (byte &amp;gt;&amp;gt; (8 - n))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn substitute_byte(byte: u8) -&amp;gt; Result&amp;lt;u8&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    if byte == 0x00 {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        return Ok(0x63);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut p = 1_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut q = 1_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut sbox = [0_u8; 256];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;* loop invariant: p * q == 1 in the Galois field *&#x2F;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    loop {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &#x2F;* multiply p by 3 *&#x2F;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        p = p ^ (p &amp;lt;&amp;lt; 1_u8) ^ (((p &amp;gt;&amp;gt; 7_u8) &amp;amp; 1) * 0x1B);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &#x2F;* divide q by 3 (equals multiplication by 0xf6) *&#x2F;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        q ^= q &amp;lt;&amp;lt; 1_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        q ^= q &amp;lt;&amp;lt; 2_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        q ^= q &amp;lt;&amp;lt; 4_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        q ^= ((q &amp;gt;&amp;gt; 7_u8) &amp;amp; 1) * 0x09;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &#x2F;* compute the affine transformation *&#x2F;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        let xformed =&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            q ^ rotate_left(q, 1) ^ rotate_left(q, 2) ^ rotate_left(q, 3) ^ rotate_left(q, 4);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        let p_as_usize: usize = p.try_into()?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *sbox&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            .get_mut(p_as_usize)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            .to_anyhow(&amp;quot;Error saving substitution box value&amp;quot;)? = xformed ^ 0x63;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        if p == 1 {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            break;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let byte_index: usize = byte.try_into()?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Ok(*sbox&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        .get(byte_index)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        .to_anyhow(&amp;quot;Error getting substitution box value&amp;quot;)?)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In the first step, each byte is mapped to its multiplicative inverse. If $p(x)$ is the polynomial associated with the byte, there exists another polynomial $q(x)$ such that $p(x)q(x)\equiv 1 \pmod{m(x)}$ (there is $q(x)$ such that $m(x)$ divides $p(x)q(x)-1$). The only edge case is the 0 byte, which has no inverse and is mapped onto itself (this is the if at the beginning of the function).&lt;&#x2F;p&gt;
&lt;p&gt;The following steps give the calculation of the inverse:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let mut p = 1_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let mut q = 1_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;p = p ^ (p &amp;lt;&amp;lt; 1_u8) ^ (((p &amp;gt;&amp;gt; 7_u8) &amp;amp; 1) * 0x1B);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;q ^= q &amp;lt;&amp;lt; 1_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;q ^= q &amp;lt;&amp;lt; 2_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;q ^= q &amp;lt;&amp;lt; 4_u8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;q ^= ((q &amp;gt;&amp;gt; 7_u8) &amp;amp; 1) * 0x09;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Next, we perform an affine transformation on the inverse, which combines the bits at different positions:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let xformed =&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            q ^ rotate_left(q, 1) ^ rotate_left(q, 2) ^ rotate_left(q, 3) ^ rotate_left(q, 4);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This last operation consists of four left rotations and four XOR operations.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shiftrows&quot;&gt;ShiftRows&lt;&#x2F;h3&gt;
&lt;p&gt;This function changes the order of the elements in each row by performing a cyclic shift. The second row shifts each element one place to the left, the third one two places, and the fourth three. This transformation is linear; the constraints associated with it will also be linear.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn shift_rows(bytes: &amp;amp;[u8; 16], cs: &amp;amp;ConstraintSystemRef&amp;lt;ConstraintF&amp;gt;) -&amp;gt; Result&amp;lt;[u8; 16]&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Add each number to the constraint system.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in bytes {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        UInt8::new_witness(ark_relations::ns!(cs, &amp;quot;shift_rows_witness&amp;quot;), || Ok(byte))?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Turn the bytes into the 4x4 AES state matrix.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; The matrix is represented by a 2D array,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; where each array is a row.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; That is, let&amp;#39;s suppose that the flattened_bytes variable&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; is formed by the bytes&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; [b0, ..., b15]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Then the AES state matrix will look like this:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; b0, b4, b8, b12,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; b1, b5, b9, b13,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; b2, b6, b10, b14,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; b3, b7, b11, b15&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; And our array will look like this:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F;[&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F;  [b0, b4, b8, b12],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F;  [b1, b5, b9, b13],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F;  [b2, b6, b10,b14],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F;  [b3, b7, b11,b15]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut state_matrix = [[0_u8; 4]; 4];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for (i, state) in state_matrix.iter_mut().enumerate() {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *state = [&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *(bytes.get(i).context(&amp;quot;Out of bounds&amp;quot;))?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *(bytes.get(i + 4).context(&amp;quot;Out of bounds&amp;quot;)?),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *(bytes.get(i + 8).context(&amp;quot;Out of bounds&amp;quot;)?),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *(bytes.get(i + 12).context(&amp;quot;Out ouf bounds&amp;quot;)?),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        ];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Rotate every state matrix row (u8 array) as specified by&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; the AES cipher algorithm.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for (rotations, bytes) in state_matrix.iter_mut().enumerate() {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &#x2F;&#x2F; For the moment, this operation does not generate constraints in the&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &#x2F;&#x2F; circuit, but it should in the future.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        bytes.rotate_left(rotations);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Turn the rotated arrays into a flattened&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F;16-byte array, ordered by column.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut flattened_matrix = [0_u8; 16];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for i in 0..4 {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        for j in 0..4 {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *flattened_matrix&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                .get_mut((i * 4) + j)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                .to_anyhow(&amp;quot;Error getting element of flattened_matrix slice&amp;quot;)? = *state_matrix&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                .get(j)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                .to_anyhow(&amp;quot;Error getting element of state_matrix&amp;quot;)?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                .get(i)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                .to_anyhow(&amp;quot;Error getting element of state_matrix&amp;quot;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Ok(flattened_matrix)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h3 id=&quot;mixcolumns&quot;&gt;MixColumns&lt;&#x2F;h3&gt;
&lt;p&gt;The MixColumn function operates over each column of the state matrix. Every four-byte column is interpreted as a polynomial degree four polynomial, modulo $x^4+1$. We can view each column as the result of a linear combination. Each byte can be multiplied by 1, 2 or 3. If the result exceeds the modulus, we have to reduce the result, similar to what we did in substitute bytes.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;fn gmix_column(input: [u8; 4]) -&amp;gt; Option&amp;lt;[u8; 4]&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut b: [u8; 4] = [0; 4];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;* The array &amp;#39;a&amp;#39; is simply a copy of the input array &amp;#39;r&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;     * The array &amp;#39;b&amp;#39; is each element of the array &amp;#39;a&amp;#39; multiplied by 2&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;     * in Rijndael&amp;#39;s Galois field&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;     * a[n] ^ b[n] is element n multiplied by 3 in Rijndael&amp;#39;s Galois field *&#x2F;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for (i, c) in input.iter().enumerate() {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        let h = (c &amp;gt;&amp;gt; 7_usize) &amp;amp; 1; &#x2F;* arithmetic right shift, thus shifting in either zeros or ones *&#x2F;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *b.get_mut(i)? = (c &amp;lt;&amp;lt; 1_usize) ^ (h * 0x1B); &#x2F;* implicitly removes high bit because b[c] is an 8-bit char, so we xor by 0x1b and not 0x11b in the next line *&#x2F;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &#x2F;* Rijndael&amp;#39;s Galois field *&#x2F;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Some([&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        b.first()? ^ input.get(3)? ^ input.get(2)? ^ b.get(1)? ^ input.get(1)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        b.get(1)? ^ input.first()? ^ input.get(3)? ^ b.get(2)? ^ input.get(2)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        b.get(2)? ^ input.get(1)? ^ input.first()? ^ b.get(3)? ^ input.get(3)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        b.get(3)? ^ input.get(2)? ^ input.get(1)? ^ b.first()? ^ input.first()?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ])&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn mix_columns(input: &amp;amp;[u8; 16]) -&amp;gt; Option&amp;lt;[u8; 16]&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut ret = [0_u8; 16];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for (pos, column) in input.chunks(4).enumerate() {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        let column_aux = [&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *column.first()?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *column.get(1)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *column.get(2)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *column.get(3)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        ];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        let column_ret = gmix_column(column_aux)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &#x2F;&#x2F; put column_ret in ret:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *ret.get_mut(pos * 4)? = *column_ret.first()?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *ret.get_mut(pos * 4 + 1)? = *column_ret.get(1)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *ret.get_mut(pos * 4 + 2)? = *column_ret.get(2)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *ret.get_mut(pos * 4 + 3)? = *column_ret.get(3)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Some(ret)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h3 id=&quot;key-scheduling&quot;&gt;Key scheduling&lt;&#x2F;h3&gt;
&lt;p&gt;This function derives the round keys from the master key.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn derive_keys(secret_key: &amp;amp;[u8; 16]) -&amp;gt; Result&amp;lt;[[u8; 16]; 11]&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    const ROUND_CONSTANTS: [u32; 10] = [&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x01, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x02, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x04, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x08, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x10, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x20, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x40, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x80, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x1B, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        u32::from_be_bytes([0x36, 0x00, 0x00, 0x00]),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut result = [0_u32; 44];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    result[0] = to_u32(&amp;amp;secret_key[..4]).to_anyhow(&amp;quot;Error converting to u32&amp;quot;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    result[1] = to_u32(&amp;amp;secret_key[4..8]).to_anyhow(&amp;quot;Error converting to u32&amp;quot;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    result[2] = to_u32(&amp;amp;secret_key[8..12]).to_anyhow(&amp;quot;Error converting to u32&amp;quot;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    result[3] = to_u32(&amp;amp;secret_key[12..16]).to_anyhow(&amp;quot;Error converting to u32&amp;quot;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for i in 4..44 {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        if i % 4 == 0 {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            let substituted_and_rotated = to_u32(&amp;amp;substitute_word(rotate_word(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                *result.get(i - 1).to_anyhow(&amp;quot;Error converting to u32&amp;quot;)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            ))?)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            .to_anyhow(&amp;quot;Error converting to u32&amp;quot;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *result.get_mut(i).to_anyhow(&amp;quot;Error getting elem&amp;quot;)? =&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                (result.get(i - 4).to_anyhow(&amp;quot;Error getting elem&amp;quot;)? ^ (substituted_and_rotated))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                    ^ ROUND_CONSTANTS&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                        .get(i &#x2F; 4 - 1)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                        .to_anyhow(&amp;quot;Error getting elem&amp;quot;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        } else {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            *result.get_mut(i).to_anyhow(&amp;quot;Error getting elem&amp;quot;)? =&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                result.get(i - 4).to_anyhow(&amp;quot;Error getting elem&amp;quot;)?&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;                    ^ result.get(i - 1).to_anyhow(&amp;quot;Error getting elem&amp;quot;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut ret = [[0_u8; 16]; 11];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for (i, elem) in result.chunks(4).enumerate() {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        elem.iter()&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            .flat_map(|e| e.to_be_bytes())&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            .collect_slice(&amp;amp;mut ret.get_mut(i).to_anyhow(&amp;quot;Error getting elem&amp;quot;)?[..]);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Ok(ret)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;fn to_u32(value: &amp;amp;[u8]) -&amp;gt; Option&amp;lt;u32&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let array_aux: [u8; 4] = [&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *value.first()?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *value.get(1)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *value.get(2)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *value.get(3)?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Some(u32::from_be_bytes(array_aux))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;fn rotate_word(input: u32) -&amp;gt; [u8; 4] {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let bytes: [u8; 4] = input.to_be_bytes();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    [&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *bytes.get(1).unwrap_or(&amp;amp;0),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *bytes.get(2).unwrap_or(&amp;amp;0),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *bytes.get(3).unwrap_or(&amp;amp;0),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        *bytes.first().unwrap_or(&amp;amp;0),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;circuits-and-gadgets&quot;&gt;Circuits and gadgets&lt;&#x2F;h2&gt;
&lt;p&gt;If we tried hardcoding the circuit of AES, this would be an impossible task, given the kind and number of operations we have to perform. For example, suppose we want to perform the XOR operation between one byte of the message and the round key, $st[i] \oplus rk[i]=st^\prime [i]$. First, we need to decompose each byte into its constituent bits and check that each of them is indeed either $0$ or $1$:&lt;br &#x2F;&gt;
$st[i,j]\times st[i,j]=st[i,j]$&lt;br &#x2F;&gt;
$rk[i,j]\times rk[i,j]=rk[i,j]$&lt;br &#x2F;&gt;
$st^\prime[i,j]\times st\prime[i,j]=st\prime[i,j]$&lt;br &#x2F;&gt;
Next, we need to compute the XOR operation between bits,&lt;br &#x2F;&gt;
$2st[i,j]\times rk[i,j]=st[i,j]+rk[i,j]-st^\prime[i,j]$&lt;br &#x2F;&gt;
We have eight equations per byte, so there are 32 constraints for every byte XOR (we could remove the checks for $st^\prime$ since the XOR guarantees that they are 0 or 1, reducing the count to 24). Every add round key function takes 16 bytes, so we take 512 (or 384) constraints per round!&lt;&#x2F;p&gt;
&lt;p&gt;We can implement a gadget that adds the constraints corresponding to its binary decomposition whenever we define a new byte variable. We can also implement an XOR gadget between bytes, adding the constraints for the operation. The following code makes use of gadgets for &lt;code&gt;u8&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;use ark_r1cs_std::bits::uint8::UInt8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let a = UInt8::new_input(cs, || Ok(1))?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let result = a.xor(&amp;amp;a)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let zero = UInt8::constant(0);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;result.enforce_equal(&amp;amp;zero)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;What happens with the substitution boxes? We could implement a gadget for the whole operation. The problem is that the number of constraints scales super fast! There are more than 10 XOR operations per step, which is time-consuming. The s-boxes are generally obtained from a lookup table, which has all the possible output values precomputed.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;fn substitute_byte(byte: &amp;amp;UInt8Gadget, lookup_table: &amp;amp;[UInt8Gadget]) -&amp;gt; Result&amp;lt;UInt8Gadget&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Ok(UInt8Gadget::conditionally_select_power_of_two_vector(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;byte.to_bits_be()?,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        lookup_table,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    )?)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn substitute_bytes(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    bytes: &amp;amp;[UInt8Gadget],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    lookup_table: &amp;amp;[UInt8Gadget],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;) -&amp;gt; Result&amp;lt;Vec&amp;lt;UInt8Gadget&amp;gt;&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ensure!(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        bytes.len() == 16,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;quot;Input must be 16 bytes length when substituting bytes&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    );&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut substituted_bytes = vec![];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in bytes {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        substituted_bytes.push(substitute_byte(byte, lookup_table)?);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ensure!(substituted_bytes.len() == 16, &amp;quot;Error substituting bytes&amp;quot;);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Ok(substituted_bytes)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;proof-generation&quot;&gt;Proof generation&lt;&#x2F;h2&gt;
&lt;p&gt;The first step to generating the proof is to obtain the proving and verification keys. These are derived from the structured reference string (SRS) obtained from a secure multiparty computation.&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let (proving_key, verifying_key) = synthesize_keys(message_length)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Here is the definition of the synthesize keys function:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn synthesize_keys(plaintext_length: usize) -&amp;gt; Result&amp;lt;(ProvingKey, VerifyingKey)&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let rng = &amp;amp;mut simpleworks::marlin::generate_rand();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let universal_srs =&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        simpleworks::marlin::generate_universal_srs(1_000_000, 250_000, 3_000_000, rng)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let constraint_system = ConstraintSystem::&amp;lt;ConstraintF&amp;gt;::new_ref();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let default_message_input = vec![0_u8; plaintext_length];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let default_secret_key_input = [0_u8; 16];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let default_ciphertext_input = vec![0_u8; plaintext_length];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut message_circuit: Vec&amp;lt;UInt8Gadget&amp;gt; = Vec::with_capacity(default_message_input.len());&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in default_message_input {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        message_circuit.push(UInt8Gadget::new_witness(constraint_system.clone(), || {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            Ok(byte)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        })?);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut secret_key_circuit: Vec&amp;lt;UInt8Gadget&amp;gt; =&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        Vec::with_capacity(default_secret_key_input.len());&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in default_secret_key_input {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        secret_key_circuit.push(UInt8Gadget::new_witness(constraint_system.clone(), || {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            Ok(byte)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        })?);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut ciphertext_circuit: Vec&amp;lt;UInt8Gadget&amp;gt; =&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        Vec::with_capacity(default_ciphertext_input.len());&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in default_ciphertext_input {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        ciphertext_circuit.push(UInt8Gadget::new_input(constraint_system.clone(), || {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            Ok(byte)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        })?);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let _ciphertext = encrypt_and_generate_constraints(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;message_circuit,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;secret_key_circuit,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;ciphertext_circuit,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        constraint_system.clone(),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    );&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    simpleworks::marlin::generate_proving_and_verifying_keys(&amp;amp;universal_srs, constraint_system)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Since this is only a test, we generate the SRS from a function instead of reading it from the result of the multiparty computation.&lt;&#x2F;p&gt;
&lt;p&gt;We now define a function that contains all the steps to generate the proof:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn encrypt(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    message: &amp;amp;[u8],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    secret_key: &amp;amp;[u8; 16],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ciphertext: &amp;amp;[u8],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    proving_key: ProvingKey,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;) -&amp;gt; Result&amp;lt;MarlinProof&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let rng = &amp;amp;mut simpleworks::marlin::generate_rand();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let constraint_system = ConstraintSystem::&amp;lt;ConstraintF&amp;gt;::new_ref();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut message_circuit: Vec&amp;lt;UInt8Gadget&amp;gt; = Vec::with_capacity(message.len());&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in message {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        message_circuit.push(UInt8Gadget::new_witness(constraint_system.clone(), || {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            Ok(byte)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        })?);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut secret_key_circuit: Vec&amp;lt;UInt8Gadget&amp;gt; = Vec::with_capacity(secret_key.len());&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in secret_key {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        secret_key_circuit.push(UInt8Gadget::new_witness(constraint_system.clone(), || {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            Ok(byte)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        })?);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut ciphertext_circuit: Vec&amp;lt;UInt8Gadget&amp;gt; = Vec::with_capacity(ciphertext.len());&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in ciphertext {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        ciphertext_circuit.push(UInt8Gadget::new_input(constraint_system.clone(), || {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            Ok(byte)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        })?);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    encrypt_and_generate_constraints(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;message_circuit,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;secret_key_circuit,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;ciphertext_circuit,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        constraint_system.clone(),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    )?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Here we clone the constraint system because deep down when generating&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; the proof the constraint system is consumed and it has to have one&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; reference for it to be consumed.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let cs_clone = (*constraint_system&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        .borrow()&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        .ok_or(&amp;quot;Error borrowing&amp;quot;)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        .map_err(|e| anyhow!(&amp;quot;{}&amp;quot;, e))?)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    .clone();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let cs_ref_clone = ConstraintSystemRef::CS(Rc::new(RefCell::new(cs_clone)));&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let proof = simpleworks::marlin::generate_proof(cs_ref_clone, proving_key, rng)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    Ok(proof)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Finally, we run the following lines to get the proof:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let message = [1_u8; 16]; \\Example message&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let secret_key = [0_u8; 16]; \\Example key&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let proof = encrypt(&amp;amp;message, &amp;amp;secret_key, &amp;amp;primitive_ciphertext, proving_key)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;verification&quot;&gt;Verification&lt;&#x2F;h2&gt;
&lt;p&gt;To verify the proof, we first encapsulate all the steps in this function, reading the verifying key, the proof and the ciphertext:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;pub fn verify_encryption(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    verifying_key: VerifyingKey,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    proof: &amp;amp;MarlinProof,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    ciphertext: &amp;amp;[u8],&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;) -&amp;gt; Result&amp;lt;bool&amp;gt; {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let mut ciphertext_as_field_array = vec![];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    for byte in ciphertext {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        let field_array = byte_to_field_array(*byte);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        for field_element in field_array {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;            ciphertext_as_field_array.push(field_element);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    }&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    simpleworks::marlin::verify_proof(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        verifying_key,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;ciphertext_as_field_array,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        proof,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &amp;amp;mut simpleworks::marlin::generate_rand(),&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    )&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Then, we run and check the result&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;let result = verify_encryption(&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    verifying_key,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &amp;amp;proof,&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &amp;amp;primitive_ciphertext&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;)?;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;assert!(result);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;AES is the most widely used encryption method. In this post, we addressed the problem of offering cryptographic proof for the correct execution of the AES encryption function for a given plaintext-key pair. Using the Arkworks library, we implemented AES and obtained its representation as an R1CS. Afterward, using Marlin and the Kate-Zaverucha-Goldberg polynomial commitment scheme, we generated a cryptographic proof. The verifier, using the ciphertext as input, can verify the proof to assert the correct execution of the function.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Fully private applications: A ZEXE protocol</title>
        <published>2023-01-05T00:00:00+00:00</published>
        <updated>2023-01-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/fully-private-applications-a-zexe-protocol/"/>
        <id>https://blog.lambdaclass.com/posts/fully-private-applications-a-zexe-protocol/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/fully-private-applications-a-zexe-protocol/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;One of the key issues in the current world is how to achieve consensus between trustless parties. Distributed ledgers have become popular since the advent of cryptocurrencies, built over a technology known as blockchain. One of the main problems is that these ledgers offer limited privacy and are quite constrained on the kind of programs they can run. Aleo provides a full-stack approach for writing private applications. One of its core components is the ZEXE protocol, which is the first ledger system in which applications can be run privately, trustlessly, and can be easily scaled.&lt;&#x2F;p&gt;
&lt;p&gt;As we mentioned before, ledger-based systems can support rich applications, but often suffer from two main drawbacks:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. Validating a transaction requires re-executing the state transition it refers to.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. Transactions are not private; they reveal information about users and the state of the system.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The latter creates a large number of issues where privacy is critical, since it may leak relevant information regarding one’s medical history, payment records, acquaintances and trading partners, etc, which can be used to the advantage of malicious parties.&lt;&#x2F;p&gt;
&lt;p&gt;The first drawback, on the other side, creates scalability issues, since every transition has to be recomputed by every device forming the network (which can have very different computational power), with the weakest one acting as a bottleneck. This has led to the introduction of mechanisms such as gas to make users pay more for expensive computations and discourage denial-of-service attacks.&lt;&#x2F;p&gt;
&lt;p&gt;Some protocols, such as Zerocash, provide privacy-preserving payments and Hawk allows for state transitions where private data remains hidden from third parties. We can say that they achieve data-privacy, but not function privacy, because the transition function being executed is not hidden (even though the input and output parameters may be secret). Function privacy means that an observer is unable to distinguish between different computations performed offline from one another.&lt;&#x2F;p&gt;
&lt;p&gt;ZEXE’s aim is to provide a scalable solution to these problems, where both data and function privacy are achieved. This can act as a solid foundation for new forms of data sharing, financial systems, and governance. The main ideas are based on the following:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. We can run programs offline (or delegate their execution to a powerful but trustless server) and obtain a proof attesting to the validity of the computation.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. We can quickly verify the validity of a computation or transition by checking the proof; this operation will be less computationally expensive than performing the whole computation.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. Transitions can be accepted into the layer by checking the proofs.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The proofs will have to satisfy two properties for this system to work:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Privacy: The proofs should not reveal anything, other than the validity of the statement.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Succinctness: the proof can be validated in a time that is independent of the cost of the computation to whose correctness it attests.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;ZEXE will offer users rich functionality, with offline computations used to realize state transitions of multiple applications, running atop the same ledger. The shared execution environment provides the following properties:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Extensibility: users can run arbitrary functions, without seeking anyone&amp;#39;s permission.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Isolation: functions of malicious users cannot interfere with the computations and data of honest users.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Interprocess communication: functions may exchange data with one another.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In the next section, we will cover the main ingredients and how the protocol works: terms such as zk-SNARKs, elliptic curve cryptography, pairings, etc, will be demystified.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ingredients&quot;&gt;Ingredients&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;decentralized-private-computation&quot;&gt;Decentralized private computation&lt;&#x2F;h3&gt;
&lt;p&gt;The core of ZEXE relies on a new cryptographic primitive to perform computations on a ledger, known as decentralized private computation (DPC), by extending the ideas on how Zerocash works. We can perform the computations offline and present a proof asserting that it is a valid transition on the ledger; the proof can be quickly verified by the nodes of the ledger (much faster than it would take each of them to repeat our original calculation) and be accepted. One disadvantage is that, even though proofs are fast to verify, their generation can be quite expensive (remember that we want to allow the user to run arbitrary programs; thus, the proof system should be able to cope with many different kinds of statements and instructions). We can leverage the construction and create a delegable DPC: we can make trustless servers or devices carry out computations and provide us with proofs that the computations were performed as they should and without leaking relevant information.&lt;&#x2F;p&gt;
&lt;p&gt;The building blocks of the DPC schemes are:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Collision-resistant hash function.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Pseudorandom function family.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Commitments.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * NIZK: non-interactive arguments of knowledge (the proofs).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To enable delegatable DPC we need a further ingredient: randomized signatures.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;records&quot;&gt;Records&lt;&#x2F;h3&gt;
&lt;p&gt;In Zerocash, when coins are created, their &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Commitment_scheme&quot;&gt;commitments&lt;&#x2F;a&gt; (1) are published on the ledger; when they are consumed, their serial number is published. Every transaction tells that some “old” coins were consumed to create “new” coins: it has the serial number of the spent coins, the commitments of the “new” coins, and a proof that the values of the “old” and “new” coins add up (the proof shows that it was a valid transaction and that no extra money was created or destroyed during the exchange). The transaction is private because we don’t know the values or addresses of the coins exchanged. Since the serial number is published, no coin can be spent more than once.&lt;&#x2F;p&gt;
&lt;p&gt;The units of data, called records (the coins in ZEXE), are bound to arbitrary programs and specify the conditions under which a record can be created and consumed. We can think of them as having tokens or coins that we can spend to run programs and get proofs that what we have done is valid (like in arcade games). To extend the idea to arbitrary functions, we can think of a record as storing some arbitrary data payload. The commitment of a record is published whenever a record is created and its serial number is revealed when it is consumed. A transaction on the ledger contains information on the records spent and created during the operation and a proof that invoking a function on the data payload of the old record produces the data payload of the new records.&lt;&#x2F;p&gt;
&lt;p&gt;A record structure contains:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The address public key.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The data payload.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Birth and death predicates.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * A serial number nonce.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The record&amp;#39;s commitment.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The record’s commitment is a commitment to all the aforementioned attributes (public key, payload, birth and death predicates, and the serial nonce).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;the-record-nano-kernel-rnk&quot;&gt;The Record Nano Kernel (RNK)&lt;&#x2F;h3&gt;
&lt;p&gt;This is an execution environment operating over the records. We can think of it as a kind of operating system for the ledger. It provides process isolation, data ownership, handling of interprocess communications, etc. The RNK ensures that birth and death predicates are met so that during the record’s lifetime certain constraints are enforced. In other words, depending on the input data, predicates can decide whether certain interactions with that record are allowed or not.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;transitions-and-transactions&quot;&gt;Transitions and transactions&lt;&#x2F;h3&gt;
&lt;p&gt;On the ledger, transactions contain the following information:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. The serial number of all consumed records during the transaction.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. The commitments of all the records created in the transaction.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    3. A memorandum. This is a string associated with the transaction.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    4. Other construction-specific information.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Recently, transactions have been updated and are made up of transitions. In other words, a transaction is composed of several transitions.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;zk-snarks&quot;&gt;zk-SNARKs&lt;&#x2F;h3&gt;
&lt;p&gt;zero-knowledge succinct non-interactive arguments of knowledge (zk-SNARKs for friends) are cryptographic primitives which allow one party (the prover) to convince another one (the verifier) of the validity of a certain statement&#x2F;calculation. It has the following properties:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Completeness: Given a statement and a witness (for example, I know $x$ such that $g^x=b$), the prover can convince an honest verifier.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Soundness: A malicious prover cannot convince the verifier of a false statement.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Zero-knowledge: the proof reveals nothing else other than the validity of the statement; it does not reveal the witness.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Succinctness: the proof is small and &amp;quot;easy&amp;quot; to verify.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Given that we want to let users perform arbitrary computations, we need the proof system to be able to handle lots of different statements in a rather general way; this will represent the largest cost in the ZEXE protocol. These statements fall in the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;NP_(complexity)&quot;&gt;class NP&lt;&#x2F;a&gt; (non-deterministic polynomial time), which are problems that can be efficiently verified in polynomial time. The NP statements that we need to prove contain predicates defined by the user, which would force us to build everything on zk-SNARKs for universal computations, which depend on very expensive tools. An advantage of zk-SNARKs, verification is done in constant-time; in other words, the amount of time needed to verify is independent of the size of the computation. This is a desirable property from the point of view of privacy because different verification times could give hints on what kind of operations are being performed.&lt;&#x2F;p&gt;
&lt;p&gt;To tackle this problem, the protocol relies on recursive proof composition: instead of checking the arbitrary NP statement, we can check succinct proofs attesting to the validity of the statement. This way, we can avoid zk-SNARKs for universal computations and can instead focus on succinct proofs, which can be hardcoded in the statement. We can achieve the goal by making use of &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2012&#x2F;095.pdf&quot;&gt;proof carrying data&lt;&#x2F;a&gt;: we append to a message a succinct proof that asserts that the result is consistent. For example, instead of checking directly the birth and death predicates (which can be quite general), we can verify succinct proofs $\pi_b$ and $\pi_d$ attesting to the satisfaction of these predicates. Since the inner proofs are succinct, it is (relatively) inexpensive to verify them. Moreover, since the outer proofs are zero knowledge (therefore, not revealing anything that is used to generate the proof), the inner proofs need not be zero-knowledge, further simplifying the calculations.&lt;&#x2F;p&gt;
&lt;p&gt;We can reduce any NP statement to an equivalent NP-complete problem, such as graph-coloring or boolean circuit satisfiability. ZEXE proves the correctness of computations by transforming our arbitrary program into an arithmetic circuit satisfiability problem, defined over a &lt;a href=&quot;&#x2F;math-survival-kit-for-developers&#x2F;&quot;&gt;finite field&lt;&#x2F;a&gt; $\mathbb{F}_r$. The problem that arises is that proof verifications involve operations over field $\mathbb{F}_q$, where $r \neq q$. It is, in principle, possible to simulate operations in $\mathbb{F}_q$ over $\mathbb{F}_r$, but this is quite expensive and would make the whole system burdensome. An alternative to this is working with a pair of &lt;a href=&quot;&#x2F;what-every-developer-needs-to-know-about-elliptic-curves&#x2F;&quot;&gt;elliptic curves&lt;&#x2F;a&gt;, with some desired properties; we call them pairing-friendly elliptic curves.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;pairing-friendly-elliptic-curves&quot;&gt;Pairing-friendly Elliptic Curves.&lt;&#x2F;h3&gt;
&lt;p&gt;Given an elliptic curve $E$ defined over some finite field $\mathbb{F}_q$, we can define an operation over the points of the curve such that they form a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;math-survival-kit-for-developers&#x2F;&quot;&gt;group&lt;&#x2F;a&gt; under that operation. The order of the subgroup $\mathbb{G}$ (that is, the number of elements) is $r$, with $r \neq q$. Two prime order curves $E_1$ and $E_2$ over fields $\mathbb{F}_q$ and $\mathbb{F}_r$ are said to be pairing friendly if the size of one’s base field $\mathbb{F}$ equals the other’s subgroup order and vice versa.&lt;&#x2F;p&gt;
&lt;p&gt;An elliptic curve pairing is a function $e:\mathbb{G}_1\times \mathbb{G}_2 \rightarrow \mathbb{G}_T$ that is bilinear. Here, $\mathbb{G}_1$ and $\mathbb{G}_2$ are the groups over elliptic curves. Bilinear means that, given two points $\mathcal{P_1}$ and $\mathcal{Q}_1$ in $\mathbb{G}_1$ and $\mathcal{P_2}$ and $\mathcal{Q}_2$ in $\mathbb{G}_2$ the following properties hold (we will write all the group operations as additions):&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    1. $e(\mathcal{P_1}+\mathcal{Q}_1,\mathcal{P}_2)=e(\mathcal{P_1},\mathcal{P}_2)+e(\mathcal{Q}_1,\mathcal{P}_2)$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    2. $e(\mathcal{P_1},\mathcal{P}_2+\mathcal{Q}_2)=e(\mathcal{P_1},\mathcal{P}_2)+e(\mathcal{P}_1,\mathcal{Q}_2)$&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;For efficiency reasons, we need both fields to have subgroups whose orders are large powers of $2$. ZEXE uses a curve from the Barreto-Scott-Lynn family, $E_{BLS}$ (with embedding degree(2) 12), which conservatively achieves 128 bits of security. The pairing friendly curve is generated via the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2006&#x2F;372.pdf&quot;&gt;Cocks-Pinch method&lt;&#x2F;a&gt;, $E_{CP}$. This is a very time consuming step since it involves exploring many different curves until we find one with the desired properties.&lt;&#x2F;p&gt;
&lt;p&gt;Given that the base field of $E_{CP}$ is larger than that of $E_{BLS}$, operations over the former are more expensive. To avoid this shortcoming, the relation $R_e$ is split into two: $R_{BLS}$ and $R_{CP}$. The last one is responsible for verifying proofs of predicates’ satisfaction, while all other checks depend on the $E_{BLS}$ curve.&lt;&#x2F;p&gt;
&lt;p&gt;Commitments and collision-resistant hash functions can be expressed as efficient arithmetic circuits for Pedersen-type constructions over Edwards curves. Therefore, two additional curves, $E_{Ed,BLS}$ and $E_{Ed,CP}$ over the fields $\mathbb{F}_r$ and $\mathbb{F}_q$ are selected, so as to implement important cryptographic primitives, such as hashing, commitments, and randomizable signatures. This allows us to reduce the difficulty of the multiple checks for NP relations.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;ZEXE is a protocol that was designed to allow users to execute arbitrary programs over public ledgers, without compromising privacy. It solves two of the main drawbacks of distributed ledgers so far: First, computations can be performed offline and a proof of the correct computation is submitted to the ledger. Since the proof is fast to verify, this avoids the problem of naïve re-execution and gives scalable solutions. Second, it achieves both data and function privacy: observers cannot get information over the data involved in the computations, not even on which specific functions are being called.&lt;&#x2F;p&gt;
&lt;p&gt;The protocol introduces new cryptographic primitives, such as DPC and delegatable DPC; the latter allows users with less powerful devices (such as smartphones) to hand their computations to untrusted parties and get proofs that show that the results obtained correspond to the correct execution of the program. These are supported by zk-SNARKs, relying on elliptic curve pairings and tools for converting arbitrary programs to an arithmetic circuit, where we can check the validity of the calculations.&lt;&#x2F;p&gt;
&lt;p&gt;It gives the basis for fully private applications, becoming an ideal platform for decentralized applications, such as finance, gaming, authentication, governance, and more.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;&#x2F;h2&gt;
&lt;p&gt;(1) A commitment allows a user to commit to one value, with the ability to later reveal it. For example, in a roulette bet, I could choose “25” (I really feel very confident) and with the commitment, I am bound to my choice of “25” (though nobody could know, a priori, that I chose 25 since it is hidden). A way to achieve this is by using a collision resistant hash function and publishing the resulting hash (to make it work, we need to add something else, otherwise we can hash all the possibilities and see which one has the corresponding hash). If I then try to change my bet, the hash will not match with that of my original bet.&lt;br &#x2F;&gt;
(2) The embedding degree of an elliptic curve over the field $\mathbb{F}_q$ is the smallest positive integer $k$ such that $q^k-1$ is divisible by $r$, the order of the group. The embedding degree should be high so that the discrete logarithm problem is hard to solve. However, if $k$ is too large, then the arithmetic over the curves becomes much slower.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Fully-homomorphic encryption, zero-knowledge proofs, and multiparty computation</title>
        <published>2022-12-16T00:00:00+00:00</published>
        <updated>2022-12-16T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/fully-homomorphic-encryption-zero-knowledge-proofs-and-multiparty-computation/"/>
        <id>https://blog.lambdaclass.com/posts/fully-homomorphic-encryption-zero-knowledge-proofs-and-multiparty-computation/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/fully-homomorphic-encryption-zero-knowledge-proofs-and-multiparty-computation/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;Cloud computing and storage have changed the way businesses and people use, store and manage their data. Data is securely stored in an encrypted form, typically using a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;symmetric-encryption&#x2F;&quot;&gt;symmetric key encryption scheme&lt;&#x2F;a&gt;, such as AES or ChaCha20. However, to perform data analytics, we have to either give the key to the server so that it can decrypt it or we have to download it, decrypt it, and run the calculations on our own, which can be costly, requiring lots of time or memory. Fully homomorphic encryption (FHE) allows us to delegate computations involving encrypted data to untrusted third parties, without any need to decrypt them first.&lt;&#x2F;p&gt;
&lt;p&gt;Even if this is a very powerful cryptographic primitive, we still face a big challenge: how do we know that the third party performed the calculation it was supposed to do? This is where zero-knowledge proofs (ZKP) come into play. ZKP allow us to prove the integrity of a given computation, without the need to re-execute it. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;the-hunting-of-the-zk-snark&#x2F;&quot;&gt;zk-SNARKs&lt;&#x2F;a&gt; (succinct non-interactive arguments of knowledge) yield short proofs which can be verified very fast and have applications in decentralized ledgers (solving both privacy and scalability issues) and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;decentralized-private-computation-zexe-and-veri-zexe&#x2F;&quot;&gt;decentralized private computations&lt;&#x2F;a&gt;. They also face some challenges: generating proofs for arbitrary computations can be expensive and users with less powerful devices may not be able to generate them. Many zk-SNARKs require trusted setups, which should be generated by an honest party to ensure that nobody can cheat and generate fake proofs.&lt;&#x2F;p&gt;
&lt;p&gt;Both of them can be solved by multiparty computation (MPC). In this scheme, the generation of the proof or the establishment of the trusted setup is entrusted to various parties, which could have partial access to the data. In the case of setup ceremonies, as long as one of the parties involved is honest, the setup is secure. MPC can also be used, with decentralized ledgers, to ensure that anyone can participate in setup ceremonies and prevent malicious parties from blocking honest participants, using proofs with transparent setups (such as STARKs - scalable, transparent argument of knowledge).&lt;&#x2F;p&gt;
&lt;p&gt;Proof generation can be carried out by multiple servers, each of them having partial information on the secret inputs. Each party can submit proof attesting to the correctness of the proof generation protocol. Multiparty computation can also be used to perform calculations among different parties, each of them having different pieces of information relevant to the problem, such as financial information between banks or health-related information for health service providers. FHE helps parties share information and make calculations without revealing it or train machine learning models without compromising sensitive data.&lt;&#x2F;p&gt;
&lt;p&gt;It is clear from all the above that FHE, ZKP, and MPC have many points in common and each has something to offer to the other. ZKP can provide integrity of computations, FHE allows data sharing and calculation without compromising it and MPC gives us the power to delegate expensive computations to other parties. These open the doors for many new and exciting applications in finance, health, and medical sectors, with an emphasis on data privacy and decentralization.&lt;&#x2F;p&gt;
&lt;p&gt;We will now explain the basic idea behind each of these primitives.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;fully-homomorphic-encryption&quot;&gt;Fully Homomorphic encryption&lt;&#x2F;h2&gt;
&lt;p&gt;Fully homomorphic encryption is a form of encryption where we can perform operations with encrypted data and the result of those operations is the encrypted form of an operation involving the ciphertext. For example, in the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;how-to-create-your-own-crappy-rsa-as-a-software-developer&#x2F;&quot;&gt;RSA cryptosystem&lt;&#x2F;a&gt;, we encrypt a message &lt;em&gt;m&lt;&#x2F;em&gt; using the public key by taking&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;E(m)=m e(&lt;&#x2F;em&gt; mod _ &lt;em&gt;n&lt;&#x2F;em&gt;)_&lt;&#x2F;p&gt;
&lt;p&gt;Now suppose that we have &lt;em&gt;m 1&lt;&#x2F;em&gt;,&lt;em&gt;m 2&lt;&#x2F;em&gt; numbers and we want to compute their product &lt;em&gt;m 1&lt;&#x2F;em&gt;× &lt;em&gt;m 2&lt;&#x2F;em&gt;. We can see that if we perform the product and then encrypt it, we get&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;E(m 1&lt;&#x2F;em&gt;× &lt;em&gt;m 2&lt;&#x2F;em&gt;)=(&lt;em&gt;m 1&lt;&#x2F;em&gt;× &lt;em&gt;m 2&lt;&#x2F;em&gt;)e  (mod   &lt;em&gt;n&lt;&#x2F;em&gt;)&lt;&#x2F;p&gt;
&lt;p&gt;If we take the product of the encrypted forms of m1,m2, then&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;E(m 1)&lt;&#x2F;em&gt;× &lt;em&gt;E(m 2&lt;&#x2F;em&gt;)=&lt;em&gt;m e1&lt;&#x2F;em&gt;× &lt;em&gt;m e2&lt;&#x2F;em&gt;=(&lt;em&gt;m 1&lt;&#x2F;em&gt;× &lt;em&gt;m 2&lt;&#x2F;em&gt;)e   (mod &lt;em&gt;n&lt;&#x2F;em&gt;)&lt;&#x2F;p&gt;
&lt;p&gt;which is the same as calculating first the product and then encrypting. The operation in the encrypted space need not be the same as the one in the original. Given this property of the RSA cryptosystem, many researchers started wondering whether it could be possible to build a fully homomorphic encryption scheme. The first FHE scheme was presented in 2009 by &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;crypto.stanford.edu&#x2F;craig&#x2F;craig-thesis.pdf&quot;&gt;Craig Gentry&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;math-interlude-homomorphisms&quot;&gt;Math interlude: Homomorphisms&lt;&#x2F;h3&gt;
&lt;p&gt;To be more precise, a homomorphism is a function between two &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;math-survival-kit-for-developers&#x2F;&quot;&gt; algebraic structures&lt;&#x2F;a&gt; (such as two groups, two rings, two vector spaces) and preserves their structure. If you had a course on linear algebra, linear transformations are examples of homomorphisms. In the context of groups, suppose we have two groups (𝔾1,⋅) and (𝔾2,⊕), each with their binary operation (it could be multiplication, addition, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;what-every-dev-needs-to-know-about-elliptic-curves&#x2F;&quot;&gt; elliptic curve addition&lt;&#x2F;a&gt;, function composition, etc). A function &lt;em&gt;f&lt;&#x2F;em&gt; :𝔾1→𝔾2 is an homomorphism if, given &lt;em&gt;x&lt;&#x2F;em&gt; ,&lt;em&gt;y&lt;&#x2F;em&gt; in 𝔾1 we have&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;f(x⋅y)  = f(x)&lt;&#x2F;em&gt; ⊕ &lt;em&gt;f(y)&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Note that the operation between the images &lt;em&gt;f(x),f(y)&lt;&#x2F;em&gt; is the operation over 𝔾2. We also saw examples of homomorphisms between rings, when we defined modular arithmetic: we have a function preserving addition and multiplication from the set of integers with the usual operations, (ℤ,+,×) and the ring of integers modulo &lt;em&gt;p&lt;&#x2F;em&gt; , (ℤ&#x2F;&lt;em&gt;p&lt;&#x2F;em&gt; ℤ,⊕,⋅) (we use different symbols for addition and multiplication to remember that these are modulo &lt;em&gt;p&lt;&#x2F;em&gt;). For example, if we take &lt;em&gt;p&lt;&#x2F;em&gt;  = 7, we have ℤ&#x2F;&lt;em&gt;p&lt;&#x2F;em&gt; ℤ={0,1,2,3,4,5,6}. We can see that: −5+3=−2. This is related to 2 ⊕ 3 ≡ 5 (mod7), where 2 is the element corresponding to −5 in ℤ&#x2F;&lt;em&gt;p&lt;&#x2F;em&gt; ℤ, 3 corresponds to itself and 5 is congruent to −2.−3×4=−12 which relates to 4⋅4≡2(mod7) in the same way as before.&lt;&#x2F;p&gt;
&lt;p&gt;It is important to see that homomorphisms are not necessarily one-to-one functions (the last ring homomorphism shows a clear example). If the homomorphism is a bijective function, it is called an isomorphism. The following is an example of an isomorphism from the real numbers ℝ with addition to the positive real numbers equipped with multiplication, ℝ +,  &lt;em&gt;f&lt;&#x2F;em&gt;  : ℝ→ℝ+ , &lt;em&gt;f  (x)&lt;&#x2F;em&gt; = exp &lt;em&gt;(x)&lt;&#x2F;em&gt;  , with its inverse, &lt;em&gt;f −1&lt;&#x2F;em&gt; : ℝ+→  ℝ,  &lt;em&gt;f&lt;&#x2F;em&gt; −1 (&lt;em&gt;z&lt;&#x2F;em&gt;) = ln(&lt;em&gt;z&lt;&#x2F;em&gt;). You can easily check that &lt;em&gt;f   (x + y )=f (x) ⋅ f (y)&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In the context of cryptography, we would like to have encryption or commitment schemes preserving some operations. For example, the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;the-hunting-of-the-zk-snark&#x2F;&quot;&gt; Kate-Zaverucha-Goldberg &lt;&#x2F;a&gt; (KZG) commitment scheme is additively homomorphic. The commitment takes polynomials (which we can think of as a group with ordinary polynomial addition, (ℙ,+)) and maps them into elliptic curve points (which also have a group structure, with elliptic curve addition, (𝔾,⊕)). We can verify that&lt;&#x2F;p&gt;
&lt;p&gt;cm(&lt;em&gt;p&lt;&#x2F;em&gt; 1 (&lt;em&gt;x&lt;&#x2F;em&gt;)  +  &lt;em&gt;p 2&lt;&#x2F;em&gt;(x))  =  cm(&lt;em&gt;p &lt;em&gt;1   (&lt;em&gt;x&lt;&#x2F;em&gt;))  ⊕  cm(&lt;em&gt;p 2&lt;&#x2F;em&gt;(x))&lt;&#x2F;em&gt;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This property is useful for &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;proof-aggregation-schemes-snarkpack-and-aplonk&#x2F;&quot;&gt;proof aggregation&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;incrementally-verifiable-computation-nova&#x2F;&quot;&gt;folding schemes&lt;&#x2F;a&gt;. Elliptic curve pairings also provide some way to compute multiplications between polynomials in committed form (using KZG).&lt;&#x2F;p&gt;
&lt;p&gt;To be able to construct an FHE scheme we need not only preserve operations but also have a way to decipher the result.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;fhe-fundamentals&quot;&gt;FHE fundamentals&lt;&#x2F;h2&gt;
&lt;p&gt;There are many libraries for FHE nowadays, such as &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.openfhe.org&#x2F;&quot;&gt;OpenFHE&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;project&#x2F;microsoft-seal&#x2F;&quot;&gt;Microsoft SEAL&lt;&#x2F;a&gt;, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;cpeikert&#x2F;Lol&quot;&gt;Λ∘λ&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;homomorphicencryption.org&#x2F;introduction&#x2F;&quot;&gt;many more&lt;&#x2F;a&gt;. With FHE you can make private queries to search engines or pages, such as Wikipedia; see &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;spiralwiki.com&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;FHE schemes are based on lattice cryptography. A lattice is given by linear combinations with integer coefficients of some base vectors. To fix ideas, imagine we have two vectors &lt;em&gt;e x&lt;&#x2F;em&gt; = (1,0) and &lt;em&gt;e y&lt;&#x2F;em&gt; = (0,1) and we consider all possible combinations &lt;em&gt;p  = xex + yey&lt;&#x2F;em&gt; with &lt;em&gt;x&lt;&#x2F;em&gt; ,&lt;em&gt;y&lt;&#x2F;em&gt; in ℤ, yielding points in space (0,0),(1,0),(1,1),(−1,−2),…. A lattice looks like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lattice_problem#&#x2F;media&#x2F;File:SVP.svg&quot;&gt; this&lt;&#x2F;a&gt;. Ideal lattices correspond to &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ideal_(ring_theory)&quot;&gt;ideals in polynomial rings&lt;&#x2F;a&gt;, inheriting the natural addition and multiplication operations of the ring (Ideals generalize the idea behind certain subsets of the integers, such as even numbers. The addition of any two even numbers is always even and, whenever we multiply any integer by an even, the result is also even -an absorption property-).&lt;&#x2F;p&gt;
&lt;p&gt;To build an FHE scheme we could picture having a ciphertext with some small attached to it, such that the decryption works as long as the noise is below a certain threshold. If we have ways to homomorphically multiply and add ciphertexts, but at the expense of increasing the noise parameters accordingly, that is, _E(a  + b) =E (a) &lt;em&gt;⊕ _  E (b)&lt;&#x2F;em&gt; and &lt;em&gt;E(a×b)=E(a)⋅E(b)&lt;&#x2F;em&gt;. We call this a somewhat homomorphic encryption scheme (SHE). If we could add a “recrypt” algorithm, which can take a given ciphertext &lt;em&gt;E(m)&lt;&#x2F;em&gt; and reduce its noise, obtaining a new ciphertext &lt;em&gt;E′(m)&lt;&#x2F;em&gt; that also encrypts &lt;em&gt;m&lt;&#x2F;em&gt; , then we can obtain an FHE scheme.&lt;&#x2F;p&gt;
&lt;p&gt;The SHE scheme can handle circuits of a certain depth (imagine this as the number of times you can multiply or add before the noise becomes too large). The SHE can be modified to have its decryption circuit have a lower multiplicative depth, making it “bootstrappable” and thus transforming it into an FHE scheme.&lt;&#x2F;p&gt;
&lt;p&gt;Some common schemes for FHE are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Brakerski-Fan-Vercauteren (BFV) and Brakerski-Gentry-Vaikuntanathan (BGV) for integer arithmetic.&lt;&#x2F;li&gt;
&lt;li&gt;Cheon-Kim-Kim-Song (CKKS) for real number arithmetic.&lt;&#x2F;li&gt;
&lt;li&gt;Ducas-Micciancio (DM) and Chillotti-Gama-Georgieva-Izabachene (CGGI) for boolean circuits and arbitrary functions using lookup tables.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Many cryptographic primitives, such as public key cryptography, are based on the hardness or intractability of mathematical problems, such as &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Integer_factorization&quot;&gt;integer factorization&lt;&#x2F;a&gt; (RSA) or the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Discrete_logarithm&quot;&gt;discrete logarithm problem&lt;&#x2F;a&gt; (elliptic curve cryptography). These problems cannot be solved efficiently with current computers (at least, provided that the integers involved are big enough or the groups have a large number of elements). However, quantum computers could easily handle these problems if certain conditions are met, via &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shor%27s_algorithm&quot;&gt;Shor’s algorithm&lt;&#x2F;a&gt;. FHE is based on the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lattice_problem&quot;&gt;shortest vector problem&lt;&#x2F;a&gt; or &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;homomorphicencryption.org&#x2F;wp-content&#x2F;uploads&#x2F;2018&#x2F;11&#x2F;HomomorphicEncryptionStandardv1.1.pdf&quot;&gt;ring learning-with-error&lt;&#x2F;a&gt; (RLWE) problem, which is an NP-hard problem that cannot be tackled with Shor’s algorithm (FHE is considered to be quantum resistant).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;zero-knowledge-proofs&quot;&gt;Zero-knowledge proofs&lt;&#x2F;h2&gt;
&lt;p&gt;Zero-knowledge proofs (ZKP) have been gaining a lot of attention during the last decade, especially after the first efficient SNARK constructions. ZKP play an important role in the solution of two of the main challenges in decentralized ledgers: scalability and privacy. To validate transactions, nodes have to re-execute them, leading to bottlenecks. Besides, all the information in the ledger is public, which can leak sensitive information about individuals and organizations.&lt;&#x2F;p&gt;
&lt;p&gt;zk-SNARKs allow one party to prove a statement, without revealing anything other than the validity of the statement. For example, we can prove that we have a given secret key, without revealing it. We can also prove that we have executed some transaction or computation, without exposing secret or sensitive information. An important property of SNARKs is their succinctness, which means that proofs:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Are short (occupy little memory, about 1 kB for some SNARKs).&lt;&#x2F;li&gt;
&lt;li&gt;Are fast to verify (typically, in the order of milliseconds).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Ethereum has been adding zero-knowledge proof technologies recently to solve scalability issues. Zcash implemented ZKP to provide private transactions, while Aleo uses them to enable running private computations in a decentralized way.&lt;&#x2F;p&gt;
&lt;p&gt;How do SNARKs work under the hood? Even if there are many different constructions (such as Marlin, Plonk, Halo, and STARKs), they have a common recipe. The building blocks of SNARKs are polynomial interactive oracle proofs (PIOP) and polynomial commitment schemes (PCS). Depending on the choices made, the resulting SNARK has different properties and requirements. For example, it may be transparent (does not need a trusted setup), post-quantum secure, need special (pairing-friendly) elliptic curves, take longer times to generate proofs, have shorter proofs (less than 1 kB), allow for easy recursion, etc. A comparison between different polynomial commitment schemes is shown &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;hackernoon.com&#x2F;kzg10-ipa-fri-and-darks-analysis-of-polynomial-commitment-schemes&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To be able to construct the proof, we first need to transform our computation into some SNARK-friendly format. We can prove the correctness of our execution by reducing it to some kind of NP-complete problem, such as graph coloring or circuit satisfiability. We will work with arithmetic circuits and the transformation of a program into a circuit is known as arithmetization. There are different forms or strategies for doing this transformation; an overview of the most commonly used is &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.entropy1729.com&#x2F;arithmetization-schemes-for-zk-snarks&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;multiparty-computation&quot;&gt;Multiparty computation&lt;&#x2F;h2&gt;
&lt;p&gt;In a secure multiparty computation, a group of participants, &lt;em&gt;p 1,p2,…,pm&lt;&#x2F;em&gt;, each having some secret information &lt;em&gt;s 1,s2,…sm&lt;&#x2F;em&gt;, want to compute a certain function that requires the knowledge of that secret information. For example, we could have m employees wanting to know their average salary without revealing their income. One easy way to do so would be that all of them trust another party and each sends their secret information and the “trusted” party computes the average. The drawback: the “trusted” party learns all the information and could leverage it. Or perhaps he is honest, but he gets hacked and an attacker obtains everything.&lt;&#x2F;p&gt;
&lt;p&gt;Luckily, there is a useful cryptographic primitive to deal with cases like these: &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.geeksforgeeks.org&#x2F;additive-secret-sharing-and-share-proactivization-using-python&#x2F;&quot;&gt; additive secret sharing&lt;&#x2F;a&gt;. Each of the participants can break their secret &lt;em&gt;s k&lt;&#x2F;em&gt; into &lt;em&gt;m&lt;&#x2F;em&gt; shares in such a way that no shareholder can, on his own, learn the secret. To be able to reconstruct the secret, all of the other parties have to collude and share their part. In the example above, each employee can break his salary, &lt;em&gt;s k&lt;&#x2F;em&gt; into &lt;em&gt;m&lt;&#x2F;em&gt; different, random shares. For example, if we have 4 employees and employee A earns 4500, he can have 4 shares, &lt;em&gt;s Ai&lt;&#x2F;em&gt;: -1200, 1500, 3600, 600, such that ∑i sAi_ = 4500. He keeps one share and distributes the rest to B, C, and D. In turn, the rest break their secret and divide it. Afterward, each participant sums all the shares he has, obtaining a partial sum, &lt;em&gt;s p,A&lt;&#x2F;em&gt; = ∑k sk,A, and then shares these partial sums to compute the final average.&lt;&#x2F;p&gt;
&lt;p&gt;Secret sharing is secure whenever parties knowing at most &lt;em&gt;m&lt;&#x2F;em&gt;  − 1 have no more information than anyone with no shares at all.&lt;&#x2F;p&gt;
&lt;p&gt;Now, how can we ensure that each party does what it is supposed to do? ZKP give us a way to guarantee that each participant does the computation as expected, by submitting proof that attests to the correct execution. If he cheats, the proof should fail and he could be penalized. Early MPC protocols had significant overhead; the last decade has seen many advances, making it efficient and leading to many applications.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;&#x2F;h2&gt;
&lt;p&gt;Fully homomorphic encryption, zero-knowledge proofs, and multiparty computations are important cryptographic primitives that have been gaining more and more attention in recent years, with the introduction of decentralized ledgers and increasing concern over data privacy. Each has its unique features and applications and has something to offer to the other primitives. FHE allows us to make cloud computations on encrypted data without needing to hand our key to the server, which prevents third parties from gaining access to the specific contents of the data. ZKP allow us to prove the correctness of a given computation by submitting a short proof, which can be quickly verified. This is seen as one of the greatest tools to solve the privacy and scalability issues of decentralized ledgers. Multiparty computation helps us distribute a complex computation or calculate something when all the inputs are distributed among several parties in a secure way; it has applications for voting, private auctions, bidding, etc. FHE can help us improve existing ZKP, which in turn can make multiparty computation much simpler and more secure. In turn, MPC is needed for the setup ceremonies of zk-SNARKs and can also help provers reduce their proof generation time by delegating them to untrusted powerful servers. ZKP can also help us ensure that the computations involving encrypted data are carried out correctly. All these fields have seen great advances over the last decade and each will help the others advance, leading to new interesting applications, with a greater focus on decentralization and privacy. In upcoming posts, we will cover in more depth the mathematical foundations of FHE and further applications of MPC and ZKP.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
