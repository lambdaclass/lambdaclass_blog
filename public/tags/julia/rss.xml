<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
      <title>LambdaClass Blog - Julia</title>
      <link>https://blog.lambdaclass.com</link>
      <description>Deep technical insights on cryptography, distributed systems, zero-knowledge proofs, and cutting-edge software engineering from the LambdaClass team.</description>
      <generator>Zola</generator>
      <language>en</language>
      <atom:link href="https://blog.lambdaclass.com/tags/julia/rss.xml" rel="self" type="application/rss+xml"/>
      <lastBuildDate>Thu, 02 Sep 2021 00:00:00 +0000</lastBuildDate>
      <item>
          <title>Simulations are about to get way, way faster with JuliaSim</title>
          <pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://blog.lambdaclass.com/posts/simulations-are-about-to-get-way-way-faster-with-juliasim/</link>
          <guid>https://blog.lambdaclass.com/posts/simulations-are-about-to-get-way-way-faster-with-juliasim/</guid>
          <description xml:base="https://blog.lambdaclass.com/posts/simulations-are-about-to-get-way-way-faster-with-juliasim/">&lt;p&gt;Today we’re excited to bring you a first glance at the result of a major multi-year project by the Julia Computing team.&lt;&#x2F;p&gt;
&lt;p&gt;JuliaSim is a cloud-based simulation platform built on top of the Julia open source stack, including SciML and ModelingToolkit, which we explored in depth &lt;a href=&quot;&#x2F;scientific-machine-learning-with-julia-the-sciml-ecosystem&#x2F;&quot;&gt;here&lt;&#x2F;a&gt; and &lt;a href=&quot;&#x2F;modeling-complexity-with-symbolics-jl-and-modelingtoolkit-jl&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;, respectively. These were just the base of JuliaSim, which aims to change the way the industry does modeling and simulation with powerful acceleration and integration within a complete ecosystem.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;juliacomputing.com&#x2F;products&#x2F;juliasim&#x2F;&quot;&gt;JuliaSim&lt;&#x2F;a&gt;’s first beta will be released in a few months. We interviewed Chris Rackauckas to learn more about the project.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;what-is-juliasim-how-does-it-compare-to-alternatives-like-modelica-dymola&quot;&gt;What is JuliaSim? How does it compare to alternatives like Modelica&#x2F;Dymola?&lt;&#x2F;h4&gt;
&lt;p&gt;JuliaSim is a cloud-based platform for accelerated modeling and simulation. Unlike tools like Dymola, it integrates with a large open source community, the Julia programming language and the SciML ecosystem, to enhance its environment with offerings like easy parallelism, automated generation of ML surrogate models, and much more. In &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=lNbU5jNp67s&quot;&gt;a recent talk at JuliaCon 2021&lt;&#x2F;a&gt; I highlight this community integration as one of its core aspects that gives JuliaSim a competitive advantage in terms of features and performance, since it allows us to contribute to and benefit from the work of many scientists and engineers from across the world.&lt;&#x2F;p&gt;
&lt;p&gt;While JuliaSim has acausal modeling as one of its core features, unlike Modelica-based tools that is just one of its domains. Accelerated simulation of PDEs with neural networks, integrating stochastic simulation into workflows, specific simulation environments for pharmacology and circuit modeling, and much more are all part of the JuliaSim product. We see the future as a place where composability will be necessary to achieve the next level of simulations.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;can-it-integrate-with-the-fmi-will-we-be-able-to-use-models-written-in-e-g-modelica-inside-juliasim&quot;&gt;Can it integrate with the FMI? Will we be able to use models written in, e.g. Modelica, inside JuliaSim?&lt;&#x2F;h4&gt;
&lt;p&gt;Yes, JuliaSim at its release will offer features for integrating with the FMI standard, allowing for FMI imports and FMI exports. This will allow for example the ability to build surrogates of models from Modelica or Simulink platforms, and allow for generating binaries that can integrate back into these platforms.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;a-key-aspect-of-modelica-is-its-standard-library-which-includes-many-of-the-most-common-components-for-modeling-will-there-be-a-similar-thing-in-juliasim&quot;&gt;A key aspect of Modelica is its standard library, which includes many of the most common components for modeling. Will there be a similar thing in JuliaSim?&lt;&#x2F;h4&gt;
&lt;p&gt;With JuliaSim we are building a standard library which includes similar domains to the Modelica Standard Library, but also includes many other domains related to the customers we have been working with. For example, with JuliaSim one will be able to easily search a database of hundreds of physiological and systems biological models for accelerating workflows in biomedical simulation and drug development. We plan to continually improve this model library and mold it to the needs of our customers.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-surrogate-models-what-makes-them-important-especially-beyond-academic-applications&quot;&gt;What are surrogate models? What makes them important, especially beyond academic applications?&lt;&#x2F;h4&gt;
&lt;p&gt;Surrogate models are an amortization of compute cost to improve simulation workflows. It gives you a way to say “do 100 simulations now, and all of my future simulations are 100x faster”. When you mix this with cloud resources you can get a major workflow update: pay for a bit of cloud compute to spawn a bunch of simulations in parallel, but now every time you click the simulate button you do not have to wait 30 minutes to check the result.&lt;&#x2F;p&gt;
&lt;p&gt;When I was in graduate school I noticed that there is a very nonlinear effect of code speed on productivity which I captured in a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.stochasticlifestyle.com&#x2F;the-nonlinear-effect-of-code-speed-on-productivity&quot;&gt;blog post&lt;&#x2F;a&gt;. If you have to wait 30 minutes for a result, that means that instead you’ll start it before lunch or before a 1 hour meeting, so the true “time to see simulation results” gets amplified further. 2 hour simulations are something you will start in the morning and check at night. Thus a surrogate changing a long simulation to something that you can analyze in real-time is invaluable to decreasing labor costs because of how simulation time effects the day-to-day life of an engineer.&lt;&#x2F;p&gt;
&lt;p&gt;While in an academic sense we focus on issues like “training the surrogate costs 100 simulations, but we needed 10,000 simulations for to optimize the building design and thus we saved in the end”, I do not think this calculus is really the major change that surrogates will bring. Even if it takes 100 simulations to train a surrogate that you use 10 times, if you integrate this will cloud compute to do those all in parallel, then to the user you only have to sit through what is effectively 2 simulations. If that’s a two hour simulation time, you’ve now changed a workflow that takes a full day to something that you set to train in the morning and then after lunch you can interactively fiddle with parameters until your controls are correct. And then any time you revisit in the future, it’s ready to be fiddled with again. When you view surrogates in this light, I think you can see why we believe this will be a gamechanger for the everyday engineer.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;one-of-the-main-innovations-brought-by-juliasim-is-the-generation-of-fast-and-accurate-surrogate-models-that-can-speedup-simulations-as-much-as-500x-even-in-the-presence-of-stiff-models-how-is-this-achieved-what-s-a-ctesn&quot;&gt;One of the main innovations brought by JuliaSim is the generation of fast and accurate surrogate models that can speedup simulations as much as 500x, even in the presence of stiff models. How is this achieved? What’s a CTESN?&lt;&#x2F;h4&gt;
&lt;p&gt;A continuous-time echo state network (CTESN) is an implicitly trained machine learning framework for capturing the dynamics of stiff models. These are models with phase transitions, fast transient behavior, and are well-known to be numerically difficult. Across many domains we have shown the CTESN training procedure to be robust due to how it incorporates implicit features of stable differential equation solvers. All that we need to generate it is a chunk of simulations done beforehand, and what results is this really fast object that will predict the simulation behavior at new parameters.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-easy-is-it-to-compose-surrogate-models-with-normal-ones-in-juliasim-what-are-the-advantages-of-this-approach&quot;&gt;How easy is it to compose surrogate models with normal ones in JuliaSim? What are the advantages of this approach?&lt;&#x2F;h4&gt;
&lt;p&gt;The CTESN represents the surrogate of a differential equation model as a differential equation model. Because of this representation, it’s not different from the other physical components. As long as you train it to cover the right states and observables, it will slot right into where you had the larger model before.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;paramount-to-surrogatization-is-the-ability-to-check-the-accuracy-of-the-surrogate-model-and-its-performance-against-the-original-one-how-does-juliasim-tackle-this-are-these-metrics-appropriately-exposed-to-the-user&quot;&gt;Paramount to surrogatization is the ability to check the accuracy of the surrogate model and its performance against the original one. How does JuliaSim tackle this? Are these metrics appropriately exposed to the user?&lt;&#x2F;h4&gt;
&lt;p&gt;JuliaSim generates diagnostics of the surrogate training process to signal to the user important features like the projected maximum error over the timeseries over the user-defined parameter space. A lot of this is done through quasi-random sampling right now, though we are investigating more complex techniques to more quickly achieve good estimates.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;does-juliasim-run-on-top-of-juliahub-or-are-they-separate-things-is-the-pricing-model-expected-to-be-the-same&quot;&gt;Does JuliaSim run on top of &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;juliahub.com&#x2F;lp&#x2F;&quot;&gt;JuliaHub&lt;&#x2F;a&gt; or are they separate things? Is the pricing model expected to be the same?&lt;&#x2F;h4&gt;
&lt;p&gt;JuliaSim is JuliaHub-based platform. JuliaSim users receive a subscription to a set of proprietary packages, such as the standard library and the surrogatization tools, which grants access to their use on JuliaHub. The pricing model of JuliaSim is simply the subscription and pay-for-compute, so users only pay for what they use but have access to the full suite.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;will-there-be-a-gui-for-modeling-or-will-we-mostly-have-to-write-all-the-julia-code-explicitly&quot;&gt;Will there be a GUI for modeling or will we mostly have to write all the Julia code explicitly?&lt;&#x2F;h4&gt;
&lt;p&gt;There will be many GUIs! The first GUI that we are building is more pharmaceutical modeling focused given our early customer base and connections with Pumas-AI. This allows for quickly building chemical reaction network and systems pharmacology models, and representing models in a visual form for presentations and reporting. It also will serve as the basis for visual programming of compartmental models in pharmacokinetics. We also will have a GUI at launch which simplifies the FMU surrogatization process, allowing non-Julia users to quickly accelerate their FMUs by entering in parameter information and clicking “surrogatize”. We have other GUIs planned for the near future as well, such as GUIs for block diagrams and acausal modeling, along with 3D visualization tools.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;juliasim-is-launching-pretty-soon-what-can-we-expect-from-this-first-version-what-features-are-next&quot;&gt;JuliaSim is launching pretty soon; what can we expect from this first version? What features are next?&lt;&#x2F;h4&gt;
&lt;p&gt;JuliaSim’s first beta will be launching fairly soon. I wouldn’t call it the first version quite yet, though the beta will already be usable as we have been working with a group of customers to showcase the performance advantage for their specific applications. The first release will be a mix of GUIs, cloud parallel surrogatization, and standard libraries. And for the future we are aiming for a lot more. The full vision of JuliaSim is expressed in &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=lNbU5jNp67s&quot;&gt;the JuliaCon 2021 video&lt;&#x2F;a&gt; so refer to that for more details.&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>We wrote a book! “Data Science in Julia for Hackers” beta is now live and free to read</title>
          <pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://blog.lambdaclass.com/posts/how-we-wrote-a-hands-on-bayesian-data-science-book-in-6-months/</link>
          <guid>https://blog.lambdaclass.com/posts/how-we-wrote-a-hands-on-bayesian-data-science-book-in-6-months/</guid>
          <description xml:base="https://blog.lambdaclass.com/posts/how-we-wrote-a-hands-on-bayesian-data-science-book-in-6-months/">&lt;h4 id=&quot;learn-about-data-science-and-julia-while-solving-real-life-problems&quot;&gt;Learn about data science and Julia while solving real-life problems&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;strong&gt;Read our book “Data Science in Julia for Hackers” at:&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;datasciencejuliahackers.com&#x2F;&quot;&gt;&lt;strong&gt;https:&#x2F;&#x2F;datasciencejuliahackers.com&#x2F;&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-t93INVDo-XNSm7iObeXODQ.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;We put together this post to share the release of our first book on data science methods, which focuses on solving real-life problems. This release is actually a beta version, as we are looking to receive constructive criticism and feedback, in order to improve the book until the first revised version is ready.&lt;&#x2F;p&gt;
&lt;p&gt;We come from different backgrounds. Federico Carrone is a developer with over 15 years of experience and founder of a startup that is running since 2014, who is currently studying towards a degree in Mathematics. Herman Obst is an Industrial Engineer who is just learning to program as is our Physicist, Mariano Nicolini. Martina Cantaro coordinated the writing process and our technical consultants, Manuel Puebla and Lucas Fernández Piana, are making sure our definitions are simple to understand yet accurate.&lt;&#x2F;p&gt;
&lt;p&gt;We do not come from academia, nor are we experts in statistics, which may make some people wonder whether we have enough authority to concern ourselves with such complex issues. But we have one thing in common: above all, we are doers. Collectively, we have experience in setting ourselves big goals and achieving them through, learning, hacking, tinkering and thinking. Data science is a toolkit that has enabled us to solve different real-life problems and we want to share what we have learned in the process.&lt;&#x2F;p&gt;
&lt;p&gt;Speaking of real-life problems, few disciplines have as much impact when it comes to solving them as data science. Currently, our society is constantly generating massive amounts of information encoding complex behaviors and relationships from a wide array of fields. And people are developing the tools to use that information to our benefit.&lt;&#x2F;p&gt;
&lt;p&gt;That was why we were so struck by the fact that almost all the books on data science and statistics had a very theoretical approach, focusing on understanding the mathematics of the algorithms and never talking about their applications to situations we might encounter in work or life in general.&lt;&#x2F;p&gt;
&lt;p&gt;Because of this (and because of our love for making and generating things) we decided to embark on the adventure of writing a book. A book whose first and foremost premise was to propose diverse and interesting problems, and solve them using the ingenuity and tools of data science. And theory does not play a minor role, not at all, but it is developed only to the extent that the resolution of the problem requires it. In this way, a real connection between theory and reality is achieved.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;what-we-want-you-to-learn&quot;&gt;What we want you to learn&lt;&#x2F;h3&gt;
&lt;p&gt;One of the ideas we are most interested in conveying is that of taking action. Nowadays there is a dominant way of thinking, widespread in the academic world, which maintains that before being able to carry any meaningful work we must first acquire a broad theoretical knowledge of the subject. We think this can be counterproductive, as it makes people afraid of taking risks and daring to immerse themselves in practice.&lt;&#x2F;p&gt;
&lt;p&gt;We decided to disprove this way of thinking by making a book about Bayesian statistics, machine learning and artificial intelligence, without (at first) knowing much about any of these fields.&lt;&#x2F;p&gt;
&lt;p&gt;That was our goal. We figured out the rest as we went along.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;what-we-learned&quot;&gt;What we learned&lt;&#x2F;h3&gt;
&lt;p&gt;With our goal always in mind, we started searching and solving a wide variety of problems that could be interesting to tackle using Bayesian inference, which at the beginning was the only topic we planned to talk about. That way we got used to the Bayesian mindset and the different probabilistic programming tools in Julia, our language of choice.&lt;&#x2F;p&gt;
&lt;p&gt;But as we profressed, we changed the focus of the book from something like “Bayesian methods from a practical perspective” to include data science topics in general, from time series prediction to the powerful Scientific Machine Learning ecosystem. Finally, although we wanted to focus on these -not so widespread- methods, we thought it pertinent to add a section on more classical methods, such as deep learning and machine learning — the latter currently in production.&lt;&#x2F;p&gt;
&lt;p&gt;As the dificulty of the scenarios we created began to increase (we often found that a problem was way more complicated than we initially thought), it became clear that we had to incorporate a little more solid knowledge about Bayesianism. That’s where reading &lt;em&gt;Bayesian Methods for Hacker&lt;&#x2F;em&gt; s and &lt;em&gt;Statistical Rethinking&lt;&#x2F;em&gt; gave us a much better understanding and tools to deal with the complexity.&lt;&#x2F;p&gt;
&lt;p&gt;Our writing process also got better over time. Although we already had some experience writing blog posts, writing a book was a brand new challenge for everyone, especially since we are not native English speakers.&lt;&#x2F;p&gt;
&lt;p&gt;At the beginning, our explanations of models and the theory behind them were somewhat lacking. Several pages were discarded, and the rest were re-written several times until we found them passable. It was a process that involved some deal of frustration, but as we iterated over them, the quality of the pages increased substantially. Finally, we found a comfort zone in terms of diagraming, coding and writing the chapters.&lt;&#x2F;p&gt;
&lt;p&gt;The road was winding and some keys to keep the progress up were to not let ourselves be overwhelmed by the enormous task, to divide the work, to go chapter by chapter and, above all, to always keep moving forward. One strategy we used was to complete the writing of the chapters until we felt they were 80% complete. That way, the progress curve always kept a good positive slope, since (by Pareto law) that last 20% would take 80% of the time to complete. Only when we had completed 80% of the book, we went back chapter by chapter to finish polishing it.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;where-you-can-read-it&quot;&gt;Where you can read it&lt;&#x2F;h3&gt;
&lt;p&gt;And that’s it! As of now, the book is up for everyone to read at:&lt;&#x2F;p&gt;
&lt;h4 id=&quot;https-datasciencejuliahackers-com&quot;&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;datasciencejuliahackers.com&#x2F;&quot;&gt;https:&#x2F;&#x2F;datasciencejuliahackers.com&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;We hope it will be useful to as many people as possible and we hope to receive lots of feedback and constructive criticism, so we can keep improving edition after edition.&lt;&#x2F;p&gt;
&lt;p&gt;Enjoy!&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>Modeling complexity with Symbolics.jl and ModelingToolkit.jl</title>
          <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://blog.lambdaclass.com/posts/modeling-complexity-with-symbolics-jl-and-modelingtoolkit-jl/</link>
          <guid>https://blog.lambdaclass.com/posts/modeling-complexity-with-symbolics-jl-and-modelingtoolkit-jl/</guid>
          <description xml:base="https://blog.lambdaclass.com/posts/modeling-complexity-with-symbolics-jl-and-modelingtoolkit-jl/">&lt;h4 id=&quot;an-interview-with-chris-rackauckas&quot;&gt;An interview with Chris Rackauckas&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-sHzrVkhNvHxdiJ2IBmfVPA.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;As we often mentioned on Not a Monad Tutorial, the world is complex, and we increasingly understand where our tools fall short when trying to model this complexity.&lt;&#x2F;p&gt;
&lt;p&gt;We’ve previously interviewed Chris Rackauckas on &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;notamonadtutorial.com&#x2F;scientific-machine-learning-with-julia-the-sciml-ecosystem-b22802951c8a&quot;&gt;SciML&lt;&#x2F;a&gt;; this time he joins us to answer questions regarding new developments in the area of symbolic computation with Julia, its relation to numerical computing, causal vs acausal approaches, how these matters are represented in Symbolics.jl and ModelingToolkit.jl, and how these packages relate to the existing simulation tooling landscape.&lt;&#x2F;p&gt;
&lt;p&gt;These packages compose easily and thus allow modelling larger, more complex systems by reusing parts, as well as helping community efforts. Having these interoperable packages is key to building a modern simulation software stack which can address the aforementioned needs of complex modelling.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;what-is-symbolics-jl-what-are-the-motivations-behind-the-creation-of-the-system&quot;&gt;What is Symbolics.jl? What are the motivations behind the creation of the system?&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;JuliaSymbolics&#x2F;Symbolics.jl&quot;&gt;Symbolics.jl&lt;&#x2F;a&gt; is a Computer Algebra System (CAS) in the Julia programming language developed by the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;juliasymbolics.org&#x2F;&quot;&gt;JuliaSymbolics Organization&lt;&#x2F;a&gt;. Think symbolic computation: write down equations and ask the computer to come up with symbolic solutions. It’s a modern CAS, meaning it’s built on a widely used modern programming language (Julia), making use of modern tooling like pervasive parallelism, new algorithms like E-Graphs, integration with machine learning, and more.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-modelingtoolkit-jl-what-are-the-needs-it-addresses&quot;&gt;What is ModelingToolkit.jl? What are the needs it addresses?&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;SciML&#x2F;ModelingToolkit.jl&quot;&gt;ModelingToolkit.jl&lt;&#x2F;a&gt; is an equation-based acausal modeling system. It’s similar to systems like Modelica which allow for composing models to quickly generate realistic simulations. This lets you take pre-built models created by other scientists and build complete systems. For example, you can take a high fidelity model of an air conditioning, then make a model of a building, and stick the air conditioning into the building and ask what kind of energy efficiency you get. Then change the building to start designing what’s most efficient.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-does-acausal-modeling-relate-to-tools-like-simulink&quot;&gt;How does acausal modeling relate to tools like Simulink?&lt;&#x2F;h4&gt;
&lt;p&gt;Simulink is a causal modeling tool. You have to know “what causes what” in order to develop the simulation. This can be difficult in our complex world: the heat of the building is read by the thermostat which turns the AC on which then changes the heat of the building. Feedbacks and “algebraic loops” cause issues in causal modeling systems: users have to break the loops or change the model. For this reason &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1909.00484&quot;&gt;experts consider causal modeling as not suitable for complex simulations&lt;&#x2F;a&gt; as they do not compose well. This is the reason acausal tools like Modelica have seen a lot of adoption. Even MATLAB has an acausal tool now, SimScape. Given the advancements in these techniques, I see the next generation of engineers all using acausal tools, with ModelingToolkit.jl being one of the only fully-featured free and open-source acausal systems.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;why-is-symbolic-computation-needed-at-all-what-are-the-advantages-compared-to-numerical-computation-when-is-one-preferred-to-the-other&quot;&gt;Why is symbolic computation needed at all? What are the advantages compared to numerical computation? When is one preferred to the other?&lt;&#x2F;h4&gt;
&lt;p&gt;Are you sure you know enough mathematics to have written the mathematical model in the most numerically-stable form? Even if you know all of the tricks that you’re supposed to do, do you want to do it all by hand? I see the main use of symbolic computation in symbolic-numerics, i.e. using symbolic techniques to improve the models which are then used in numerical methods. For example, in a recent blog post titled &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.stochasticlifestyle.com&#x2F;generalizing-automatic-differentiation-to-automatic-sparsity-uncertainty-stability-and-parallelism&#x2F;&quot;&gt;Generalizing Automatic Differentiation to Automatic Sparsity, Uncertainty, Stability, and Parallelism&lt;&#x2F;a&gt;, I describe how a two-dimensional pendulum simulation without the small angle approximation requires a differential-algebraic equation. The intuitive model of “position moves by velocity, velocity moves by acceleration, and length is constant” is actually an unstable description of the full pendulum. You have to differentiate the “length is constant” equation twice, then substitute other relationships, and then you arrive at an “index-1” DAE which is easier to numerically solve. Even if you know enough of these details to do it, you don’t want to handle that! Symbolic-numeric computation is how we will get to a future where that is all automated.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;wolfram-mathematica-and-sympy-in-python-are-some-of-the-most-popular-choices-when-dealing-with-symbolic-manipulation-nowadays-what-are-the-advantages-symbolics-jl-offers-in-comparison-to-them&quot;&gt;Wolfram Mathematica and SymPy in Python are some of the most popular choices when dealing with symbolic manipulation nowadays. What are the advantages Symbolics.jl offers in comparison to them?&lt;&#x2F;h4&gt;
&lt;p&gt;Symbolics.jl is being built from the ground up for speed, being built from the ground up with parallelism, and last but not least, it’s being built up from a community of tools. There is so much good stuff out there that I think it would be unreasonable to silo one’s organization off and do everything from scratch. Julia has many great initiatives like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.computeralgebra.de&#x2F;sfb&#x2F;&quot;&gt;OSCAR.jl&lt;&#x2F;a&gt; which are building fast implementations of the mathematical guts. We are using the fact that Julia is a high performance language to both develop high level interfaces and ensure that all of these tools can be used with minimal overhead, mental and computational. So while you might know nothing about Galois fields, there might be a fancy algorithm underneath the hood when you call factorize(x² + 2x + 1) that does it efficiently and scales to large systems.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-is-modelingtoolkit-jl-related-to-symbolics-jl-what-role-does-julia-s-composability-play-in-the-relationship-between-the-two-packages&quot;&gt;How is ModelingToolkit.jl related to Symbolics.jl? What role does Julia’s composability play in the relationship between the two packages?&lt;&#x2F;h4&gt;
&lt;p&gt;Acausal modeling requires symbolic transformations of equations. In that pendulum example, “differentiate the equation twice and substitute”, what kind of tool provides features like differentiation and high performance equation rewriting (i.e. substitution)? A CAS! So ModelingToolkit.jl let’s someone say “this is an ODE”, where it’s equations are described by Symbolics.jl expressions. There are then functions that do things like “transform this to index-1 form” and “analytically discover which equations are redundant and delete them”, and those transformations are written using the tools of Symbolics.jl. This means that as the CAS grows more powerful, so will ModelingToolkit.jl and its environment.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-does-modelingtoolkit-jl-compare-to-other-modeling-frameworks-such-as-modelica-and-simulink-how-easily-would-a-person-with-some-background-on-these-frameworks-adapt-to-modelingtoolkit-jl&quot;&gt;How does ModelingToolkit.jl compare to other modeling frameworks such as Modelica and Simulink? How easily would a person with some background on these frameworks adapt to ModelingToolkit.jl?&lt;&#x2F;h4&gt;
&lt;p&gt;ModelingToolkit.jl’s focus at this point has mainly been on flexibility and speed. In terms of flexibility, ModelingToolkit.jl is the only one which has a hackable compiler that allows composing transformations. All of the symbolic enhancements that are allowed in the Modelica and Simulink compilers are those that are built-in. While it sounds like that’s all that most users need, what that really does is stifle innovation. There are people working in these fields that need a common framework to build off of. Many of these researchers are now in Julia. So for example, could we add an analysis pass that automatically tells you whether you can distinguish between parameters with the data you have? Yes, anyone could extend the ModelingToolkit.jl system with a pass that does that, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;alexeyovchinnikov&#x2F;SIAN-Julia&quot;&gt;and we’re already talking with authors of Julia libraries about doing this&lt;&#x2F;a&gt;. There is so much going on in this space that it’s hard to express, but expect tons of unique transformations to be allowed on your models. “Make a model that doesn’t solve in other systems solve here” is not just a dream.&lt;&#x2F;p&gt;
&lt;p&gt;And then there’s speed. We haven’t done complete and comprehensive benchmarking against all of the systems yet, but we have seen &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2103.05244&quot;&gt;some good performance against some Modelica compilers&lt;&#x2F;a&gt;, indicating we’re doing really well. One NASA user of ModelingToolkit.jl said &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;tQpqsmwlfY0&quot;&gt;a 15 minute Simulink simulation took 50ms in ModelingToolkit.jl&lt;&#x2F;a&gt;. A user mentioned at the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;FMVOUvWNlLE&quot;&gt;AAS&#x2F;AIAA Space Flight Mechanics meeting that every case against a Fortran package with a MATLAB interface, they saw at least an order of magnitude acceleration by moving to ModelingToolkit.jl&lt;&#x2F;a&gt;. In a very early version of ModelingToolkit.jl, we did a demo with Pfizer where we &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;juliacomputing.com&#x2F;case-studies&#x2F;pfizer&#x2F;&quot;&gt;demonstrated a 175x acceleration over their original C-based simulations&lt;&#x2F;a&gt;. Part of all of this is just due to the solvers, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;benchmarks.sciml.ai&#x2F;html&#x2F;MultiLanguage&#x2F;wrapper_packages.html&quot;&gt;which benchmark really well in a cross-language way&lt;&#x2F;a&gt;. Another good chunk is due to the feature sets of the solvers, and ModelingToolkit.jl automatically enabling some of the best choices of combinations. This is explored a bit in a talk at JuliaCon 2020 titled &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=UNkXNZZ3hSw&quot;&gt;Auto-Optimization and Parallelism in DifferentialEquations.jl&lt;&#x2F;a&gt;, which was the video that announced the release of ModelingToolkit.jl as a new front end to the solvers for further improving speed.&lt;&#x2F;p&gt;
&lt;p&gt;That said, we have focused so far on the details. We want the biggest hardest models with the users who have the most demands. These other tools have put a lot more time into user interface, specifically graphical user interfaces (GUIs). Modelica and Simulink has a lot of tooling for drag-and-drop model building. Also, they have libraries of premade libraries. But, this will change very soon. Keep your eyes peeled for some announcements.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-the-main-challenges-to-solve-in-order-to-build-such-a-high-level-modeling-library&quot;&gt;What are the main challenges to solve in order to build such a high level modeling library?&lt;&#x2F;h4&gt;
&lt;p&gt;You want to make the modeling language be expressive enough so that every detail you can mathematically specialize on and optimize for is there, but you want to make it easy for users to actually use. Striking that balance is difficult. ModelingToolkit.jl has around 4 years in various prototype forms going through and breaking designs until we found one that could actually solve the problem to the level we hoped.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-is-julia-code-generated-from-symbolic-expressions&quot;&gt;How is Julia code generated from symbolic expressions?&lt;&#x2F;h4&gt;
&lt;p&gt;The symbolic expressions use the same exact pieces under different symantics. For example, square roots in the model are &lt;code&gt;sqrt&lt;&#x2F;code&gt; in Symbolics.jl and &lt;code&gt;sqrt&lt;&#x2F;code&gt; in Julia. This means all we have to do is take the symbolic expression and write it into a Julia function, and invoke the compiler. Invoking compilation on the fly as part of a symbolic language is an interesting challenge though, something that a tool like SymPy skips but which reduces speed by orders of magnitude. The specific details of this are pretty esoteric so I will spare you, but to make this all work we created a new hook into the Julia compiler called &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;SciML&#x2F;RuntimeGeneratedFunctions.jl&quot;&gt;RuntimeGeneratedFunctions&lt;&#x2F;a&gt; which allow for staged compilation that composes with garbage collection, making generated code safe.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-the-mechanisms-that-allow-easy-model-composition-with-modelingtoolkit-jl&quot;&gt;What are the mechanisms that allow easy model composition with ModelingToolkit.jl?&lt;&#x2F;h4&gt;
&lt;p&gt;It’s the acasual modeling. You can develop pieces in isolation and just declare relationships between the components. For example, build a model of a power generator, and a model of a computer chip. Now you want to connect these two completely different models? Physically, a wire would connect them and then Kirchoff’s laws would have to hold, i.e. the voltages would have to be equal at the connection points and the currents would sum to zero. So in ModelingToolkit.jl that’s what you’d do: you’d say “current from generator + current to chip = 0” and “voltage at generator = voltage at chip” and bingo you’re there. Now this might produce some redundant variables and equations, but it’s okay: the symbolic preprocessing system eliminates all of this and simplifies down to the most efficient problem to simulate. Then at the end, you can ask “give me the timeseries of the voltage at the chip” and it will give you it, regardless if it was actually in the simulation of not, because it has the information to reconstruct these values.&lt;&#x2F;p&gt;
&lt;p&gt;ModelingToolkit.jl goes one step further. There are &lt;code&gt;connect&lt;&#x2F;code&gt; statements which let you define a common behavior. For example, a &lt;code&gt;Pin&lt;&#x2F;code&gt; in an electrical circuit always has a voltage and a current, and those laws from above are how “connections” physically work. So at this higher level you can say “connect the pin of the generator to the pin of the circuit”, and it generates all of the physical relationships associated with that statement. There are many prebuilt systems which are coming very soon (likely to be completed before these responses are public!), so heat flow, enthalpy relationships, etc. are all simple &lt;code&gt;connect&lt;&#x2F;code&gt; statements. The connection mechanism is extendable too, so if you have a common meaning in say pharmacological models which differs, you can create a new variable type and make connections automatically enforce the laws you want. Connect the heart to the kidney means blood flow is conserved but oxygen is not. This makes it easy to specialize the systems to each of the specific scientific domains.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-the-advantages-of-symbolic-preprocessing-of-models&quot;&gt;What are the advantages of symbolic preprocessing of models?&lt;&#x2F;h4&gt;
&lt;p&gt;From the previous statement, note that simple modeling requires the ability to build things in isolation, and then just say “a=b”. Numerically simulating with “a=b” is rather difficult though, numerical methods really want to not solve equations exactly. But if the current at one side is 10^(-8) higher than the other, you lose conservation of current, and you can have the power of the system steadily rising until it spirals out of control and the simulation crashes. This is actually a very common behavior in causal modeling systems. But if you eliminate the variable “b” and replace it with “a” in every place where it shows up, and then if the user asks for “b” you give them “a”, now you’ve symbolically enforced equality and you will never have a numerical issue due to that effect. So not only does it make the set of equations you have to solve smaller (making the solving process faster), but it also makes the numerical solving a lot more stable and more likely to succeed.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;who-are-modelingtoolkit-jl-and-symbolics-jl-aimed-to-beyond-academia-do-you-consider-people-in-the-industry-might-find-them-useful&quot;&gt;Who are ModelingToolkit.jl and Symbolics.jl aimed to? Beyond academia, do you consider people in the industry might find them useful?&lt;&#x2F;h4&gt;
&lt;p&gt;Symbolics.jl is more academically focused. People doing symbolic computer algebra are everywhere, but I tend to see more in academia. Physicists, computational biologists, etc. Because Symbolics.jl allows for translating back and forth between Julia code and symbolic code automatically, we’re seeing computer scientists even adopt it as a nice and easy way to analyze code.&lt;&#x2F;p&gt;
&lt;p&gt;ModelingToolkit.jl on the other hand is more focused towards engineers and modelers. Mechanical engineers, robotics experts, building designers, synthetic biologists. These people are found commonly in both academia and industry. We’re getting a lot of praise from industry users of ModelingToolkit.jl already, so it’s likely to find a nice foothold there.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-many-people-are-involved-in-the-projects-what-are-their-backgrounds&quot;&gt;How many people are involved in the projects? What are their backgrounds?&lt;&#x2F;h4&gt;
&lt;p&gt;There are far too many involved, so I’m just going to give a shoutout to the top few. Yingbo Ma is a super star, still an undergrad but a major contributor to both SciML (the differential equation solvers and ModelingToolkit.jl) and JuliaSymbolics. Shashi Gowda is a PhD student at MIT who has been driving a lot of the internals of JuliaSymbolics. Then there have been many contributions by NASA folks, high schoolers, professors in math departments and biology departments, pandemic researchers, etc. It’s still very early on in the project but the community around it is already great.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-the-next-steps-for-each-project&quot;&gt;What are the next steps for each project?&lt;&#x2F;h4&gt;
&lt;p&gt;We’re going to have a major announcement very soon, so stay tuned.&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>Scientific Machine Learning with Julia: the SciML ecosystem</title>
          <pubDate>Fri, 13 Nov 2020 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://blog.lambdaclass.com/posts/scientific-machine-learning-with-julia-the-sciml-ecosystem/</link>
          <guid>https://blog.lambdaclass.com/posts/scientific-machine-learning-with-julia-the-sciml-ecosystem/</guid>
          <description xml:base="https://blog.lambdaclass.com/posts/scientific-machine-learning-with-julia-the-sciml-ecosystem/">&lt;h4 id=&quot;interview-with-chris-rackauckas&quot;&gt;Interview with Chris Rackauckas&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-zBLCNU10DE1qv5PG4wdwbg.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We live in a complex world. For those scientists who dare to immerse themselves in that complexity and generate a deeper understanding of it, it is very common to have to deal with differential equation models that are not possible to solve without the use of a computer.&lt;&#x2F;p&gt;
&lt;p&gt;A lot of time is usually be spent in coding the particular differential equation for each problem. Julia SciML works to create and maintain tools that improve this process— from the creation of a framework that allows to automate the pipeline to create and solve problem-specific differential equations with a high level syntax, to introducing machine learning methods to infer unknown components of the model, and many other functionalities.&lt;&#x2F;p&gt;
&lt;p&gt;We interviewed the creator of SciML, Chris Rackauckas, to get to know a little more about his work.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-bCCWEZqP_ORcTiLPPV13rw.png&quot; alt=&quot;&quot; &#x2F;&gt;Source: DifferentialEquations.jl documentation&lt;&#x2F;p&gt;
&lt;h4 id=&quot;please-tell-us-a-bit-about-yourself-what-is-your-background-what-is-your-current-position&quot;&gt;Please tell us a bit about yourself. What is your background? what is your current position?&lt;&#x2F;h4&gt;
&lt;p&gt;I am an applied mathematics instructor at MIT, the Director of Scientific Research at Pumas-AI, and a senior research analyst at the University of Maryland, School of Pharmacy. My background is numerical differential equations and systems biology, where my PhD was in new methods for efficient solving of stochastic differential equations to model the control of randomness in the developing zebrafish hindbrain.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-sciml-why-was-it-born-and-what-s-its-purpose&quot;&gt;What is SciML? Why was it born and what’s its purpose?&lt;&#x2F;h4&gt;
&lt;p&gt;Before the “SciML” organization, there was just DifferentialEquations.jl and JuliaDiffEq, but it grew beyond just a single project. There were methods for symbolically manipulating equations, sparse automatic differentiation, automated model discovery, neural PDE solvers, and even packages in Python and R for using these tools. So the name didn’t fit and we did a reorganization around the central principle: scientific machine learning. Scientific machine learning is a burgeoning field that mixes scientific computing, like differential equation modeling, with machine learning. That is the essence of the organization: many tools for scientific simulation with differential equation solvers, chemical reaction network tools and N-body simulators, but all of them can compose with machine learning.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-YtKzMw7VOvNbOCsOkXwU0A.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;scientific-computing-and-machine-learning-are-often-perceived-as-very-different-areas-what-would-you-say-are-the-strengths-and-weaknesses-of-each-one-and-how-does-sciml-take-advantage-of-them&quot;&gt;Scientific Computing and Machine Learning are often perceived as very different areas. What would you say are the strengths and weaknesses of each one and how does SciML take advantage of them?&lt;&#x2F;h4&gt;
&lt;p&gt;Scientific computing generally requires a lot of prior knowledge about the system. You need to be able to create a “mechanistic model”, which requires knowing the physical laws, the chemicals which react, or other way to mathematically encode each interaction of the system. If you know this, great! Then you have a very predictive model. You might know all of the chemicals which interact but not know the reaction rates, and then 12 data points can turn this into quite a predictive model. So these models are interpretable (since it’s all about the mechanism), data efficient, etc. They are great at extrapolating too: the theory of gravity gives pretty good predictions for what happens on Earth as it does for the solar system as it does for galaxies.&lt;&#x2F;p&gt;
&lt;p&gt;Data-driven modeling, like machine learning, takes a completely opposite approach of being “data first”. You have a non-mechanistic model, and you “train” the model based on the data. This requires a lot of data, but you can do this even when you have no idea what the mechanism is. What’s the mechanism for what movie someone will want to watch next on Netflix given the previous movies they’ve seen? Einstein didn’t have a theory for that! But with big data, you can generate such a model.&lt;&#x2F;p&gt;
&lt;p&gt;Scientific machine learning is about pairing together these two paradigms. Incorporating mechanism into machine learning makes it more interpretable, more data efficient, and better able to predict beyond the training data, all without requiring that you know all of the mechanisms. We’re using this in cases like pharmacometrics, where in the first clinical trial we may not know everything about how the drug works, but we can start with a pretty good guess by using mechanistic models derived for similar drugs, and use the incoming data to train models which transforms the prior model towards the data.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-ihyMT0ujkdDopf3M8eiXUw.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-neural-odes-when-is-it-appropiate-to-work-with-one-do-you-fear-that-accuracy-or-interpretability-is-lost-by-introducing-a-neural-network-as-part-of-the-equation-aren-t-there-other-learning-methodologies-suited-for-such-a-thing&quot;&gt;What are Neural ODEs? When is it appropiate to work with one? Do you fear that accuracy or interpretability is lost by introducing a Neural Network as part of the equation? Aren’t there other learning methodologies suited for such a thing?&lt;&#x2F;h4&gt;
&lt;p&gt;Neural Ordinary Differential Equations or Neural ODEs are ordinary differential equations defined by a neural network. Indeed the result is less interpretable than having a mechanistic physical model, but it allows for the model to be learned directly from data. The neural network makes it not just estimating parameters, but estimating functions. As a middle ground, we created the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2001.04385&quot;&gt;universal differential equation&lt;&#x2F;a&gt; which is a partially mechanistic model where the neural networks fill in areas of the model which are unknown or have a lot of uncertainty. In this sense, there is more of a continuum between the data-driven and mechanistic models.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;currently-there-exist-many-differential-equations-solvers-why-do-you-think-this-is-the-case-is-there-a-way-to-choose-the-best-one-for-each-situation&quot;&gt;Currently, there exist many differential equations solvers, why do you think this is the case? Is there a way to choose the best one for each situation?&lt;&#x2F;h4&gt;
&lt;p&gt;We created an automated algorithm chooser in order to mitigate this issue. If you do ‘solve(prob)’ (i.e. don’t specify a solver algorithm), it will choose one for you. Then you can give it hints to go down different branches. As time goes on I think we will keep refining that algorithm and pushing more people towards that.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-the-key-reasons-the-sciml-differential-equations-solver-is-so-fast-how-does-it-differ-from-others-how-influential-was-writing-it-in-julia&quot;&gt;What are the key reasons the SciML Differential Equations solver is so fast? How does it differ from others? How influential was writing it in Julia?&lt;&#x2F;h4&gt;
&lt;p&gt;Every differential equation solver specializes on some aspect of the differential equation, giving them different performance aspects. For example, BDF integrators, “the standard” for stiff equations, use values from past steps. This can speed things up if the equation is not too stiff, but if it’s too stiff then you cannot use a high order (known as the Dahlquist barrier) and it slows down. So it’s problem dependent as to how well it can mitigate numerical issues, which means for some problems it’s fast and in others it breaks down. Then, if you have discontinuities which are frequent, like dosing in pharmacometrics simulations, this also requires order reduction and thus makes this particular method slower. DifferentialEquations.jl has about 300 methods when you consider all of the tableaus across not just ODEs but also SDEs, DAEs, and DDEs, and it’s this collection that allows it to be efficient.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;regarding-the-importance-of-being-able-to-quantify-the-uncertainty-of-the-numerical-resolution-when-solving-differential-equations-how-does-sciml-address-this-problem&quot;&gt;Regarding the importance of being able to quantify the uncertainty of the numerical resolution when solving differential equations, how does SciML address this problem?&lt;&#x2F;h4&gt;
&lt;p&gt;DifferentialEquations.jl comes with a module DiffEqUncertainty.jl that gives sampling-based estimates of numerical uncertainty by causing jitter on the order of the error estimates calculated on each step. Normally these error estimates are only used for adapting dt, but this gives a way to get essentially a free estimate of what other possible paths look like. Andrew Stuart’s group at CalTech then has a full publication that describes that this method indeed matches the error distribution of the full solve. So if you run this a hundred times you’ll get a sense of what all of the possible trajectories could’ve been given the error tolerance that you allowed.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-are-multiscalearrays-in-what-ways-do-these-data-structures-help-us-in-simulating-complex-scientific-models&quot;&gt;What are MultiScaleArrays? In what ways do these data structures help us in simulating complex scientific models?&lt;&#x2F;h4&gt;
&lt;p&gt;The differential equation solvers, and actually all of the SciML ecosystem, work on abstract interfaces which allow for the concrete implementation of a type to be radically different from the standard implementation. MultiScaleArrays is a nice example of this where an entire multi-scale model is represented as both a graph structure and an array simultaneously. This lets the user write a model like “for every cell in the lung, do the chemical reactions of a lung cell” to define a model, but have the stiff high-performance ODE solver automatically know how to interact with this object. It’s not even an array: it’s an array of arrays of arrays etc., which acts like an array. In this form it’s very efficient to allows cells to divide and die, and the ODE solver will adapt the size of the solution vector automatically as this changes.&lt;&#x2F;p&gt;
&lt;p&gt;While this was made for the specific case of multi-scale biological modeling in mind, other users have since come up with other great examples. CuArrays are CUDA-accelerated arrays that live on the GPU that can be dropped in as a replacement to the standard array, or ComponentArrays.jl defines an array type similar to MultiScaleArrays which is backed by a real vector, so it’s faster for standard computations but slower for size changes. A lot of new features can thus be directly added and optimized in the ODE solver just by changing the input types!&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-crRcFtHznAsYXDCb1uh4oQ.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;are-the-processes-of-solving-differential-equations-and-training-a-neural-networks-similar-how-do-you-put-together-both-frameworks&quot;&gt;Are the processes of solving differential equations and training a Neural Networks similar? How do you put together both frameworks?&lt;&#x2F;h4&gt;
&lt;p&gt;Training a neural network is solving an ODE defined by the gradient of the loss function until zero. Solving that ODE with Euler’s method is gradient descent. So you could use an ODE solver as the algorithm for training a neural ODE, and there is a use case that we’re looking into for that. Differential equations are more ubiquitous than I think most people realize.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;is-gpu-computing-integrated-in-the-sciml-ecosystem-how-important-is-having-this-feature-to-a-scientific-computing-framework-nowadays&quot;&gt;Is GPU computing integrated in the SciML ecosystem? How important is having this feature to a scientific computing framework nowadays?&lt;&#x2F;h4&gt;
&lt;p&gt;Yes, there’s two major ways. If you have “big kernels”, like PDE solving or neural networks integrated into models, you can do those calculations on the GPU. This is what’s known as “within-method parallelism”. One of the more recent techniques that we have is “between-method parallelism”, where we can automatically generate CUDA kernels from your model and parallelize that between trajectories of the solution. This uses some fancy code generation tricks thanks to tools like KernelAbstractions.jl, and allows “small problems” to have an effective way to use GPUs as well.&lt;&#x2F;p&gt;
&lt;p&gt;How important is it? Somewhat. There are problems which are extremely GPU-parallelizable, like neural ODEs and PDEs, and there are problems which are not, like lots of semi-mechanistic universal differential equation models. Whether a GPU is useful is very dependent on what and how you’re trying to model.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;in-which-cases-is-it-worth-to-add-a-bayesian-analysis-to-the-parameter-estimation-for-example-with-the-use-of-diffeqbayes-jl-what-are-its-advantages-over-more-classical-optimization-algorithms&quot;&gt;In which cases is it worth to add a Bayesian analysis to the parameter estimation, for example with the use of DiffEqBayes.jl? What are its advantages over more classical optimization algorithms?&lt;&#x2F;h4&gt;
&lt;p&gt;Bayesian analysis gives you a posterior distribution which has a sense of uncertainty quantification, i.e. it doesn’t just give you the “best parameter” but also a distribution which you can use to understand the error bars on your parameter estimate. In many cases this is a fundamentally interesting quantity. For example, in pharmacology we often want to know the probability that the drug concentration is in the safe zone. To evaluate this, we need a probabilistic fit of the model since only by including the uncertainty of our parameters can we get an accurate guess of the probabilities of the dynamics.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-c8-WwVO2Mlef4QqXP7SvSA.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;are-there-any-relevant-books-or-papers-you-would-like-to-recommend-for-digging-deeper-in-these-topics&quot;&gt;Are there any relevant books or papers you would like to recommend for digging deeper in these topics?&lt;&#x2F;h4&gt;
&lt;p&gt;Books schmooks. You’ll want to go directly to the sources. The only books I really recommend these days are Ernst Hairer’s “Solving Ordinary Differential Equations” I and II tomes: those are a work of art. Also Kloeden’s book on numerical methods for stochastic differential equations. Other than that, dive right into the scientific literature.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-next-for-sciml-are-you-currently-working-on-some-other-features-to-add-in-the-near-future&quot;&gt;What is next for SciML? Are you currently working on some other features to add in the near future?&lt;&#x2F;h4&gt;
&lt;p&gt;There’s tons we’re doing! I think a lot of what’s next is the integration of symbolic computing into all of our tools. ModelingToolkit.jl is the centerpiece of that push, and while I gave a talk at JuliaCon 2020 showcasing how it can be used as an automated code optimization tool (and gave a PyData 2020 talk on how you can GPU-accelerate ODE solves in R using it!), it’s so much more than that. It’s sometimes hard to numerically do things correctly, like ensuring positivity in an ODE solution can be difficult. But if you log transformed your model, then by definition your solution will always be positive. Right now this is up to the user, but what if we could automatically change the equations you wrote so that, not only are they more efficient, but they are also mathematically easier to solve and estimate? That’s the scope of ModelingToolkit, and if that interests you then you might want to stay tuned to that and its sister product NeuralSim which is about automated surrogate acceleration for the ModelingToolkit modeling language.&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>Julia GPU</title>
          <pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://blog.lambdaclass.com/posts/julia-gpu/</link>
          <guid>https://blog.lambdaclass.com/posts/julia-gpu/</guid>
          <description xml:base="https://blog.lambdaclass.com/posts/julia-gpu/">&lt;h3 id=&quot;how-the-julia-language-is-making-it-easy-for-programmers-to-use-gpu-capabilities-with-juliagpu&quot;&gt;How the Julia language is making it easy for programmers to use GPU capabilities with JuliaGPU&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-KJX3T1Y9T1Cj0aV3m-A22w.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We are living in a time where more and more data is being created every day as well as new techniques and complex algorithms that try to extract the most out of it. As such, CPU capabilities are approaching a bottleneck in their computing power. GPU computing opened its way into a new paradigm for high-performance and parallel computation a long time ago, but it was not until recently that it become massively used for data science.&lt;br &#x2F;&gt;
In this interview, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;maleadt&quot;&gt;Tim Besard&lt;&#x2F;a&gt;, one of the main contributors to the JuliaGPU project, digs into some of the details about GPU computing and the features that make Julia a language suited for such tasks, not only from a performance perspective but also from a user one.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;please-tell-us-a-bit-about-yourself-what-is-your-background-what-is-your-current-position&quot;&gt;Please tell us a bit about yourself. What is your background? what is your current position?&lt;&#x2F;h4&gt;
&lt;p&gt;I’ve always been interested in systems programming, and after obtaining my CS degree I got the opportunity to start a PhD at Ghent University, Belgium, right when Julia was first released around 2012. The language seemed intriguing, and since I wanted to gain some experience with LLVM, I decided to port some image processing research code from MATLAB and C++ to Julia. The goal was to match performance of the C++ version, but some of its kernels were implemented in CUDA C… So obviously Julia needed a GPU back-end!&lt;&#x2F;p&gt;
&lt;p&gt;That was easier said than done, of course, and much of my PhD was about implementing that back-end and (re)structuring the existing Julia compiler to facilitate these additional back-ends. Nowadays I’m at Julia Computing, where I still work on everything GPU-related.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-juliagpu-what-is-the-goal-of-the-project&quot;&gt;What is JuliaGPU? What is the goal of the project?&lt;&#x2F;h4&gt;
&lt;p&gt;JuliaGPU is the name we use to group GPU-related resources in Julia: There’s a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;JuliaGPU&quot;&gt;GitHub organization&lt;&#x2F;a&gt; where most packages are hosted, a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;juliagpu.org&#x2F;&quot;&gt;website&lt;&#x2F;a&gt; to point the way for new users, we have &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;JuliaGPU&#x2F;gitlab-ci&quot;&gt;CI infrastructure&lt;&#x2F;a&gt; for JuliaGPU projects, there’s a Slack channel and Discourse category, etc.&lt;&#x2F;p&gt;
&lt;p&gt;The goal of all this is to make it easier to use GPUs for all kinds of users. Current technologies often impose significant barriers to entry: CUDA is fairly tricky to install, C and C++ are not familiar to many users, etc. With the software we develop as part of the JuliaGPU organization, we aim to make it easy to use GPUs, without hindering the ability to optimize or use low-level features that the hardware has to offer.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-gpu-computing-how-important-is-it-nowadays&quot;&gt;What is GPU computing? How important is it nowadays?&lt;&#x2F;h4&gt;
&lt;p&gt;GPU computing means using the GPU, a device originally designed for graphics processing, to perform general-purpose computations. It has grown more important now that CPU performance is not improving as steadily as it used to. Instead, specialized devices like GPUs or FPGAs are increasingly used to improve the performance of certain computations. In the case of GPUs, the architecture is a great fit to perform highly-parallel applications. Machine learning networks are a good example of such parallel applications, and their popularity is one of the reasons GPUs have become so important.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;do-you-think-julia-is-an-appropriate-language-to-efficiently-use-gpu-capabilities-why&quot;&gt;Do you think Julia is an appropriate language to efficiently use GPU capabilities? Why?&lt;&#x2F;h4&gt;
&lt;p&gt;Julia’s main advantage is that the language was designed to be compiled. Even though the syntax is high-level, the generated machine code is&lt;br &#x2F;&gt;
compact and has great performance characteristics (for more details, see &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;janvitek.org&#x2F;pubs&#x2F;oopsla18b.pdf&quot;&gt;this paper&lt;&#x2F;a&gt;). This is crucial for GPU execution, where we are required to run native binaries and cannot easily (or efficiently) interpret code as is often required by other language’s semantics.&lt;&#x2F;p&gt;
&lt;p&gt;Because we’re able to directly compile Julia for GPUs, we can use almost all of the language’s features to build powerful abstractions. For example, you can define your own types, use those in GPU arrays, compose that with existing abstractions like lazy “Transpose” wrappers, access those on the GPU while benefiting from automatic bounds-checking (if needed), etc.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;from-a-python-programmer-perspective-how-does-cuda-jl-compare-to-pycuda-are-their-functionalities-equivalent&quot;&gt;From a Python programmer perspective, how does CUDA.jl compare to PyCUDA? Are their functionalities equivalent?&lt;&#x2F;h4&gt;
&lt;p&gt;PyCUDA gives the programmer access to the CUDA APIs, with high-level Python functions that are much easier to use. CUDA.jl provides the same, but in Julia. The &lt;code&gt;hello world&lt;&#x2F;code&gt; from PyCUDA’s home page looks almost identical in Julia:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;using CUDA&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;function multiply_them(dest, a, b)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; i = threadIdx().x&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; dest[i] = a[i] * b[i]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; return&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;end&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;a = CuArray(randn(Float32, 400))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;b = CuArray(randn(Float32, 400))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;dest = similar(a)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;@cuda threads=400 multiply_them(dest, a, b)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;println(dest-a.*b)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There’s one very big difference: “multiply_them” here is a function written in Julia, whereas PyCUDA uses a kernel written in CUDA C. The reason is straightforward: Python is not simple to compile. Of course, projects like Numba prove that it is very much possible to do so, but in the end those are separate compilers that try to match the reference Python compilers as closely as possible. With CUDA.jl, we integrate with that reference compiler, so it’s much easier to guarantee consistent semantics and follow suit when the language changes (for more details,&lt;br &#x2F;&gt;
refer to &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1712.03112&quot;&gt;this paper&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;h4 id=&quot;are-the-packages-in-the-juliagpu-organization-targeted-to-experienced-programmers-only&quot;&gt;Are the packages in the JuliaGPU organization targeted to experienced programmers only?&lt;&#x2F;h4&gt;
&lt;p&gt;Not at all. CUDA.jl targets different kinds of (GPU) programmers. If you are confident writing your own kernels, you can do so, while using all of the low-level features CUDA GPUs have to offer. But if you are new to the world of GPU programming, you can use high-level array operations that use existing kernels in CUDA.jl. For example, the above element-wise multiplication could just as well be written as:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;using CUDA&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;a = CuArray(randn(Float32, 400))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;b = CuArray(randn(Float32, 400))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;dest = a .* b&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h4 id=&quot;is-it-necessary-to-know-how-to-code-in-cuda-jl-to-take-full-advantage-of-gpu-computing-in-julia&quot;&gt;Is it necessary to know how to code in CUDA.jl to take full advantage of GPU computing in Julia?&lt;&#x2F;h4&gt;
&lt;p&gt;Not for most users. Julia has a powerful language of generic array operations (“map”, “reduce”, “broadcast”, “accumulate”, etc) which can be applied to all kinds of arrays, including GPU arrays. That means you can often re-use your codebase developed for the CPU with CUDA.jl (&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;S0965997818310123&quot;&gt;this paper&lt;&#x2F;a&gt; shows some powerful examples). Doing so often requires minimal changes: changing the array type, making sure you use array operations instead of for loops, etc.&lt;&#x2F;p&gt;
&lt;p&gt;It’s possible you need to go beyond this style of programming, e.g., because your application doesn’t map cleanly onto array operations, to use specific GPU features, etc. In that case, some basic knowledge about CUDA and the GPU programming model is sufficient to write kernels in CUDA.jl.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-is-the-experience-of-coding-a-kernel-in-cuda-jl-in-comparison-to-cuda-c-and-how-transferable-is-the-knowledge-to-one-another&quot;&gt;How is the experience of coding a kernel in CUDA.jl in comparison to CUDA C and how transferable is the knowledge to one another?&lt;&#x2F;h4&gt;
&lt;p&gt;It’s very similar, and that’s by design: We try to keep the kernel abstractions in CUDA.jl close to their CUDA C counterparts such that the programming environment is familiar to existing GPU programmers. Of course, by using a high-level source language there’s many quality-of-life improvements. You can allocated shared memory, for example, statically and dynamically as in CUDA C, but instead of a raw pointers we use an N-dimensional array object you can easily index. An example from the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;developer.nvidia.com&#x2F;blog&#x2F;using-shared-memory-cuda-cc&#x2F;&quot;&gt;NVIDIA developer blog&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;__global__ void staticReverse(int *d, int n)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;{&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; __shared__ int s[64];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; int t = threadIdx.x;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; int tr = n-t-1;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; s[t] = d[t];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; __syncthreads();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; d[t] = s[tr];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The CUDA.jl equivalent of this kernel looks very familiar, but uses array objects instead of raw pointers:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;function staticReverse(d)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; s = @cuStaticSharedMem(Int, 64)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; t = threadIdx().x&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; tr = length(d)-t+1&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; s[t] = d[t]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; sync_threads()&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; d[t] = s[tr]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; return&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;end&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Using array objects has many advantages, e.g. multi-dimensional is greatly simplified and we can just do “d[i,j]”. But it’s also safer, because these accesses are bounds checked:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;julia&amp;gt; a = CuArray(1:64)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;64-element CuArray{Int64,1}:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 1&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 2&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 3&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; ⋮&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 62&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 63&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 64&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;julia&amp;gt; @cuda threads=65 staticReverse(a)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;ERROR: a exception was thrown during kernel execution.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;Stacktrace:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; [1] throw_boundserror at abstractarray.jl:541&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Bounds checking isn’t free, of course, and once we’re certain our code is correct we can add an “@inbounds” annotation to our kernel and get the high-performance code we expect:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;julia&amp;gt; @device_code_ptx @cuda threads=64 staticReverse(a)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;.visible .entry staticReverse(.param .align 8 .b8 d[16]) {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; .reg .b32 %r&amp;lt;2&amp;gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; .reg .b64 %rd&amp;lt;15&amp;gt;;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; .shared .align 32 .b8 s[512];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;mov.b64 %rd1, d;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; ld.param.u64 %rd2, [%rd1];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; ld.param.u64 %rd3, [%rd1+8];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; mov.u32 %r1, %tid.x;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; cvt.u64.u32 %rd4, %r1;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; mul.wide.u32 %rd5, %r1, 8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; add.s64 %rd6, %rd5, -8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; add.s64 %rd7, %rd3, %rd6;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; ld.global.u64 %rd8, [%rd7+8];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; mov.u64 %rd9, s;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; add.s64 %rd10, %rd9, %rd6;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; st.shared.u64 [%rd10+8], %rd8;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; bar.sync 0;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; sub.s64 %rd11, %rd2, %rd4;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; shl.b64 %rd12, %rd11, 3;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; add.s64 %rd13, %rd9, %rd12;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; ld.shared.u64 %rd14, [%rd13+-8];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; st.global.u64 [%rd7+8], %rd14;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; ret;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;julia&amp;gt; a&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;64-element CuArray{Int64,1}:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 64&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 63&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 62&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; ⋮&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 3&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 2&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 1&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Tools like “@device_code_ptx” make it easy for an experienced developer to inspect generated code and ensure the compiler does what he wants.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;why-does-having-a-compiler-have-such-an-impact-in-libraries-like-cuda-jl-how-was-the-process-of-integrating-it-to-the-julia-compiler&quot;&gt;Why does having a compiler have such an impact in libraries like CUDA.jl? (How was the process of integrating it to the Julia compiler?)&lt;&#x2F;h4&gt;
&lt;p&gt;Because we have a compiler at our disposal, we can rely on higher-order functions and other generic abstractions that specialize based on the arguments that users provide. That greatly simplifies our library, but also gives the user very powerful tools. As an example, we have carefully implemented a &lt;code&gt;mapreduce&lt;&#x2F;code&gt; function that uses shared memory, warp intrinsics, etc to perform a high-performance reduction. The implementation is generic though, and will automatically re-specialize (even at run time) based on the arguments to the function:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;julia&amp;gt; mapreduce(identity, +, CuArray([1,2,3]))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;6&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;julia&amp;gt; mapreduce(sin, *, CuArray([1.1,2.2,3.3]))&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;-0.11366175839582586&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;With this powerful &lt;code&gt;mapreduce&lt;&#x2F;code&gt; abstraction, implemented by a experienced GPU programmer, other developers can create derived abstractions without such experience. For example, let’s implement a &lt;code&gt;count&lt;&#x2F;code&gt; function that evaluates for how many items a predicate holds true:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;count(predicate, array) = mapreduce(predicate, +, array)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;julia&amp;gt; a = CUDA.rand(Int8, 4)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;4-element CuArray{Int8,1}:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 51&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 3&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 70&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; 100&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;julia&amp;gt; count(iseven, a)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;2&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Even though our &lt;code&gt;mapreduce&lt;&#x2F;code&gt; implementation has not been specifically implemented for the &lt;code&gt;Int8&lt;&#x2F;code&gt; type or the &lt;code&gt;iseven&lt;&#x2F;code&gt; predicate, the Julia compiler automatically specializes the implementation, resulting in kernel optimized for this specific invocation.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-were-the-biggest-challenges-when-developing-packages-for-juliagpu-particularly-writing-a-low-level-package-such-as-cuda-jl-in-a-high-level-programming-language-such-as-julia&quot;&gt;What were the biggest challenges when developing packages for JuliaGPU, particularly writing a low level package such as CUDA.jl in a high level programming language such as Julia?&lt;&#x2F;h4&gt;
&lt;p&gt;Much of the initial work focused on developing tools that make it possible to write low-level code in Julia. For example, we developed the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;maleadt&#x2F;LLVM.jl&quot;&gt;LLVM.jl&lt;&#x2F;a&gt; package that gives us access to the LLVM APIs. Recently, our focus has shifted towards generalizing this functionality so that other GPU back-ends, like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;JuliaGPU&#x2F;AMDGPU.jl&quot;&gt;AMDGPU.jl&lt;&#x2F;a&gt; or &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;JuliaGPU&#x2F;oneAPI.jl&quot;&gt;oneAPI.jl&lt;&#x2F;a&gt; can benefit from developments to CUDA.jl. Vendor-neutral array operations, for examples, are now implemented in &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;JuliaGPU&#x2F;GPUArrays.jl&quot;&gt;GPUArrays.jl&lt;&#x2F;a&gt; whereas shared compiler functionality now lives in &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;JuliaGPU&#x2F;GPUCompiler.jl&quot;&gt;GPUCompiler.jl&lt;&#x2F;a&gt;. That should make it possible to work on several GPU back-ends, even though most of them are maintained by only a single developer.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;regarding-the-latest-release-announced-in-the-juliagpu-blog-about-multi-device-programming-what-are-the-difficulties-that-this-new-functionality-solves-is-this-relevant-in-the-industry-where-big-computational-resources-are-needed&quot;&gt;Regarding the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;juliagpu.org&#x2F;2020-07-18-cuda_1.3&#x2F;&quot;&gt;latest release&lt;&#x2F;a&gt; announced in the JuliaGPU blog about multi-device programming, what are the difficulties that this new functionality solves? Is this relevant in the industry where big computational resources are needed?&lt;&#x2F;h4&gt;
&lt;p&gt;In industry or large research labs, MPI is often used to distribute work across multiple nodes or GPUs. Julia’s MPI.jl supports that use case, and integrates with CUDA.jl where necessary. The multi-device functionality added to CUDA 1.3 additionally makes it possible to use multiple GPUs within a single process. It maps nicely on Julia’s task-based concurrency, and makes it easy to distribute work within a single node:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;Threads.@threads for dev in devices()&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; device!(dev)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt; # do some work here&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;end&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h4 id=&quot;what-are-the-plans-for-the-near-future&quot;&gt;&lt;strong&gt;What are the plans for the near future?&lt;&#x2F;strong&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;There aren’t any specific roadmaps, but one upcoming major feature is proper support for reduced-precision inputs, like 16-bits floating point. We already support Float16 arrays where CUBLAS or CUDNN does, but the next version of Julia will make it possible to write kernels that operate on these values.&lt;&#x2F;p&gt;
&lt;p&gt;Other than that, features come as they do :-) Be sure to subscribe to the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;juliagpu.org&#x2F;post&#x2F;&quot;&gt;JuliaGPU blog&lt;&#x2F;a&gt; where we publish a short post for every major release of Julia’s GPU back-ends.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;You can find Tim at @&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;maleadt&quot;&gt;maleadt&lt;&#x2F;a&gt; on Twitter!&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>For the first time, enjoy all the talks of BuzzConf 2020 online and free of charge!</title>
          <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://blog.lambdaclass.com/posts/for-the-first-time-enjoy-all-the-talks-of-buzzconf-2020-online-and-free-of-charge/</link>
          <guid>https://blog.lambdaclass.com/posts/for-the-first-time-enjoy-all-the-talks-of-buzzconf-2020-online-and-free-of-charge/</guid>
          <description xml:base="https://blog.lambdaclass.com/posts/for-the-first-time-enjoy-all-the-talks-of-buzzconf-2020-online-and-free-of-charge/">&lt;h3 id=&quot;come-see-buzzconf-2020-a-software-and-data-science-conference-open-and-online-for-one-and-for-all&quot;&gt;Come see BuzzConf 2020, a software and data science conference, open and online for one and for all!&lt;&#x2F;h3&gt;
&lt;p&gt;The third edition of BuzzConf will be held freely online via Zoom and Youtube live. We’re very proud of our speaker lineup, which covers a wide range of topics that expand the frontiers of our technical knowledge.&lt;&#x2F;p&gt;
&lt;p&gt;We will dive into the topics of functional programming, Julia, Python, data science, machine learning, observability, operating systems and more! We believe Functional Programming and Data Science are two of the most interesting topics in the field which will open many opportunities in the near future, and we want to bring the latest developments in these areas to the global community.&lt;&#x2F;p&gt;
&lt;p&gt;This conference is the result of a combined effort of private and public sectors from Argentina, which aspires to help the development of our country’s technical capabilities while spreading and sharing knowledge with the world.&lt;&#x2F;p&gt;
&lt;p&gt;We thank the generosity of the speakers who agreed to donate their time and we hope you enjoy their talks as much as we will.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-LzNJkTEcILfT_6U8t2baDg.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mon-july-27-2pm-pdt-6pm-gmt-3-9pm-utc&quot;&gt;Mon, July 27–2pm PDT &#x2F; 6pm GMT-3 &#x2F; 9pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;charity-majors-what-got-you-here-won-t-get-you-there-how-your-team-can-become-a-high-performing-team-by-embracing-observability&quot;&gt;Charity Majors — “What got you here won’t get you there: How your team can become a high-performing team by embracing observability”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;To Be Announced&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;92898927496&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;92898927496&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;QK6zEFdvXYw&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;QK6zEFdvXYw&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-fTWA2R7HUM90oPA7.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mon-july-27-3pm-pdt-7pm-gmt-3-10pm-utc&quot;&gt;Mon, July 27–3pm PDT &#x2F; 7pm GMT-3 &#x2F; 10pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;maria-vanina-martinez-symbolic-reasoning-to-model-sentiment-and-knowledge-diffusion-in-social-networks&quot;&gt;María Vanina Martinez: “Symbolic Reasoning to model Sentiment and Knowledge Diffusion in Social Networks”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;Social media platforms, taken in conjunction, can be seen as complex networks; in this context, understanding how agents react to sentiments expressed by their connections is of great interest. We show how Network Knowledge Bases help represent the integration of multiple social networks, and explore how information flow can be handled via belief revision operators for local (agent-specific) knowledge bases. We report on preliminary experiments on Twitter data showing that different agent types react differently to the same information — this is a first step toward developing symbolic tools to predict how agents behave asinformation flows in their social environment.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;92898927496&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;92898927496&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;QK6zEFdvXYw&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;QK6zEFdvXYw&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-Urum4vQ0RP0TZub3.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;tue-july-28-2pm-pdt-6pm-gmt-3-9pm-utc&quot;&gt;Tue, July 28–2pm PDT &#x2F; 6pm GMT-3 &#x2F; 9pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;will-kurt-the-limits-of-probability&quot;&gt;Will Kurt: “The Limits of Probability”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;Probability is an increasingly ubiquitous part of our daily lives, especially as developers, researchers and data scientists. It is easy to mistakenly think this powerful tool is all we need to understand our world. This talk will show how our current environment of global pandemic, political unrest and economic uncertainty forces us to face the limits of probability as a tool for reasoning and understanding. This talk will cover both practical examples of the limitations of probability as well as dive into the philosophical roots of these limitations to show that it cannot be our only means to engage with our world.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;97859783809&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;97859783809&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;N75ebGWz2o4&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;N75ebGWz2o4&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-a3kX6m-ky8tQK27S.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;tue-july-28-3pm-pdt-7pm-gmt-3-10pm-utc&quot;&gt;Tue, July 28–3pm PDT &#x2F; 7pm GMT-3 &#x2F; 10pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;lightning-talks&quot;&gt;Lightning Talks&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Juan Pablo Lorenzo:&lt;&#x2F;strong&gt; “Delete your code: in search of a minimalist approach to software development”&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Gajendra Deshpande:&lt;&#x2F;strong&gt; “Computation Techniques for Encrypted Data using Python”&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;97859783809&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;97859783809&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;N75ebGWz2o4&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;N75ebGWz2o4&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wed-july-29-2pm-pdt-6pm-gmt-3-9pm-utc&quot;&gt;Wed, July 29–2pm PDT &#x2F; 6pm GMT-3 &#x2F; 9pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;aditya-siram-what-fp-can-learn-from-static-introspection&quot;&gt;Aditya Siram: “What FP Can Learn From Static Introspection”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;What if compile time and type level programming in functional programming languages were easy, something you reach for without even thinking about it? What if you could debug type errors with a simple compile time print statement? Write highly flexible systems by being able to introspect into types at compile time? Pre-calculate large portions of your programs for great efficiency? Typed functional programming is a great and fun way to write resilient software, and as type systems have become more and more expressive in recent years, we are able to program sophisticated and useful properties at the type level for even better compile time safety. Just one problem: It is very difficult, requires advanced knowledge of the type system, the syntax is convoluted, the error messages are impenetrable, and it is nearly impossible to debug. This talk will dive into why we should steal static introspection from languages like Nim, and D, state-of-the-art imperative programming languages which can solve all these issues, make type systems much more approachable without losing any expressive power, and offer new design possibilities for functional programs.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;95139644343&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;95139644343&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;yGq0KnkqOgI&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;yGq0KnkqOgI&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-Koxyxq-wrBx9iGfe.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wed-july-29-3pm-pdt-7pm-gmt-3-10pm-utc&quot;&gt;Wed, July 29–3pm PDT &#x2F; 7pm GMT-3 &#x2F; 10pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;sergio-chouhy-deeploying-deep-q-learning-with-pytorch&quot;&gt;Sergio Chouhy: “Deeploying Deep Q Learning with Pytorch”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;Many things are being said about Deep Reinforcement Learning, but sometimes it is really hard to know where to start. In this talk, I will tell you all about the basis of this algorithms and show you how to deploy Deep Q Learning from scratch using Pytorch. I will be also talking about industrial applications for this technology.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;95139644343&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;95139644343&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;yGq0KnkqOgI&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;yGq0KnkqOgI&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-82lbeMCqONPXiKq1.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;thu-july-30-2pm-pdt-6pm-gmt-3-9pm-utc&quot;&gt;Thu, July 30–2pm PDT &#x2F; 6pm GMT-3 &#x2F; 9pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;viral-b-shah-julia-a-language-for-ai-and-much-more&quot;&gt;Viral B. Shah: “Julia — A language for AI and much more”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;The Julia language is now used by over half a million programmers worldwide. Created to solve the two language problem, Julia is demonstrating performance gains of 50x-100x for many data science tasks such as data loading, data processing, graph processing, machine learning and scaling. Robust support for modern deep learning and the ability to do differentiable programming in an intuitive way is quickly leading to Julia becoming the language of choice for AI workloads. My talk will discuss the origin story of Julia, the formation of the Julia community, and all the amazing things happening in the world of Julia.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;94906592252&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;94906592252&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;fbKHLdoG7wA&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;fbKHLdoG7wA&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0--gyZtKfGpWBIxRoz.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;thu-july-30-3pm-pdt-7pm-gmt-3-10pm-utc&quot;&gt;Thu, July 30–3pm PDT &#x2F; 7pm GMT-3 &#x2F; 10pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;chris-rackauckas-sciml-how-language-is-changing-scientific-research&quot;&gt;Chris Rackauckas: “SciML: How Language is Changing Scientific Research”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;Scientific machine learning is a burgeoning field and its taking off in Julia. Why? The purpose of this talk is to dive into that question: how has language accelerated the development of Julia’s SciML ecosystem? The core is composibility through multiple dispatch. We will showcase how this feature is not only what makes standard Julia code as fast as C or Fortran, but also allows Julia to eschew the traditional idea of “machine learning frameworks” and instead have machine learning directly work on the standard functions and libraries of the whole Julia programming language. This language-wide differentiable programming then builds a foundation where existing climate models, helicopter simulations, and efficiency simulators for battery-powered airplanes can be instantly composed with new tools for machine learning, and we will demonstrate how this has changed the way that researchers in Julia do science.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;94906592252&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;94906592252&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;fbKHLdoG7wA&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;fbKHLdoG7wA&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-2EbV5-0r03R6aPYH.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;fri-july-31-2pm-pdt-6pm-gmt-3-9pm-utc&quot;&gt;Fri, July 31–2pm PDT &#x2F; 6pm GMT-3 &#x2F; 9pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;pablo-fernandez-machine-learning-in-the-real-world&quot;&gt;Pablo Fernandez: “Machine Learning in The Real World”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;A tour of the last 3 years of my career where I’ve productionized 3 different machine learning projects on kind-of-a-big-company (Despegar). Some of the challenges faced, not only technical but also from a product standpoint, some of the pedagogical work needed to convince others of letting important decisions be made by a machine. Hopefully insights that help you bring your own models to production.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;92628004626&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;92628004626&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;t-ebpSHyBEE&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;t-ebpSHyBEE&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-7y4gqQ8d8_6veW_j.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;fri-july-31-3pm-pdt-7pm-gmt-3-10pm-utc&quot;&gt;Fri, July 31–3pm PDT &#x2F; 7pm GMT-3 &#x2F; 10pm UTC&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;peter-alvaro-what-not-where-why-a-blue-sky-os&quot;&gt;Peter Alvaro: “What not where: why a blue sky OS?”&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;em&gt;A world of distributed, persistent memory is on its way. Our programming models traditionally operate on short-lived data representations tied to ephemeral contexts such as processes or computers. In the limit, however, data lifetime is infinite compared to these transient actors. We discuss the implications for programming models raised by a world of large and potentially persistent distributed memories, including the need for explicit, context-free, invariant data references. We present a novel operating system that uses wisdom from both storage and distributed systems to center the programming model around data as the primary citizen, and reflect on the transformative potential of this change for infrastructure and applications of the future.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on Zoom:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;92628004626&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;zoom.us&#x2F;j&#x2F;92628004626&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Join us on YouTube:&lt;&#x2F;em&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;t-ebpSHyBEE&quot;&gt;&lt;em&gt;https:&#x2F;&#x2F;youtu.be&#x2F;t-ebpSHyBEE&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-MOD81ymGRUNUrv7m.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We hope to see you there! There will be time for Q&amp;amp;A with the speakers. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.eventbrite.com.ar&#x2F;e&#x2F;buzzconf-2020-tickets-111836742708&quot;&gt;Register now&lt;&#x2F;a&gt;!&lt;&#x2F;p&gt;
</description>
      </item>
      <item>
          <title>Soss: Probabilistic Programming with Julia</title>
          <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://blog.lambdaclass.com/posts/soss-probabilistic-programming-with-julia/</link>
          <guid>https://blog.lambdaclass.com/posts/soss-probabilistic-programming-with-julia/</guid>
          <description xml:base="https://blog.lambdaclass.com/posts/soss-probabilistic-programming-with-julia/">&lt;h4 id=&quot;an-interview-with-its-creator-chad-scherrer&quot;&gt;An interview with its creator, Chad Scherrer&lt;&#x2F;h4&gt;
&lt;p&gt;By: Javier Rodríguez Chatruc and Federico Carrone&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-m6924vcooeuQEu7FiBOI5A.png&quot; alt=&quot;&quot; &#x2F;&gt;Credit: Chad Scherrer&lt;&#x2F;p&gt;
&lt;p&gt;Probabilistic programming is at this point an established field both for research and industry applications, but like everything else (especially in the tech industry), it is undergoing constant evolution. This is where &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;julialang.org&#x2F;&quot;&gt;Julia&lt;&#x2F;a&gt; comes in — designed for high performance in the world of data science, it seems to be the perfect fit for probabilistic programming.&lt;&#x2F;p&gt;
&lt;p&gt;To learn more about this world we contacted &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;chadscherrer&quot;&gt;Chad Scherrer&lt;&#x2F;a&gt;, the creator of &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;cscherrer&#x2F;Soss.jl&quot;&gt;Soss&lt;&#x2F;a&gt;, a probabilistic programming library written entirely in Julia. With a very clean syntax resembling math notation, Soss seems to bridge the gap between the more academic side of data science and the more technical&#x2F;developer one, while also providing speed and &lt;em&gt;first-class&lt;&#x2F;em&gt; models.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;please-tell-us-a-bit-about-yourself-what-is-your-background-what-is-your-current-position&quot;&gt;Please tell us a bit about yourself. What is your background? what is your current position?&lt;&#x2F;h4&gt;
&lt;p&gt;Starting out, I thought I would end up focusing on algebraic topology. So I did coursework along these lines for a few years before switching to stats. My thesis is on a special case of multivariate normal distributions with a group symmetry, so algebra still plays a big part.&lt;&#x2F;p&gt;
&lt;p&gt;After graduating, I worked at Pacific Northwest National Laboratory, mostly doing computational statistics. I learned some Python, then R. The high-level coding was nice, but I was frustrated by how awkward it was to make it fast.&lt;&#x2F;p&gt;
&lt;p&gt;One day I came across this “Great Computer Language Shootout”, where Ocaml was really dominating. So I used that for a few years. Then multicore hardware started really picking up, but at the time the Ocaml team said they wouldn’t really be doing anything with SMP (symmetric multiprocessing). So I started looking around again, and found Haskell.&lt;&#x2F;p&gt;
&lt;p&gt;Along the way, I had collaborated with the high-performance computing group doing &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1206.6409.pdf&quot;&gt;parallel&lt;&#x2F;a&gt; &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1212.4174v1.pdf&quot;&gt;machine learning&lt;&#x2F;a&gt; using C&#x2F;OpenMP. And I started getting interested in probabilistic programming. I wanted to make something like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;mcmc-jags.sourceforge.net&#x2F;&quot;&gt;JAGS&lt;&#x2F;a&gt;, but using Haskell and allowing more high-level expressiveness. So I collaborated with Galois to develop &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;cscherrer.github.io&#x2F;pdf&#x2F;Scherrer%20-%202012%20-%20Passage%20A%20Parallel%20Sampler%20Generator%20for%20Hierarchical%20Bayesian%20Modeling.pdf&quot;&gt;Passage&lt;&#x2F;a&gt;, which works in terms of a now-standard probability monad, and produces C&#x2F;OpenMP code for parallel Gibbs sampling.&lt;&#x2F;p&gt;
&lt;p&gt;Based on the Passage work, Galois started getting involved with &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.darpa.mil&#x2F;program&#x2F;probabilistic-programming-for-advancing-machine-Learning&quot;&gt;&lt;em&gt;Probabilistic Programming for Advancing Machine Learning (PPAML)&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;, but they needed someone to serve as technical lead. So I moved to Portland and did that for a few years. Galois is a (mostly) Haskell shop, so I was able to dig deeper into both Haskell and probabilistic programming.&lt;&#x2F;p&gt;
&lt;p&gt;Still wanting to extend some of the ideas from Passage, I moved to Seattle and spent a couple of years at &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.thisismetis.com&#x2F;&quot;&gt;Metis&lt;&#x2F;a&gt; teaching data science. In my free time, I got more up to speed on &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;julialang.org&#x2F;&quot;&gt;Julia&lt;&#x2F;a&gt;, and started work on what would become &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;cscherrer&#x2F;Soss.jl&quot;&gt;Soss&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;These days, I work as a Senior Research Scientist at &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.relational.ai&#x2F;&quot;&gt;RelationalAI&lt;&#x2F;a&gt;. Most machine learning pipelines treat database queries and model training as entirely independent, so to go between them requires throwing away all of the structure and just joining everything.&lt;&#x2F;p&gt;
&lt;p&gt;As it turns out, that throws away some big opportunities for optimization. So our system has an expressive language for reasoning about relational structure, and works in terms that make these optimizations natural for machine learning and probabilistic programming.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-probabilistic-programming-how-does-it-differ-from-other-forms-of-programming&quot;&gt;What is probabilistic programming? How does it differ from other forms of programming?&lt;&#x2F;h4&gt;
&lt;p&gt;When people talk about &lt;em&gt;Probabilistic Programming Languages&lt;&#x2F;em&gt; (PPLs), they usually mean a system for building and reasoning about Bayesian models. Maybe the simplest way to think about this is as a way of reasoning about simulations. Say you have a simulation that you can run to make a simulated “world”. Every part of the simulation has some randomness. This includes the things you can actually observe, but also the underlying choices the simulator made for things that affect those observations. But those are random too, so they might depend on &lt;em&gt;other&lt;&#x2F;em&gt; random choices.&lt;&#x2F;p&gt;
&lt;p&gt;Ok, so choices made along the way will affect the distribution of things downstream. But we can also use this to reason the other way! We observe some data, and ask “what choices along the way could have led to this?”&lt;&#x2F;p&gt;
&lt;p&gt;In the simplest case, say we have a simulation for biased coin flips where we pick a random probability of heads, say &lt;code&gt;p ~ Uniform(0,1)&lt;&#x2F;code&gt;, and simulate 20 flips. Then we observe 15 heads and 5 tails. We can’t say for certain what &lt;code&gt;p&lt;&#x2F;code&gt; was, but we can find a distribution that’s updated based on the observed data.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-did-you-get-into-julia-why-choose-julia-over-python-or-r&quot;&gt;How did you get into Julia? Why choose Julia over Python or R?&lt;&#x2F;h4&gt;
&lt;p&gt;I want to be able to express ideas at a high level of abstraction without sacrificing performance. I’ve used Python and R quite a bit, for the things I wanted to do I always felt constrained because getting performance always means pushing things to another language. Then there are concerns with the cost of crossing that language barrier, both in a human and computational sense.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-soss-how-did-it-come-about-and-what-was-the-motivation-behind-it&quot;&gt;What is Soss? How did it come about and what was the motivation behind it?&lt;&#x2F;h4&gt;
&lt;p&gt;Soss is a Julia-based PPL that represents the right-hand side of each assignment (&lt;code&gt;=&lt;&#x2F;code&gt;) and sample (&lt;code&gt;~&lt;&#x2F;code&gt;) as an AST. The nice thing about this is, it gives ultimate flexibility in what a model can do. For example, we have inference methods that take a &lt;code&gt;Model&lt;&#x2F;code&gt; and return an &lt;code&gt;AST&lt;&#x2F;code&gt; that generates code at run-time, but we also have model transformation functions that return another model. Models are first-class, and can be used inside other models, etc.&lt;&#x2F;p&gt;
&lt;p&gt;There are some other things too, for example we have an interface to SymPy so you can easily get to a symbolic representation of the log-density. Simplifications here can lead to faster code, so we also have a way to generate SSA Julia code from this. There’s still plenty more speed to be had, but I’ve seen 100x-1000x speedup with this vs a direct implementation.&lt;&#x2F;p&gt;
&lt;p&gt;I’ve wanted to build Soss for a long time, it was just a matter of finding a language with metaprogramming support that could handle the syntax I wanted, while also having the speed and a good numerically-oriented ecosystem.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-does-the-probabilistic-programming-ecosystem-in-julia-compare-to-the-ones-in-python-r-in-particular-how-does-soss-compare-to-pymc3&quot;&gt;How does the probabilistic programming ecosystem in Julia compare to the ones in Python&#x2F;R? In particular, how does Soss compare to PyMC3?&lt;&#x2F;h4&gt;
&lt;p&gt;To get speed, both Python and R have to call to other languages. I’ve spent a lot of time using PyMC3, and I really like it. But it still requires keeping in your head which lines of code are talking to Python, vs which are talking to Theano. There’s a language barrier to play across, and losing track of it tends to break things.&lt;&#x2F;p&gt;
&lt;p&gt;When you write a Soss model, it’s all Julia. You can use Julia functions freely. Even if you want to do Soss development (please do!), it’s still all Julia.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;on-that-note-who-is-the-end-user-for-the-library-is-it-mostly-just-used-in-academic-settings-or-are-there-industry-uses-as-well&quot;&gt;On that note, who is the end user for the library, is it mostly just used in academic settings or are there industry uses as well?&lt;&#x2F;h4&gt;
&lt;p&gt;It’s certainly intended for both. One thing I like about the AST approach is that generated code can be as fast as you can make it.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-were-the-biggest-challenges-in-developing-probabilistic-programming-for-a-new-language&quot;&gt;What were the biggest challenges in developing probabilistic programming for a new language?&lt;&#x2F;h4&gt;
&lt;p&gt;There’s always some overhead in learning a new programming language. Julia has a very Python-like syntax, so learning the basics was very fast. But metaprogramming requires different ways of thinking about things, so that took a lot of spinning up.&lt;&#x2F;p&gt;
&lt;p&gt;Macros weren’t enough, we had to use Julia’s &lt;code&gt;@generated&lt;&#x2F;code&gt; functions, which let you do staged programming. Even with this, the types weren’t quite working out, so I was using &lt;code&gt;eval&lt;&#x2F;code&gt; all over the place, which does evaluation in global scope and can cause some problems.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thautwarm.github.io&#x2F;Site-32&#x2F;index.html&quot;&gt;Taine Zhao&lt;&#x2F;a&gt; got us out of the rut with some great Julia packages like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thautwarm&#x2F;GeneralizedGenerated.jl&quot;&gt;GeneralizedGenerated.jl&lt;&#x2F;a&gt;. Generated functions compile new code for each new type they’re evaluated on, so she realized the model’s type could contain a representation of the entire model. It’s a clever solution, and helped a lot of other parts of the design to fall into place.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;there-seems-to-be-an-explosion-in-probabilistic-programming-on-julia-with-other-libraries-like-turing-or-gen-how-does-soss-compare-to-them&quot;&gt;There seems to be an explosion in probabilistic programming on Julia with other libraries like Turing or Gen, how does Soss compare to them?&lt;&#x2F;h4&gt;
&lt;p&gt;I’d say the syntax is closer to Turing, but the semantics are closer to Gen.&lt;&#x2F;p&gt;
&lt;p&gt;The Gen team independently came up with the same approach we’re using of representing a model as a function. In most PPLs, the model includes some indication of which data will later be observed. But leaving this out until inference time makes it much easier to compose models in different ways.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;any-books-you-recommend-reading-on-the-topic-besides-the-classics-statistical-rethinking-and-bayesian-methods-for-hackers&quot;&gt;Any books you recommend reading on the topic, besides the classics Statistical Rethinking and Bayesian Methods for Hackers?&lt;&#x2F;h4&gt;
&lt;p&gt;Both of these are great. If you’re interested in a particular system, most of the well-funded ones have a nice collection of examples and tutorials; walking through those usually helps.&lt;&#x2F;p&gt;
&lt;p&gt;If you want a broader and deeper view, I’d suggest digging into Bayesian analysis directly. One of my favorites is David MacKay’s &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.inference.org.uk&#x2F;mackay&#x2F;itila&#x2F;&quot;&gt;Information Theory, Inference, and Learning Algorithms&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-s-next-for-soss&quot;&gt;What’s next for Soss?&lt;&#x2F;h4&gt;
&lt;p&gt;There’s always more to do. Currently we’re starting work to make the documentation better. I think we need lots more examples, tutorials, and comparisons to other systems.&lt;&#x2F;p&gt;
&lt;p&gt;If you have any questions about Soss, the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;discourse.julialang.org&#x2F;&quot;&gt;Julia Discourse&lt;&#x2F;a&gt; or &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;julialang.zulipchat.com&quot;&gt;Zulip&lt;&#x2F;a&gt; are both great. And of course, there’s always GitHub issues for &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;cscherrer&#x2F;Soss.jl&quot;&gt;the Soss repo&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</description>
      </item>
    </channel>
</rss>
