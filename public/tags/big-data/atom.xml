<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>LambdaClass Blog - Big Data</title>
    <subtitle>Deep technical insights on cryptography, distributed systems, zero-knowledge proofs, and cutting-edge software engineering from the LambdaClass team.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://blog.lambdaclass.com/tags/big-data/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2019-10-22T00:00:00+00:00</updated>
    <id>https://blog.lambdaclass.com/tags/big-data/atom.xml</id>
    <entry xml:lang="en">
        <title>Interview with Noria’s creator: a promising dataflow research database implemented in Rust</title>
        <published>2019-10-22T00:00:00+00:00</published>
        <updated>2019-10-22T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/interview-with-norias-creator-a-promising-dataflow-database-implemented-in-rust/"/>
        <id>https://blog.lambdaclass.com/posts/interview-with-norias-creator-a-promising-dataflow-database-implemented-in-rust/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/interview-with-norias-creator-a-promising-dataflow-database-implemented-in-rust/">&lt;p&gt;After reading &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;paper&#x2F;whitepaper2015.pdf&quot;&gt;Tensorflow’s original paper&lt;&#x2F;a&gt; I learnt that four of its authors were authors of Microsoft &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;sigops.org&#x2F;s&#x2F;conferences&#x2F;sosp&#x2F;2013&#x2F;papers&#x2F;p439-murray.pdf&quot;&gt;Naiad’s research paper&lt;&#x2F;a&gt; too. Naiad influenced many systems like Tensorflow.&lt;&#x2F;p&gt;
&lt;p&gt;The Naiad paper is really interesting since it brings together many computation patterns: batch computation, streaming computation, and graph computation. It seemed to be a higher level abstraction than the traditional MapReduce and at the same time a more elegant &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lambda_architecture&quot;&gt;Lambda architecture&lt;&#x2F;a&gt; that combines batch processing with streaming methods. Being an practitioner and not a researcher this paper helped me to understand to compare different projects like Hadoop, Spark, Storm, Samza, Flink, Kafka streams.&lt;&#x2F;p&gt;
&lt;p&gt;Naiad’s paper introduced a computational model called timely dataflow that has influenced other systems like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;mit-pdos&#x2F;noria&quot;&gt;Noria&lt;&#x2F;a&gt;. Noria is a fast storage backend for read-heavy web applications implemented in Rust with a MySQL adapter. One of its creators is Jon Gjengset, a PhD student in the Parallel and Distributed Operating Systems group at MIT. Jon has a great &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thesquareplanet.com&#x2F;blog&#x2F;&quot;&gt;blog&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UC_iD0xppBwwsrM9DegC5cQQ&quot;&gt;youtube channel&lt;&#x2F;a&gt; where he discusses everything from distributed algorithms to how to implement a ZooKeeper clone. He is into many subjects I love: Rust, distributed systems, databases and the relationship between Noria, Naiad and Tensorflow were enough reasons to interview him. We did not discuss Tensorflow but I hope to find out more about the relationship between it in the future.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;What is Noria?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Noria is a dynamic dataflow database that supports partial and incremental materialized views. To break that down a bit more, it is a database that is implemented using a streaming dataflow engine that can be changed on the fly (that’s the dynamic part). It supports pre-computing the results of queries (materialized views), and updates those materialized results as the data is updated (view maintenance).&lt;&#x2F;p&gt;
&lt;p&gt;When this happens, the results are updated in-place rather than recomputed wholesale (i.e., the maintenance is incremental). When queries have parameters (e.g., &lt;em&gt;foo = ?&lt;&#x2F;em&gt;), it supports materializing the results for only some value of &lt;em&gt;foo&lt;&#x2F;em&gt; , and will compute results for “missing” values only when they are required (i.e., the materializations are partial).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How does it compare to other software like a relational database, in memory key-value stores, map reduce systems like Spark, stream processing like Storm or timely data flow?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The answer here requires some nuance, so please bear with me.&lt;&#x2F;p&gt;
&lt;p&gt;Noria is very similar to a relation database on the outside. It has tables and SQL queries, and even supports the MySQL binary protocol. You interact with Noria through prepared statements, &lt;em&gt;SELECT&lt;&#x2F;em&gt; s, &lt;em&gt;INSERT&lt;&#x2F;em&gt; s, and &lt;em&gt;UPDATE&lt;&#x2F;em&gt; s. Internally though, it is quite different. Whereas a traditional database executes a query when it receives a &lt;em&gt;SELECT&lt;&#x2F;em&gt; , Noria generally executes &lt;strong&gt;all&lt;&#x2F;strong&gt; queries when data the query’s result depends on changes. In the steady state of the system, we expect queries to be executed on &lt;strong&gt;write&lt;&#x2F;strong&gt; , not on &lt;strong&gt;read&lt;&#x2F;strong&gt;. There are some smarts required here to not do undue work. For example, Noria only computes and maintains results for query parameters the application cares about (this is the “partial materialization” piece). Noria also executes queries “incrementally”; if you add a new vote to an article with a million votes, it knows to increase the count by one, rather than count a million and one things all over again.&lt;&#x2F;p&gt;
&lt;p&gt;While Noria implements a key-value store internally to maintain and serve its materialized results, it does not seem like a key-value store to users of the system. Application authors write full SQL queries, and get structured results (rows of columns) back, just like with a normal database. The only sense in which Noria is like a key-value store is in its performance — query results will generally be fetched about as fast as a key-value store lookup.&lt;&#x2F;p&gt;
&lt;p&gt;Streaming data-flow systems like Spark, Storm, Kafka, and timely dataflow share many similarities with Noria. They process data in the same streaming fashion, and have a similar distributed system design that relies on sharding and operator partitioning. Noria differs from these systems in a few principal ways though. First, users of Noria can change the running dataflow at any time without downtime. If a new SQL query is issued that the system has not seen before, it adapts the running dataflow on the fly to incorporate the operators from the new query. The adaptation also knows to re-use existing operators where possible to produce an overall more efficient dataflow than what you would get if you just ran each query as its own dataflow program.&lt;br &#x2F;&gt;
Second, Noria supports partial materialization. Existing dataflow systems that support materialization are usually either windowed (i.e., they only reflect “recent” updates) or fully materialized (i.e., all results are always stored and maintained). Neither of these would work for web applications. When you issue a query, you expect to get all the results for &lt;strong&gt;that&lt;&#x2F;strong&gt; query (so no windowing), but you also don’t expect the system to waste resources on results that your application does not care around. You can think of this latter requirement as “you need the ability to evict from your cache”. And finally, Noria has a familiar interface for its queries: you issue SQL queries, and the system automatically translates them into efficient dataflow and adapts the running system to them. The other systems do not generally support this.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The Noria paper states that it can scale to 5x higher load than a hand optimised MySQL database. How does it manage to do that?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The primary reason for this is Noria’s view materialization. When you issue a query to MySQL, the system has to &lt;strong&gt;execute&lt;&#x2F;strong&gt; that query to produce the query’s results. In Noria on the other hand, an application query effectively turns into a hashmap lookup in the common case, which is &lt;strong&gt;very&lt;&#x2F;strong&gt; fast. Noria’s reads can also happen entirely in parallel, with very little synchronization overhead, whereas MySQL includes a lot of machinery to support full-fledged transactions (which Noria does not support). Part of the trick here is Noria’s use of a neat little datastructure called an evmap (for “eventual map”; named for its support for eventual consistency). It allows reads and writes to proceed entirely in parallel with very little overhead by having reads and writes go to different hashmaps, and then occasionally atomically switching between them.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What is the relationship between&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;jon.tsp.io&#x2F;papers&#x2F;osdi18-noria.pdf&quot;&gt;&lt;strong&gt;Noria’s paper&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;strong&gt;, the&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;timelydataflow&#x2F;differential-dataflow&#x2F;blob&#x2F;master&#x2F;differentialdataflow.pdf&quot;&gt;&lt;strong&gt;Differential dataflow&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;strong&gt;paper and&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;sigops.org&#x2F;s&#x2F;conferences&#x2F;sosp&#x2F;2013&#x2F;papers&#x2F;p439-murray.pdf&quot;&gt;&lt;strong&gt;Naiad: a timely dataflow system&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;strong&gt;?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Differential dataflow, timely dataflow, and its implementation in Naiad is one of the hallmark systems in the dataflow community. Most papers in this area relate to timely dataflow in one way or another. In the case of Noria, we also implement an incremental dataflow model, but we are trying to solve for a different use-case, and therefore arrive at different solutions.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, timely established a model for incrementally executing dataflow programs that include iteration and cycles with strong guarantees about consistency through a sophisticated timestamp tracking scheme. Noria does not support iteration or cycles, and is eventually consistent.&lt;&#x2F;p&gt;
&lt;p&gt;Instead, we provide high-performance materialized views for fast reads, partial state so only the working set needs to be kept in memory, automatic multi-query optimization, support for dynamically modifying the dataflow as it runs, and of course SQL support. Timely does not support these features, though you could likely manually implement some of them on top of timely’s core given enough time and research effort.&lt;&#x2F;p&gt;
&lt;p&gt;Overall, I don’t think it’s fair to say one system is &lt;strong&gt;better&lt;&#x2F;strong&gt; than another. In many ways they complement each other. Timely largely targets arbitrary batch computations over a large, interconnected dataset where reads are less frequent than just observing the “output” of the computation. And it is &lt;strong&gt;very&lt;&#x2F;strong&gt; good at that. Noria targets read-heavy applications where repeated and similar queries are common, and where the queries change over time. And it is &lt;strong&gt;very&lt;&#x2F;strong&gt; good at that.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Do you implement the full SQL language?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It’s not clear what “full SQL” even really means, with all of the various extensions to the language that have been added to different implementations over time. Even if you restrict yourself to ANSI SQL though, the answer for Noria is no, although mostly for uninteresting reasons. Noria is a research prototype, and as such we have focused on the features that required active research to implement in Noria’s database model. Many of the remaining features we believe could be added with sufficient engineering effort, but without too much technical difficulty. To give some examples of things we don’t support: joins whose join condition is not a single column equality; &lt;em&gt;ORDER BY&lt;&#x2F;em&gt; without a limit; &lt;em&gt;CASE&lt;&#x2F;em&gt; statements; &lt;em&gt;LIKE&lt;&#x2F;em&gt; conditions; and of course the &lt;em&gt;SOUNDEX&lt;&#x2F;em&gt; operator. There are also patterns that we support, but that we believe could be optimized further, such as multi-way joins and multiple aggregations in a single query.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why did you choose to use RocksDB to persist the data?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This was a more or less arbitrary decision. We initially wrote all base table writes directly to disk as a log, but quickly realized we needed to also keep indices over that on-disk data, otherwise recovery would be far too slow. We looked for an off-the-shelf solution, and RocksDB seemed to fit the bill. The interface for this base storage layer is pretty straightforward, and it should not be too difficult to slot in another solution there. The biggest challenge in doing so is maintaining some invariants around what writes are visible when for the purposes of Noria’s upqueries, but we believe these are solveable without too much trouble.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why does Noria needs to have Zookeeper running? Why did you choose ZooKeeper over etcd or consul?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Zookeeper serves two purposes in Noria at the moment. Service discovery and leadership change. And in fact, if you run a Noria server and client in a single process, you don’t actually need Zookeeper running at all.&lt;&#x2F;p&gt;
&lt;p&gt;If you run a single Noria worker, and a separate client, Zookeeper is only used for the client to have an easy way to discover the location of the server. We’re considering adding a non-distributed mode to Noria which supports this single-worker use-case without Zookeeper. The bits are already in place (take a look at the &lt;em&gt;Authority&lt;&#x2F;em&gt; trait in the code if you are curious), it just hasn’t been a priority for us to fix. If you are running &lt;strong&gt;multiple&lt;&#x2F;strong&gt; Noria workers, then they need some way to agree on which worker is responsible for driving application-issued changes to the dataflow, and that is where Zookeeper’s consensus comes into play. We needed a system that allowed the workers to agree on who that should be, with a mechanism for failover, and Zookeeper provided that. The API we need from Zookeeper is very limited, and it should be straightforward to slot in another consensus provider in its place.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Is Noria production ready? Do you know anybody using it?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Noria is most definitely still a research prototype, though I think the thing standing between where it is now and a production-ready version is mostly just engineering effort. We are a small team of researchers working on it, and we focus our efforts on the aspects of the system that are related to our ongoing research. There is relatively little room for spending lots of time on doing “production engineering” in the academic setting :)&lt;&#x2F;p&gt;
&lt;p&gt;That said, I know of several large companies who are very interested in&lt;br &#x2F;&gt;
using Noria in a production setting, and many of them have gotten in touch with me about what might be required to achieve that. I also know that multiple companies are trying Noria out privately internally to test its viability as a replacement for certain parts of their stack. What ultimately comes of that is unclear at the moment, but I of course hope that they find Noria promising, and that they are willing to invest time into making it production ready!&lt;&#x2F;p&gt;
&lt;p&gt;There are a few features missing from Noria that I think are the primary blockers from using it in production. The first is checkpointing of materialized views. Currently, if the system is turned off and then restarted, all the materialized views are empty. This would be equivalent to a full cache purge. An external system could heat the cache, but it’d be better if Noria internally kept some form of snapshot to aid in this process. The second is fault tolerance — when Noria is run in a distributed setting, a machine failure results in related parts of the dataflow being blown away entirely and restarted. This is obviously problematic in production settings. We are actively pursuing research in this area, and have some ideas for how to fix it, but it is a complex subject. And finally, Noria currently requires ownership of its base storage. If you have an existing data store that you’d like to keep using, you’d have to feed changes to that data to Noria, and Noria would store it a second time. Changing Noria such that it can handle the base storage being managed by a different system is possible, though would require some careful engineering with respect to consistency of upqueries.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;When would you avoid using Noria?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Noria is not great for systems that do not have a well-defined working set. If your application is constantly issuing entirely new queries over your data that do not relate to previous queries, or if it rarely queries by the same set of keys more than once, then Noria’s materializations will be mostly useless and just add unnecessary overhead. Noria is also primarily built for read-heavy applications; if your application rarely does reads, but sustains a &lt;strong&gt;very&lt;&#x2F;strong&gt; high write load, then Noria in its current form is probably not what you want. I say “probably” because Noria already supports a fairly high write throughput, and if your inputs fall below that threshold, Noria will still work fine. And, crucially, its materializations will make your reads over this quickly growing collection really fast!&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why did you choose to use Rust to implement it?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The answer to this is perhaps less interesting than you’d expect. When&lt;br &#x2F;&gt;
we first started this iteration of the project in mid-2016, Rust was just starting to appear as a viable systems language. We were looking for a systems language to use for this new project, and did not particularly want to write C++, and wanted to explore something beyond Go. We heard about Rust, its claims were enticing, so as researchers we figured we’d try it out and see if it could live up to its promises. The cost of failure was low, as we could always start over, so we just ran with it. And now we’re 80k lines of code in, and I still think it was the right choice.&lt;&#x2F;p&gt;
&lt;p&gt;I can also give a post-hoc analysis of the journey. I think choosing Rust has worked out great for us; specifically, it has saved us countless hours of debugging. We write a lot of concurrent code in Noria, and the Rust compiler has caught a ridiculous number of concurrency bugs that would just have slipped right by in another language. Debugging those in a distributed context in a research system where we don’t even know if the underlying &lt;strong&gt;algorithm&lt;&#x2F;strong&gt; is sound would have been a major pain (and still is when it happens). Rust has also allowed us to write low-level code when we needed to squeeze out those last bits of performance in the core pieces of Noria. What is more, I have found the Rust ecosystem to be a joy to participate in and to rely on; solid libraries like tokio have allowed us to focus on the core research-y parts of the application, and lots of knowledgeable Rustaceans have helped us when we’ve run into weird issues. Not only that, but we have been able to contribute back to that ecosystem by publishing libraries of our own and by contributing back to the compiler and other core libraries that we relied on.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;You have videos on how to implement a TCP stack, a minimal Zookeeper implementation and blog posts explaining how to implement Raft. What would gain a traditional full stack developer learning how to implement low level structures, distributed algorithms or distributed software?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I think much of this comes down to curiosity. If you’re curious about how stuff works, there’s an endless number of rabbit holes you can easily start down that teach you all of this stuff. For me, much of my distributed systems knowledge came from MIT’s 6.824 Distributed Systems class, which is &lt;strong&gt;excellent&lt;&#x2F;strong&gt;. All of their reading materials, lecture notes, and labs are also available online. For algorithms, the easiest way to get started in thinking about them is to read some of the early papers describing these algorithms, and then trying to implement them yourself! You’ll find that many of them aren’t as difficult to build as you may think, and you will learn a lot in the process. Including how tobread academic papers! I can also recommend trying to follow some low-level OS-building resources. For example, Philipp Oppermann has a great blog series on implementing an operating system from scratch in Rust that goes through all of the low-level details you’ll need to know.&lt;&#x2F;p&gt;
&lt;p&gt;Alternatively, all the components of MIT’s 6.828 Operating Systems class are also available online.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What papers, readings and exercises do you recommend doing to learn about distributed programming?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I would highly recommend following the 6.824 class schedule&lt;br &#x2F;&gt;
(&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;pdos.csail.mit.edu&#x2F;6.824&#x2F;schedule.html&quot;&gt;https:&#x2F;&#x2F;pdos.csail.mit.edu&#x2F;6.824&#x2F;schedule.html&lt;&#x2F;a&gt;). It covers both classic&lt;br &#x2F;&gt;
papers, established approaches, and new research in the area. Do the&lt;br &#x2F;&gt;
labs as well; they will force you to get &lt;strong&gt;intimately&lt;&#x2F;strong&gt; familiar with many&lt;br &#x2F;&gt;
subtle distributed systems problems!&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Interview with Jay Kreps about Apache Kafka</title>
        <published>2016-02-22T00:00:00+00:00</published>
        <updated>2016-02-22T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/interview-with-jay-kreps-about-apache-kafka/"/>
        <id>https://blog.lambdaclass.com/posts/interview-with-jay-kreps-about-apache-kafka/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/interview-with-jay-kreps-about-apache-kafka/">&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;1-lRRJqrarJFi5TPtnBm_8hA.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This time we interviewed &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;jaykreps&quot;&gt;Jay Kreps&lt;&#x2F;a&gt;, one of the creators of Apache Kafka. Kafka is an open source messaging system with a few design choices that make it particulary useful for high throughput and low latency scenarios.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;“This experience lead me to focus on building Kafka to combine what we had seen in messaging systems with the log concept popular in databases and distributed system internals. We wanted something to act as a central pipeline first for all activity data, and eventually for many other uses, including data deployment out of Hadoop, monitoring data, etc.” — Jay Kreps&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Kafka is built around the concept of a distributed database commit log. If you have no idea what that is, then I highly recommend that after you finish reading the interview you check the links I have pasted at the end. I learnt a lot by reading them.&lt;&#x2F;p&gt;
&lt;p&gt;In the following weeks I am going to publish an interview with &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;martinkl&quot;&gt;Martin Kleppmann&lt;&#x2F;a&gt;, one of the authors of Samza, about his book Data Intensive Applications and realtime stream processing systems vs batch processing systems.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;What problem does Kafka solve?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Kafka is a distributed storage system for data streams. It allows you to publish streams of data and subscribe to them. It is built around the concept of a persistent log that is appended to — publishers of data append to this log and consumers subscribe to changes. Perhaps most importantly, it scales really well so it can function as a central hub for these data streams even in a company with a lot of data like LinkedIn or Netflix or Uber.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why RabbitMQ, ActiveMQ and other similar open source projects where not useful to solve this problem?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;There are a few things that are different about Kafka:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;It is built from the ground up as a modern distributed system. It handles replication, fault-tolerance, and partitioning. You think about Kafka as a cluster, not a collection of individual brokers. This impacts everything from how you manage it to how programs behave.&lt;&#x2F;li&gt;
&lt;li&gt;Kafka does a good job of persistence. Data in Kafka is always persisted and can be re-read.&lt;&#x2F;li&gt;
&lt;li&gt;Kafka is faster than traditional messaging system and hence more suitable to really large volume data streams such as would come from logging use cases, or massive streams of sensor data.&lt;&#x2F;li&gt;
&lt;li&gt;Kafka was designed to support distributed stream processing as a layer on top of its core primitives. This is why Kafka is so commonly used with things like Spark Streaming or Storm.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;In what type of structure do you persist messages and in which format?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;A message or record in Kafka is just a key-value pair, where the key and value are some string of bytes.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;max&#x2F;2000&#x2F;0-4N-FW2mHbx6AsraV.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Kafka provides the abstraction of a “topic” which is split into one or more partitions (usually many) and spread over a cluster of nodes. A topic is a kind of feed of records. Applications publish records into a topic, and the record’s key determines the partition within that topic that the record goes to. Each partition is replicated on multiple machines for fault-tolerance.&lt;&#x2F;p&gt;
&lt;p&gt;The core abstraction Kafka provides (as well as the data structure it uses in its implementation) is a write ahead log. This log is just an ordered sequence of the records written to the cluster that is persisted to disk. Each record is assigned a sequential number called an offset. This offset acts as a position in the log.&lt;&#x2F;p&gt;
&lt;p&gt;An application consuming that partition can be thought of as having a position in the log designated by that offset, which means it has consumed all the records earlier and none of the records after. The application controls that position and can continue to read forward or go back in time to re-read.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How does Kafka manage to handle easily many dozens of thousand of messages per seconds if it persists them to disk instead of keeping them in memory?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Careful design! Our observation was that there was no fundamental reason that the log abstraction we wanted couldn’t be as fast as the underlying filesystem at linear writes, which means anything from hundreds of MB&#x2F;sec on spinning disks to GBs&#x2F;sec on SSDs. To make this happen Kafka does a good job of batching together lots of small writes into big linear appends to files. This batching happens both in the consumer, in the replication protocol, in the consumer, and in the operating system itself.&lt;&#x2F;p&gt;
&lt;p&gt;I do think the domain of infrastructure engineering is different in this way. Application developers are warned against the dangers of premature optimization, but for infrastructure I think you need to start thinking about performance in the design phase. The reason it is so different is that the fundamental constraints are well known ahead of time and usually system designs are not very flexible, so if you ignore performance initially it is generally very hard to get it back later by optimizing within your existing design.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What guarantees does Kafka provides? In which cases can messages be lost?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Kafka guarantees that writes are replicated across N instances in the same order (where N is a replication factor you choose) and that your write won’t be lost as long as at least one of these instance remains alive.&lt;&#x2F;p&gt;
&lt;p&gt;In combination with the way consumers control their own offset this translates to an “at-least-once” delivery.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;In&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;9RMOc0SwRro&quot;&gt;&lt;strong&gt;this talk&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;strong&gt;you mentioned Kafka streams. Could you briefly explain what is it and why it will be useful?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Kafka Streams is a stream processing layer for Kafka we’ve been working on. It’s a little different from the existing stream processing frameworks that are out there — more focused on building streaming applications and less a kind of real-time version of MapReduce.&lt;&#x2F;p&gt;
&lt;p&gt;We’ll be doing a preview release in early March (we’re really excited).&lt;&#x2F;p&gt;
&lt;p&gt;Combined with the work we did on &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.confluent.io&#x2F;blog&#x2F;announcing-kafka-connect-building-large-scale-low-latency-data-pipelines&quot;&gt;Kafka Connect&lt;&#x2F;a&gt;, we think this makes Kafka a really compelling &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.confluent.io&#x2F;blog&#x2F;stream-data-platform-1&#x2F;&quot;&gt;platform for streaming data&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What needs to be done before releasing Kafka 1.0?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We’ll get there. We thought we needed to at least get a stable version of Connect and Streams done as they are a pretty essential part of the platform.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why did you choose Java to implement Kafka? Did you ever consider using another programming language?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We had a lot of experience with the JVM and knew it was possible to build reliable and fast infrastructure on top of it — and it was more convenient to work with than C or C++.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;I work as an Erlang developer and I was thrilled by your comments about concurrency and languages in&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;blog.empathybox.com&#x2F;post&#x2F;90318905473&#x2F;concurrency-is-not-a-language-thing-anymore&quot;&gt;&lt;strong&gt;“Concurrency is not a language thing anymore”&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;strong&gt;:&lt;&#x2F;strong&gt;&lt;br &#x2F;&gt;
&lt;em&gt;“In the near-real-time processing domain stream processing frameworks do a good job of providing asynchronous processing without directly thinking about concurrency at all. Again you interact with concurrency and parallelism only at the framework level, the code you write appears completely single-threaded.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;The offline world seems to be moving in the direction of a handful of YARN frameworks for different specialized purposes. What almost all of these share is that they don’t require users to directly manage concurrency.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;This leads me to think that putting more time into language support for single-server concurrency (software transactional memory and all that) is of limited utility. It will only help the implementors of these frameworks, not the end user.”&lt;&#x2F;em&gt;&lt;br &#x2F;&gt;
&lt;strong&gt;Apart from Erlang, some languages like Go and Clojure added a good concurrency model and semantics from the start. Don’t you think there is any area where having good concurrency baked into the language is useful for the normal developer and not only for the implementor of frameworks?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The critique I was trying to make is sort of analogous to the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;End-to-end_principle&quot;&gt;end-to-end principle&lt;&#x2F;a&gt; for network protocols, basically you end up needing to solve the concurrency problem at a higher level anyway which makes the lower-level primitive the languages provide redundant. What I see is each language is trying to provide built-in primitives for multi-core programming. Other than Erlang I think most of these ignore the problem of distributed computing. But what has changed is that modern programming is always done in some framework that introduces a concurrency model at a higher level. Examples of these frameworks would be the whole Apple and Android stacks, numerous microservice frameworks, and things like Spark or Kafka Streams. These higher level frameworks are able to do a better job because they can make assumptions about the environment that just aren’t possible at the language level. So, for example, many of them are able to introduce a model that simultaneously solves for spreading computation over CPU cores on one machine as well as over multiple machines.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why does Kafka depends on Zookeeper? What job does Zookeeper do for Kafka?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.confluent.io&#x2F;blog&#x2F;distributed-consensus-reloaded-apache-zookeeper-and-replication-in-kafka&quot;&gt;This article&lt;&#x2F;a&gt; gives an overview of its role in Kafka’s replication design.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Do you have any recommendation for those of us who want to start learning about distributed systems? Was there any books, papers or codebase that really helped you implement and design Kafka?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I think a great place to start is Martin Kleppman’s book &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;shop.oreilly.com&#x2F;product&#x2F;0636920032175.do?cmp=af-strata-books-videos-product_cj_9781491903094_%25zp&quot;&gt;Designing Data Intensive Applications&lt;&#x2F;a&gt;. I have only read parts of it, but from what I’ve seen it is the best accessible introduction to distributed systems out there. Unfortunately only 9 of 12 chapters are available so we should all bug him to finish it!&lt;&#x2F;p&gt;
&lt;p&gt;A good textbook you &lt;em&gt;can&lt;&#x2F;em&gt; buy today is &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.amazon.com&#x2F;Introduction-Reliable-Secure-Distributed-Programming&#x2F;dp&#x2F;3642152597&quot;&gt;Introduction to Reliable and Secure Distributed Programming&lt;&#x2F;a&gt;. This book isn’t great for learning but it is an order of magnitude better than other text books which are utterly terrible. Unfortunately distributed systems research had several decades in which it wasn’t really very practical and hence it developed a culture that seems to pride itself on it’s lack of connection to mainstream practice. For example, that book, manages to spend on the order of a hundred pages introducing different possible communication primitives and talking about their properties without bothering to connect any of these to the actual mainstream network protocols like UDP and TCP which seems pretty silly to me.&lt;&#x2F;p&gt;
&lt;p&gt;The best thing is that these days there are hundreds of open source distributed systems available and you can learn quite a lot about the design and implementation of these.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;As I mentioned in the introduction to the interview, I highly recomend that you read the following three links about Kafka, its design and uses:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;engineering.linkedin.com&#x2F;distributed-systems&#x2F;log-what-every-software-engineer-should-know-about-real-time-datas-unifying&quot;&gt;The Log: What every software engineer should know about real-time data’s unifying abstractionI joined LinkedIn about six years ago at a particularly interesting time. We were just beginning to run up against the…engineering.linkedin.com&lt;img src=&quot;&#x2F;images&#x2F;fit&#x2F;c&#x2F;160&#x2F;160&#x2F;0-EKXwv5gzf95qmyw9.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.confluent.io&#x2F;blog&#x2F;stream-data-platform-1&#x2F;&quot;&gt;Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 1)These days you hear a lot about “stream processing”, “event data”, and “real-time”, often related to technologies like…www.confluent.io&lt;img src=&quot;&#x2F;images&#x2F;fit&#x2F;c&#x2F;160&#x2F;160&#x2F;0-wDG3xKC21IQ4qaC7.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.confluent.io&#x2F;blog&#x2F;stream-data-platform-2&#x2F;&quot;&gt;Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 2)This is the second part of our guide on streaming data and Apache Kafka. In part one I talked about the uses for real…www.confluent.io&lt;img src=&quot;&#x2F;images&#x2F;fit&#x2F;c&#x2F;160&#x2F;160&#x2F;0-D5SxPsgpumFDy5py.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;kafka.apache.org&#x2F;documentation.html#design&quot;&gt;Apache KafkaKafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system…kafka.apache.org&lt;img src=&quot;&#x2F;images&#x2F;fit&#x2F;c&#x2F;160&#x2F;160&#x2F;0-5a9GIkEZZHsR80Z0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Jay also gave a few excellent talks about Kafka that explain why it was created and what are its uses:&lt;&#x2F;p&gt;
&lt;h4 id=&quot;&quot;&gt;&lt;&#x2F;h4&gt;
</content>
        
    </entry>
</feed>
