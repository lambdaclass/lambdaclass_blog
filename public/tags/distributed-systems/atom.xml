<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>LambdaClass Blog - Distributed Systems</title>
    <subtitle>Deep technical insights on cryptography, distributed systems, zero-knowledge proofs, and cutting-edge software engineering from the LambdaClass team.</subtitle>
    <link rel="self" type="application/atom+xml" href="https://blog.lambdaclass.com/tags/distributed-systems/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-04-09T00:00:00+00:00</updated>
    <id>https://blog.lambdaclass.com/tags/distributed-systems/atom.xml</id>
    <entry xml:lang="en">
        <title>The Wisdom of Iroh</title>
        <published>2025-04-09T00:00:00+00:00</published>
        <updated>2025-04-09T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/the-wisdom-of-iroh/"/>
        <id>https://blog.lambdaclass.com/posts/the-wisdom-of-iroh/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/the-wisdom-of-iroh/">&lt;p&gt;As we‚Äôve written before, most of us at Lambda are internet natives. The formative experiences that made us who we are include meeting people on the other side of the world through IRC, sharing knowledge, media, and code via BitTorrent, wikipedia, and software version control systems, the birth of the first search engines, and the feeling that &lt;em&gt;everything&lt;&#x2F;em&gt;  was accessible. We then grew up and found frustration that this experience did not yet extend to the financial tasks needed to be an adult, and that terms like &lt;em&gt;walled garden&lt;&#x2F;em&gt;  better described the new state of our internet home.&lt;&#x2F;p&gt;
&lt;p&gt;This is why we get a double high when learning about projects like Iroh: an emotional tug from a project that enables building distributed systems in a way that gives users more agency, and a nerdy thrill from the technical challenges they‚Äôve solved to achieve it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-is-it&quot;&gt;What is it?&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;&quot;&gt;Iroh&lt;&#x2F;a&gt; is a distributed systems toolkit, focused on easily setting up reliable p2p connections. It includes facilities for establishing direct connections, moving data, syncing state, and pluggable application-level protocols. It‚Äôs working in production and has managed 200k concurrent connections and millions of devices on the same network with low service costs.&lt;&#x2F;p&gt;
&lt;p&gt;In their own words:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Iroh is a library for establishing the most direct QUIC connection possible between two devices. Every &lt;em&gt;endpoint&lt;&#x2F;em&gt;  uses the public half of a cryptographic keypair to identify itself. Assuming at least one configured &lt;em&gt;relay server&lt;&#x2F;em&gt;  is reachable, an endpoint keeps exactly one TCP connection to a ‚Äúhome relay‚Äù that other nodes use for connection establishment, and as a fallback transport. Iroh uses a suite of &lt;em&gt;discovery services&lt;&#x2F;em&gt;  to resolve home relays &amp;amp; endpoint IDs. Connections between endpoints use QUIC ALPNs to distinguish between &lt;em&gt;protocols&lt;&#x2F;em&gt; , while &lt;em&gt;routers&lt;&#x2F;em&gt;  automate the endpoint accept loop for protocol multiplexing.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;One of the things we like about Iroh is that it is clear on what it is about. It runs on QUIC, started out as a new implementation of IPFS, went through several iterations, and reduced its scope to better solve the problems they were facing. They wrote about this process in their &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;blog&#x2F;smaller-is-better&quot;&gt;Smaller is Better&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;blog&#x2F;road-to-1-0&quot;&gt;Roadmap&lt;&#x2F;a&gt; posts, and we fully agree that this is good engineering practice.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-can-iroh-be-used-for&quot;&gt;What can Iroh be used for?&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;n0.computer&#x2F;&quot;&gt;&lt;code&gt;n0&lt;&#x2F;code&gt;&lt;&#x2F;a&gt;, the company behind Iroh, keeps a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;n0-computer&#x2F;awesome-iroh&quot;&gt;list&lt;&#x2F;a&gt; of projects building on them but to get a quick idea, it can be of use in anything that needs file sync, p2p game streaming, distributed object storage, peer discoverability and swarm membership, local-first design, or compute job orchestration.&lt;&#x2F;p&gt;
&lt;p&gt;One of our partners, &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;nousresearch&quot;&gt;Nous Research&lt;&#x2F;a&gt; is using it in a decentralized program which relies on iroh to manage communications between nodes training LLMs, sending messages between the clients to advance the state of the network and share the gradients calculated by each node.&lt;&#x2F;p&gt;
&lt;p&gt;Today, we interviewed the team to get some insight.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-many-of-the-n0-team-members-are-ex-ipfs-or-libp2p-developers-one-of-the-first-questions-asked-is-how-iroh-compares-to-libp2p-and-as-we-understand-it-the-answer-is-related-to-having-a-tighter-focus-keeping-the-core-about-making-p2p-connections-that-just-work-and-moving-the-rest-to-application-level-protocols-such-as-iroh-gossip-blobs-and-docs-that-can-be-mixed-and-matched-as-desired-can-you-elaborate-on-this-process-and-how-reducing-scope-helped&quot;&gt;&lt;em&gt;1. Many of the n0 team members are ex-IPFS or libp2p developers. One of the first questions asked is how Iroh compares to libp2p and as we understand it, the answer is related to having a tighter focus, keeping the core about making p2p connections that just work, and moving the rest to application-level protocols such as iroh-gossip, -blobs and -docs that can be mixed and matched as desired. Can you elaborate on this process and how reducing scope helped?&lt;&#x2F;em&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;b5: The process was one of slowly divesting ourselves of a lot of ‚Äúp2p project baggage‚Äù. Most p2p projects end up defaulting into a boil-the-ocean stance where they try to ship one of everything: a DHT, transports, pubsub, RPC, and over time we‚Äôve come to believe this is a big contributing factor to p2p projects feeling like half-baked prototypes. It clicked for us when our CTO dig pointed out ‚Äúno one wants the nginx team to ship postgres‚Äù. A DHT is a huge undertaking, reliable sync is a huge undertaking, reliable transports are a huge undertaking. Sometime last year we realized it just wouldn‚Äôt be possible to ship all this stuff with the team we had, so we picked the transport layer, and are focused on integrating with other projects &amp;amp; the community forming near iroh for the things we can‚Äôt ship. Our bet is things will work better if a project like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;loro.dev&quot;&gt;loro&lt;&#x2F;a&gt; ships &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;loro-dev&#x2F;iroh-loro&quot;&gt;optional iroh support&lt;&#x2F;a&gt;, the loro team makes a truly robust CRDT, and we make a truly robust transport. There‚Äôs pressure on both teams to make the public APIs small &amp;amp; composable, to make integration easier.&lt;br &#x2F;&gt;
A lot of this is testament to just how incredible a technical feat &lt;code&gt;libp2p&lt;&#x2F;code&gt; is, especially when you see the sheer number of language implementations, it‚Äôs truly impressive. But that amount of work comes with a big API surface area, makes it very challenging to port all of that functionality into a robust package that works well on a phone. It also creates the expectation that &lt;code&gt;libp2p&lt;&#x2F;code&gt; maintainers commit to delivering both a robust DHT &lt;em&gt;and&lt;&#x2F;em&gt; a reliable transport. When we more focus we explicitly mean fewer features that both work more consistently &amp;amp; are integrated across organizations.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-how-did-the-decision-to-use-quic-come-about-a-few-months-ago-some-research-indicated-quic-might-have-some-downsides-and-there-seems-to-be-anecdotal-evidence-of-hostility-to-the-new-protocol-from-network-engineers-does-your-team-have-opinions-wrt-to-any-aspect-of-this-are-there-any-indications-for-iroh-adopters-that-might-stem-from-quic-usage&quot;&gt;2. How did the decision to use QUIC come about? A few months ago some &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3589334.3645323&quot;&gt;research&lt;&#x2F;a&gt; indicated QUIC might have some downsides and there seems to be anecdotal evidence of hostility to the new protocol from network engineers. Does your team have opinions wrt to any aspect of this? Are there any indications for Iroh adopters that might stem from QUIC usage?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: the goals of QUIC closely resemble what we‚Äôre trying to do with iroh: ship new capabilities on the internet &lt;em&gt;with software&lt;&#x2F;em&gt;  because changing the hardware is impractical. QUIC is trying to tackle protocol ossification that set in because routers can inspect TCP headers, and doing that by dropping down to the UDP layer &amp;amp; working from there. Along with being aligned at ‚Äúspiritual‚Äù level, things like QUIC multipath support seem almost designed for our exact use case. It‚Äôs a young technology that we‚Äôre all-in on.&lt;&#x2F;p&gt;
&lt;p&gt;I haven‚Äôt heard much in the way of hostility from network engineers, but I‚Äôm not entirely surprised. QUIC is intentionally trying to reduce the visible surface area to routers &amp;amp; internet middleboxes, which I‚Äôm sure would be frustrating. I happen to be of the mind that internet middle boxes shouldn‚Äôt be messing with those packets in the first place, but hey, that‚Äôs just me üòÑ&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-you-ve-mentioned-that-iroh-has-seen-a-million-devices-on-the-same-network-is-this-in-relation-to-the-public-iroh-relays-or-in-another-context-what-are-the-scalability-limits-you-ve-seen-and-in-which-scenarios&quot;&gt;3. You‚Äôve mentioned that Iroh has seen a million devices on the same network. Is this in relation to the public Iroh relays or in another context? What are the scalability limits you‚Äôve seen and in which scenarios?&lt;&#x2F;h3&gt;
&lt;p&gt;The biggest numbers we‚Äôve seen have come from app developers deploying iroh as part of an update to an existing app. Each of those has stressed iroh in different ways. We‚Äôve shipped against those stress tests for the last 6 months. It‚Äôs by no means done, but it is giving us in-production feedback that‚Äôs critical as we work toward our 1.0 release later this year.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-iroh-gossip-is-particularly-interesting-as-a-modern-implementation-of-hyparview-and-plumtree-what-made-you-choose-these-protocols-have-you-done-load-tests-on-this-protocol-in-particular-what-is-your-approach-to-testing-and-load-testing-in-general&quot;&gt;4. Iroh-gossip is particularly interesting as a modern implementation of HyParView and Plumtree. What made you choose these protocols? Have you done load tests on this protocol in particular? What is your approach to testing and load testing in general?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: phones. If we‚Äôre going to make p2p work on a mobile devices, ‚Äústar‚Äù topologies that compensate for high network churn with lots of connections simply aren‚Äôt viable, which makes the active&#x2F;passive divide in PlumTree particularly appealing. As I‚Äôm writing this someone in our discord is running a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;discord.com&#x2F;channels&#x2F;1161119546170687619&#x2F;1161119546644627528&#x2F;1357726363657834788&quot;&gt;2000 node iroh gossip stress test&lt;&#x2F;a&gt; using an erlang supervisor, so yes, it‚Äôs being tested! We also have a battery of smoke &amp;amp; simulation tests that run against the iroh gossip protocol as part of CI.&lt;br &#x2F;&gt;
Gossip has been getting more attention lately, which is driving us to put more time into it. Frando from our team has been actively working on stability as we speak.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-you-encourage-users-to-set-up-their-own-relays-for-their-networks-but-are-also-very-generous-with-the-three-public-ones-you-offer-aside-from-avoiding-the-rate-limits-why-use-private-relays-are-there-any-security-or-other-feature-considerations&quot;&gt;5. You encourage users to set up their own relays for their networks but are also very generous with the three public ones you offer. Aside from avoiding the rate limits, why use private relays? Are there any security or other feature considerations?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: It‚Äôs totally fine to use the public relays! Honestly, we‚Äôd love to see more use so we can stress them more :). As a gentle reminder for everyone: relay traffic is e2ee, so the relays can‚Äôt see traffic, but relays &lt;em&gt;do&lt;&#x2F;em&gt; have a list of nodeIDs, and list of connections they‚Äôre facilitating, which is privileged information. Many of our more serious users are using private relays to avoid exposing that information to the public, or even to number 0, which is things working as intended in our view. We have some plans in the works for a complimentary service that will make spinning up relays very easy. Stay tuned for that!&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-when-developing-distributed-systems-observability-becomes-a-prime-concern-iroh-doctor-seems-like-a-cool-tool-to-have-does-iroh-offer-other-facilities-for-observing-and-debugging-its-internals-or-the-application-what-role-does-iroh-metrics-play-in-this&quot;&gt;6. When developing distributed systems, observability becomes a prime concern. &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;blog&#x2F;iroh-0-16-a-better-client#iroh-doctor-plot&quot;&gt;Iroh-doctor&lt;&#x2F;a&gt; seems like a cool tool to have. Does Iroh offer other facilities for observing and debugging its internals or the application? What role does Iroh-metrics play in this?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: We‚Äôre actively working on this. Gathering actionable network metrics in a p2p system is critical as we make p2p a mature, reliable thing. We‚Äôll have way more to say on this one in the coming months.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-p2p-systems-usually-disclose-the-ip-addresses-of-the-participating-nodes-and-iroh-explicitly-chooss-to-give-applications-flexibility-in-what-if-anything-to-do-in-this-regard-what-choices-do-you-see-are-usually-taken-and-what-mechanisms-aside-from-vpns-can-applications-implement&quot;&gt;7. P2P systems usually disclose the IP addresses of the participating nodes and Iroh explicitly chooss to give applications flexibility in what (if anything) to do in this regard. What choices do you see are usually taken, and what mechanisms (aside from VPNs) can applications implement?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: I should clarify that any connection within iroh will &lt;em&gt;always&lt;&#x2F;em&gt; end up exposing your IP address to the peer that you‚Äôre dialing, and the relay server your node uses as it‚Äôs home. This is also true of &lt;em&gt;so&lt;&#x2F;em&gt; many services you use every day, so iroh isn‚Äôt new in this regard. With that said, yeah a VPN is rarely a bad idea, and we expicitly run one-off tests between n0 staff where we start a big file transfer &amp;amp; switch VPN on &amp;amp; off during transfer to confirm it works (spoiler: it does).&lt;br &#x2F;&gt;
The implications of connecting users will be different for each application, but we generally ask folks to use their heads: if your app is 5-100 person invite-only chat rooms, then it makes sense to couple iroh connections with room memberships. If your app is, say, twitter, then you might need to introduce a new opt-in mechanism that makes it clear to the user that you‚Äôre disclosing something that might be abused.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;8-the-local-first-software-movement-prioritizing-user-data-being-stored-and-processed-on-their-own-devices-rather-than-relying-on-cloud-servers-is-new-and-slowly-gaining-traction-do-you-see-iroh-being-used-in-this-context-or-are-most-of-the-main-users-focused-on-other-use-cases&quot;&gt;8. The local-first software movement (prioritizing user data being stored and processed on their own devices rather than relying on cloud servers) is new and slowly gaining traction. Do you see Iroh being used in this context or are most of the main users focused on other use cases?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: YES. we &amp;lt;3 local first in a big way, and think p2p is the only way to get to software that is both local first and networked. The thing user agency, p2p, and local first all have in common is shipping more capabilities to the end-user‚Äôs device than we traditionally get with today‚Äôs ‚Äúview layer on an API‚Äù apps.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-coupling-iroh-with-a-crdt-such-as-automerge-seems-to-be-a-common-pattern-iroh-docs-seems-geared-to-be-a-distributed-kv-store-but-is-based-on-range-based-set-reconciliation-do-you-see-these-higher-level-usage-patterns-being-codified-as-other-protocols-are-there-other-protocols-in-development-or-do-you-see-any-particular-pattern-as-a-likely-future-protocol&quot;&gt;9. Coupling Iroh with a CRDT such as &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;automerge.org&#x2F;&quot;&gt;automerge&lt;&#x2F;a&gt; seems to be a common pattern. Iroh-docs seems geared to be a distributed KV store but is based on range-based set reconciliation. Do you see these higher-level usage patterns being codified as other protocols? Are there other protocols in development, or do you see any particular pattern as a likely future protocol?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: yes, iroh + automerge is definitely ‚Äúusing iroh as intended‚Äù, and you get at a good point: there are common patterns like message bootstrapping, incremental updates, and pairwise reconciliation that are commmon across a bunch of these protocols. To be able to actually have those protocols share abstractions for these patterns we‚Äôd need a more robust story for protcol composition than we currently have, because we‚Äôd need a way for a protocol to express dependencies &amp;amp; do protocol version matching across the set of registered protocols at compilation time. Even then, it would require the buy-in from projects like automerge, which really isn‚Äôt a goal of ours right now.&lt;br &#x2F;&gt;
I think it‚Äôs going to take years, but I do think we‚Äôll get to a place where we declare a dependency graph of protocols, the compiler will be able to tell you if you have a version mismatch, and we‚Äôll be able to further decompose these patterns as a community. I‚Äôm doing some experiments in this direction on the side, but don‚Äôt expect to see anything in this department before we cut iroh 1.0.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;10-you-ve-written-about-the-challenges-of-using-async-rust-and-we-can-certainly-relate-in-our-experience-greenspun-s-tenth-rule-applies-transmuted-to-distributed-systems-sometimes-called-virding-s-rule-any-sufficiently-complicated-concurrent-program-in-another-language-contains-an-ad-hoc-informally-specified-bug-ridden-slow-implementation-of-half-of-erlang-what-is-your-experience-with-the-actor-and-message-passing-approach-both-in-rust-when-implementing-iroh-and-more-generally-when-using-iroh-to-build-systems-that-communicate&quot;&gt;10. You‚Äôve &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;blog&#x2F;async-rust-challenges-in-iroh&quot;&gt;written&lt;&#x2F;a&gt; about the challenges of using async rust and we can certainly relate! In our experience Greenspun‚Äôs tenth rule applies transmuted to distributed systems (sometimes called Virding‚Äôs rule) ‚ÄúAny sufficiently complicated concurrent program in another language contains an ad hoc informally-specified bug-ridden slow implementation of half of Erlang.‚Äù What is your experience with the actor and message passing approach, both in Rust when implementing Iroh and more generally when using Iroh to build systems that communicate?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: lol yes very much to the half-Erlang. We‚Äôre very much in that uncanny valley right now with iroh. Most of the internal guts are implemented with actors, but we haven‚Äôt formalized that into an actor abstraction, and it‚Äôs unclear that we ever will. Where that pain is felt more accutely is at the protocol level. At the level of protocol developement, it would be very nice to have easy-to-implement patterns that abstract around distibuted fault tolerance &amp;amp; give you that ‚Äúfail whenever you want‚Äù characteristic the supervisor trees bring. The protocol dev is also at the right height in the stack, dealing with logical messages instead of raw packets.&lt;br &#x2F;&gt;
We‚Äôre still working on the groundwork of getting tutorials in place for writing a protocol in the first place, but I‚Äôd love to see us spend more time cooking up recipes for protcol development atop an actor model abstraction.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;&quot;&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;11. Your &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;roadmap&quot;&gt;roadmap&lt;&#x2F;a&gt; is quite clear, webassembly support being oft-requested and recently merged, and better support on the way for clients wanting to use Iroh in browsers without having to send all data over relays. Some notable items in the more distant roadmap are a spec and FFI integrations. Can you elaborate on their importance and&#x2F;or motivation? Do you have an estimate on when 1.0 is due and any comments on what motivates the upcoming features? What are you most excited about?&lt;&#x2F;p&gt;
&lt;p&gt;b5: The spec part is fun because iroh can be pretty easily expressed as a composition of existing specs, which is our plan. In our view 1.0 means you know clearly what the thing is, and how it &lt;em&gt;should&lt;&#x2F;em&gt; behave, so why not write that down in a spec? That said, we‚Äôre far more concerned with working software than a spec, and see taking the time to write out a spec as a means of confirming we‚Äôve considered everything we need to as part of a 1.0 push, and can communicate that consideration clearly. As for FFI bindings, we &lt;em&gt;really&lt;&#x2F;em&gt; , &lt;em&gt;really&lt;&#x2F;em&gt; want to get to languages outside of rust, but have a lot of work to do here. More on FFI in the July-August time range. Current plan for 1.0 is sometime in September.&lt;br &#x2F;&gt;
As for excitement, Divma &amp;amp; Floris on our team have been hard at work on support for QUIC multipath for &lt;em&gt;months&lt;&#x2F;em&gt;. It‚Äôs a huge undertaking, and we‚Äôre all very excited to see it come together.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;12-are-there-any-bindings-or-plans-for-bindings-to-other-languages-iroh-ffi-seems-to-provide-support-for-python-what-is-it-s-status-and-do-you-plan-to-offer-official-support-for-any-other-languages&quot;&gt;12. Are there any bindings or plans for bindings to other languages? Iroh-ffi seems to provide support for Python, what is it‚Äôs status and do you plan to offer official support for any other languages?&lt;&#x2F;h3&gt;
&lt;p&gt;b5: Yes, we have plans, but need to figure out some hard stuff around what basically amounts to duck-typing in UniFFI bindings first :)&lt;&#x2F;p&gt;
&lt;p&gt;Many thanks to the Iroh team for taking the time to answer our questions!&lt;&#x2F;p&gt;
&lt;p&gt;References&lt;br &#x2F;&gt;
‚Ä¢ &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;proto&#x2F;iroh-gossip&quot;&gt;https:&#x2F;&#x2F;www.iroh.computer&#x2F;proto&#x2F;iroh-gossip&lt;&#x2F;a&gt;&lt;br &#x2F;&gt;
‚Ä¢ &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.bartoszsypytkowski.com&#x2F;hyparview&quot;&gt;https:&#x2F;&#x2F;www.bartoszsypytkowski.com&#x2F;hyparview&lt;&#x2F;a&gt;&lt;br &#x2F;&gt;
‚Ä¢ &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;asc.di.fct.unl.pt&#x2F;~jleitao&#x2F;pdf&#x2F;dsn07-leitao.pdf&quot;&gt;https:&#x2F;&#x2F;asc.di.fct.unl.pt&#x2F;~jleitao&#x2F;pdf&#x2F;dsn07-leitao.pdf&lt;&#x2F;a&gt;&lt;br &#x2F;&gt;
‚Ä¢ &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.bartoszsypytkowski.com&#x2F;plumtree&#x2F;&quot;&gt;https:&#x2F;&#x2F;www.bartoszsypytkowski.com&#x2F;plumtree&#x2F;&lt;&#x2F;a&gt;&lt;br &#x2F;&gt;
‚Ä¢ &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;asc.di.fct.unl.pt&#x2F;~jleitao&#x2F;pdf&#x2F;srds07-leitao.pdf&quot;&gt;https:&#x2F;&#x2F;asc.di.fct.unl.pt&#x2F;~jleitao&#x2F;pdf&#x2F;srds07-leitao.pdf&lt;&#x2F;a&gt;&lt;br &#x2F;&gt;
‚Ä¢ &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;proto&#x2F;iroh-docs&quot;&gt;https:&#x2F;&#x2F;www.iroh.computer&#x2F;proto&#x2F;iroh-docs&lt;&#x2F;a&gt;&lt;br &#x2F;&gt;
‚Ä¢ &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.iroh.computer&#x2F;proto&#x2F;iroh-blobs&quot;&gt;https:&#x2F;&#x2F;www.iroh.computer&#x2F;proto&#x2F;iroh-blobs&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;ssl.gstatic.com&#x2F;ui&#x2F;v1&#x2F;icons&#x2F;mail&#x2F;images&#x2F;cleardot.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Transforming the Future with Zero-Knowledge Proofs, Fully Homomorphic Encryption and new Distributed Systems algorithms</title>
        <published>2023-04-13T00:00:00+00:00</published>
        <updated>2023-04-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/transforming-the-future-with-zero-knowledge-proofs-fully-homomorphic-encryption-and-new-distributed-systems-algorithms/"/>
        <id>https://blog.lambdaclass.com/posts/transforming-the-future-with-zero-knowledge-proofs-fully-homomorphic-encryption-and-new-distributed-systems-algorithms/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/transforming-the-future-with-zero-knowledge-proofs-fully-homomorphic-encryption-and-new-distributed-systems-algorithms/">&lt;p&gt;Disclaimer: To maintain brevity and clarity, we have simplified certain concepts. In this discussion, Zero Knowledge Proofs and Computational Integrity are considered as a single concept, and we will not address the distinct security properties of Proof of Work and Proof of Stake.&lt;&#x2F;p&gt;
&lt;p&gt;The evolution of every scientific discipline or engineering field experiences cycles akin to those observed in economies. Incremental advancements are made daily by corporations, individuals, and academic institutions. Occasionally, a researcher or engineer makes a groundbreaking discovery that alters the course of the field. One such example is Sir Isaac Newton, who made significant contributions to calculus, motion, optics, and gravitation during the time of the bubonic plague, which claimed millions of lives. His relentless pursuit of knowledge throughout the pandemic proved instrumental in shaping the development of mathematics, physics, and engineering. Our comfortable modern lives stand upon the foundation of these monumental discoveries.&lt;&#x2F;p&gt;
&lt;p&gt;The general public is aware of the big breakthroughs made in the aerospatial industry, energy production, internet of things, and last but not least artificial intelligence. However, most don‚Äôt know that during the COVID pandemic, enormous advances were made in cryptography. 47 years ago Diffie and Hellman wrote in their famous cryptography paper: ‚Äúwe stand today on the brink of a revolution in cryptography‚Äù, which enabled two people to exchange confidential information even when they can only communicate via a channel monitored by an adversary. This revolution enabled electronic commerce and the communication between citizens of the free world. We believe the discoveries made by researchers and engineers in cryptography during this COVID pandemic will be as important as the discoveries made by Diffie and Hellman in the upcoming decades.&lt;&#x2F;p&gt;
&lt;p&gt;One of the big discoveries has been how to make Zero-Knowledge Proofs fast enough for real-world applications. This technology has been around since 1984 but as Diffie also said, ‚ÄúLots of people working in cryptography have no deep concern with real application issues. They are trying to discover things clever enough to write papers about‚Äù. Fortunately for humanity, researchers and engineers have made this technology practical enough in the last decade (especially the last 2 years) to be useful.&lt;&#x2F;p&gt;
&lt;p&gt;The financial system depends on the existence of intermediaries: an army of auditors, regulators, and accountants. The correct working of the financial machine depends on the integrity of its financial institutions. Integrity is maintained due to positive economic incentives and jail time, fines, and costly lawsuits if the intermediaries don‚Äôt do what the state and society expect from them. Bitcoin, a result of the 2008 crisis, created a permissionless financial system where its users can send and receive digital money without intermediaries and without anybody being able to block transactions. In countries like Argentina, Nigeria, or Lebanon, where stagnation and inflation erode its citizens‚Äô trust in the financial system and the state, Bitcoin and stablecoins on top of Ethereum are used on a daily basis by the young population to save and avoid capital controls. In developed countries, its usage is not as massive since the traditional financial system and the state is trusted by most citizens. However, the world is becoming more complex. Banks are failing in the US and Europe, a new war is taking place in Europe, debt levels are not sustainable in many countries, the fight between left and the right is retaking the main stage, tension between the West and the East increases, and technological change keeps accelerating.&lt;&#x2F;p&gt;
&lt;p&gt;New applications built on top of unstoppable and trustless technologies that don‚Äôt depend on social trust will grow and thrive in this type of environment. Everything is being questioned. Only things that can‚Äôt be questioned will fully resist the passage of time. This will happen not only in developing countries but also in developed ones. Systems like Bitcoin, where everyone can verify how it‚Äôs running, are more resilient and become more useful by the day in a world that is getting more complex.&lt;&#x2F;p&gt;
&lt;p&gt;Bitcoin‚Äôs focus has been to become a new type of monetary asset and financial network. For this reason, the development of more complex programs on top of Bitcoin has always been restricted by design. Newer blockchains like Ethereum added the ability to create new types of applications. DeFi Protocols that enabled lending and borrowing, exchange of digital currencies and the ability to buy, sell and trade digital collectives and arts rapidly grew on top of Ethereum. However the cost of creating and transferring relevant amounts of assets in blockchains is costly. The ability to create more complex applications that sit on top of blockchains is also very limited. Applications can‚Äôt run more than a few milliseconds on Ethereum.&lt;&#x2F;p&gt;
&lt;p&gt;These systems do not rely on social integrity like traditional systems. Instead, they operate as a permissionless and censorship-resistant network, allowing anyone to add a node and submit updates to its state. To ensure verification, each node must re-execute all transactions, which makes the system decentralized and secure, albeit slower than centralized systems. Consequently, this imposes a limitation on the types of applications that can be built on blockchains. Applications requiring frequent database state updates, such as those exceeding a few times per second, or machine learning algorithms, are not feasible on blockchain platforms.&lt;&#x2F;p&gt;
&lt;p&gt;This is where Zero Knowledge Proofs (ZKPs) and other cryptographic and distributed systems primitives will help society create tools that can be used by everyone. ZKPs enable a party to demonstrate a statement to other parties without revealing any information beyond the proof. In more concrete terms, this enables a person to show another person that the computation they did is correct without having to redo it and without even having to grant access to the data that was used. An important aspect of this is that the verification is done in a much faster time than the proving. In even simpler terms, it proves that the output of a certain computation is correct. The verification is way easier and faster to do than the execution or proving. Anybody can check the proof, and this saves computing time and money.&lt;&#x2F;p&gt;
&lt;p&gt;At the beginning it‚Äôs difficult to grasp, even for engineers, that such a technology is even possible. The mathematics behind it, until recently, seemed magical, and that‚Äôs why it was called moon math. Thanks to ZKPs, transferring money in blockchains similar to Bitcoin is cheaper and way faster since there is no need to re-execute each transaction by each node. Only one node is needed to process all the transactions and prove them using a ZKPs, while the rest simply need to verify it, saving valuable computing resources. Among other things, ZKPs enable creating a financial system that doesn‚Äôt depend on social trust like traditional finance and that doesn‚Äôt depend as much on re-executing algorithms as Bitcoin.&lt;&#x2F;p&gt;
&lt;p&gt;Zero Knowledge Proofs facilitate the development of an entirely new range of applications that are executed and proven on a single computer outside the blockchain, with verification occurring within Ethereum. The verification cost is way cheaper than the time it takes to prove or execute it. Ethereum will evolve from a slow yet secure distributed mainframe, where execution time is shared among all users to run small programs, into a distributed computer that stores and verifies proofs generated externally from the blockchain.&lt;&#x2F;p&gt;
&lt;p&gt;Not only will blockchains benefit from the development of new cryptographic primitives like Zero Knowledge Proofs (ZKPs), but other areas will also be significantly impacted. As AI-generated content begins to overshadow human-generated content on the internet, ZKPs will become essential for verifying that such content was produced by unbiased AI models. ‚ÄúProof of humanity‚Äù systems are already employing ZKPs to ensure the accurate computation of a human accessing specific resources.&lt;&#x2F;p&gt;
&lt;p&gt;Hardware is another area where ZKPs will make an impact. Similar to how graphics cards in the 1990s revolutionized the video game industry, zero-knowledge hardware acceleration will be integrated into computers to enhance efficiency.&lt;&#x2F;p&gt;
&lt;p&gt;ZKPs can also be utilized to balance storage and computation securely. For instance, security cameras generate vast amounts of data. ZKPs can provide a compact proof that AI models did not detect any critical information in the video, allowing the system to delete the footage and save storage space.&lt;&#x2F;p&gt;
&lt;p&gt;ZKPs will even be used for national security purposes. As energy production shifts from centralized power plants to distributed sources like solar panels and wind turbines, verifying the proper execution of software on their controllers becomes vital. In the coming decades, ZKPs will play a crucial role in securing these devices.&lt;&#x2F;p&gt;
&lt;p&gt;Software industry regulations are inevitable, and industries such as online casinos and ad networks using Real-Time Bidding protocols will be legally required to demonstrate that they have not deceived their clients. Laws protecting users from large tech corporations are already in place in Europe, partly due to concerns about data misuse by foreign powers to influence political campaigns.&lt;&#x2F;p&gt;
&lt;p&gt;Requirements for secure storage and processing of encrypted data will become increasingly necessary. Fully Homomorphic Encryption (FHE), a technology akin to ZKPs, will be one of the tools utilized for this purpose. FHE enables computation on encrypted data, ensuring privacy. As FHE becomes more efficient and practical, most databases will integrate some FHE functionality, preventing administrators from accessing user data directly.&lt;&#x2F;p&gt;
&lt;p&gt;Zero-knowledge proofs (ZKPs), which generate evidence for a third party to confirm the accurate execution of a computation, and Fully Homomorphic Encryption (FHE), which enables calculations on encrypted data, will be combined with distributed systems algorithms that are capable of tolerating significant network failures similar to those employed by Bitcoin. Together they will be utilized to comply with regulations while creating trustless applications.&lt;&#x2F;p&gt;
&lt;p&gt;In the past decade, we have successfully launched applications serving dozens of millions of users. Leveraging our expertise, we are now dedicated to providing both technical and financial support to help others create startups focused on developing and implementing these vital technologies. As society grapples with the challenges of our rapidly evolving world these innovations will prove to be indispensable.&lt;&#x2F;p&gt;
&lt;p&gt;Federico Carrone.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>A walkthrough on the open source Aleo VM implemented with Arkworks and blockchain implemented with Tendermint</title>
        <published>2023-02-03T00:00:00+00:00</published>
        <updated>2023-02-03T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/open-source-aleo-vm-implemented-with-arkworks-and-blockchain-implemented-with-tendermint/"/>
        <id>https://blog.lambdaclass.com/posts/open-source-aleo-vm-implemented-with-arkworks-and-blockchain-implemented-with-tendermint/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/open-source-aleo-vm-implemented-with-arkworks-and-blockchain-implemented-with-tendermint/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;For the last 12 weeks, at LambdaClass, we have been developing an alternative implementation of the Aleo Blockchain. We want to thank Alex Pruden and Howard Wu from Aleo for their support throughout the process.&lt;&#x2F;p&gt;
&lt;p&gt;At a high level, the project consists of a Consensus Layer using Tendermint and a Zero-Knowledge Virtual Machine targeting Aleo instructions implemented with the arkworks framework.&lt;&#x2F;p&gt;
&lt;p&gt;You can check out the code:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * [Tendermint Blockchain implementation](https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;aleo_lambda_blockchain)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * [Virtual Machine implemented with Arkworks](https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;aleo_lambda_vm)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The key features of this blockchain revolve around the fact that it is designed to be a fully-private platform for users to develop applications that can then be built and executed off-chain, generating a proof of execution which is then sent to the blockchain nodes for verification and storage.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you‚Äôre in need of a team of engineers and researchers who‚Äôve been working together for a decade in areas like distributed systems, machine learning, compilers, and cryptography, we‚Äôre your guys. Wanna chat more about it? Book a meeting with us by sending us an &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;calendly.com&#x2F;federicocarrone&quot;&gt;email&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;consensus-layer&quot;&gt;Consensus Layer&lt;&#x2F;h2&gt;
&lt;p&gt;The consensus layer is in charge of validating incoming transactions which perform state changes and replicating these transactions (and the order in which they were performed) on an arbitrary number of nodes.&lt;&#x2F;p&gt;
&lt;p&gt;To achieve this, we decided to utilize &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;tendermint&#x2F;tendermint&quot;&gt;Tendermint Core&lt;&#x2F;a&gt;, an implementation of a consensus mechanism written in Go. Alongside the Tendermint Core binaries, you need to run your implementation of an &lt;em&gt;Application Blockchain Interface&lt;&#x2F;em&gt; (or &lt;em&gt;ABCI&lt;&#x2F;em&gt; for short). This ABCI needs to implement specific hooks that Tendermint Core calls through a socket whenever required. For example, when receiving a transaction, it will call &lt;code&gt;CheckTx&lt;&#x2F;code&gt;, which is supposed to validate the transaction before entering it into the mempool and relaying it to other nodes. This flexible approach allows for the ABCI to be written in any language as long as it responds to the calls appropriately. We decided to write our implementation in Rust.&lt;&#x2F;p&gt;
&lt;p&gt;You can see the code for this implementation &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;AleoHQ&#x2F;aleo_lambda_blockchain&quot;&gt;here&lt;&#x2F;a&gt;. The repository also contains a CLI application to compile, deploy and execute programs and send these transactions to the blockchain easily. It also has several other features related to accounts, such as retrieving a user‚Äôs balance or seeing which &lt;em&gt;records&lt;&#x2F;em&gt; the account possesses. We will explain the motivation behind records in the integration section of this post, but they are essentially a way to encapsulate state and ownership functionality in the blockchain.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;design-considerations&quot;&gt;Design considerations&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;images&#x2F;external&#x2F;architecture.png?raw=true&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Considering the VM implementation and the requirements from the blockchain, we had to make several design decisions on the consensus layer. Here‚Äôs a general overview of how Tendermint Core was implemented:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The Tendermint Core and the ABCI need to run side by side in the same node and are coupled by the interface defined by the protocol&amp;#39;s hooks.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * All code executed on the ABCI needs to be deterministic and isolated from external services since we want to ensure all transactions perform deterministic state changes on every node in the network.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The ABCI implements two databases to maintain the current state of the blockchain: The program store and the record store. &lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;      * The program store keeps track of every deployed program&amp;#39;s verifying keys and uses. The store contains the `credits` program&amp;#39;s keys as a built-in default. This program defines credit records. It is essentially a native Aleo program that has functions for managing credit records.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;      * The record store encapsulates functionality related to validating whether the records utilized in incoming transactions have already been spent. &lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        * The privacy requirements imply that we cannot disclose what records have been spent and which have not. Due to this, any record in the blockchain (i.e., it was output from the execution of a program) is stored separately from records that have been spent, of which we only store serial numbers.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The genesis block needs to be provided to Tendermint on startup and is done through a JSON file. We have written a particular binary to generate it for any number of nodes and give each of them a fixed amount of starting credits.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * To make testing simple, we have created several `make` targets to initialize and start multiple validators that can run locally or on a remote network.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Both the CLI and the consensus layer support Aleo&amp;#39;s SnarkVM and our own LambdaVM and are currently interchangeable through a compiler flag&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h3 id=&quot;staking&quot;&gt;Staking&lt;&#x2F;h3&gt;
&lt;p&gt;Tendermint supports adding new nodes to the network. In general, nodes in the network can work in two different modes:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Non-validator: The node catches up with the blockchain by performing every transaction but does not have voting power to validate and commit blocks.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Validator: The node is part of the network and can vote and sign blocks.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To add a non-validator, the node needs to have the same Genesis block and point to persistent peers (IP addresses acting as fixed nodes in the network). To transform a node into a validator, the ABCI needs to implement functionality to update the voting power of a Tendermint node.&lt;&#x2F;p&gt;
&lt;p&gt;For this, we implemented a &lt;code&gt;stake&lt;&#x2F;code&gt; command to ‚Äúfreeze‚Äù credits by exchanging them for staking records (and increase the voting power of a validator), which you can, in turn, &lt;code&gt;unstake&lt;&#x2F;code&gt; whenever you desire (decreasing the voting power accordingly).&lt;&#x2F;p&gt;
&lt;p&gt;When a node is a validator, it gets rewards on each block commit where it was involved.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;virtual-machine&quot;&gt;Virtual Machine&lt;&#x2F;h2&gt;
&lt;p&gt;At a high level, our VM provides an API to take an Aleo program that looks like this:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;program main.aleo;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;function add:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    input r0 as u16.public;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    input r1 as u16.private;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    add r0 r1 into r2;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    output r2 as u16.public;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;And generate a pair of proving and verifying keys for it (this is usually called &lt;em&gt;building&lt;&#x2F;em&gt; or &lt;em&gt;synthesizing&lt;&#x2F;em&gt; the program), allowing anyone to execute the program and provide proof of it or verify said proof. The consensus layer uses this to deploy programs (i.e., upload their verifying key along with the code), execute them, and verify them.&lt;&#x2F;p&gt;
&lt;p&gt;Internally, this VM uses &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;arkworks-rs&quot;&gt;Arkworks&lt;&#x2F;a&gt; as a backend. Programs are turned into a Rank One Constraint System (&lt;code&gt;R1CS&lt;&#x2F;code&gt;), which is then passed on to the &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;arkworks-rs&#x2F;marlin&quot;&gt;Marlin&lt;&#x2F;a&gt; prover for execution. As we started using Arkworks, we noticed some aspects of the API and its genericity were becoming a burden for developers, so we created a thin wrapper around it called &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;simpleworks&quot;&gt;Simpleworks&lt;&#x2F;a&gt;, along with &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;lambdaclass.github.io&#x2F;simpleworks&#x2F;overview.html&quot;&gt;some basic documentation&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;example&quot;&gt;Example&lt;&#x2F;h3&gt;
&lt;p&gt;Given the following Aleo program&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;program foo.aleo;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;function main:&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    input r0 as u64.public;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    input r1 as u64.public;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    add r0 r1 into r2;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    output r2 as u64.public;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Executing the function &lt;code&gt;main&lt;&#x2F;code&gt; would look like this:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;use lambdavm::jaleo::UserInputValueType::U16;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;fn main() {&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    use lambdavm::{build_program, execute_function};&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Parse the program&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let program_string = std::fs::read_to_string(&amp;quot;.&#x2F;programs&#x2F;add&#x2F;main.aleo&amp;quot;).unwrap();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let (program, build) = build_program(&amp;amp;program_string).unwrap();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let function = String::from(&amp;quot;main&amp;quot;);&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Declare the inputs (it is the same for public or private)&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let user_inputs = vec![U16(1), U16(1)];&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    &#x2F;&#x2F; Execute the function&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let (_execution_trace, proof) = execute_function(&amp;amp;program, &amp;amp;function, &amp;amp;user_inputs).unwrap();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    let (_proving_key, verifying_key) = build.get(&amp;amp;function).unwrap();&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    assert!(lambdavm::verify_proof(verifying_key.clone(), &amp;amp;user_inputs, &amp;amp;proof).unwrap())&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;h3 id=&quot;internals&quot;&gt;Internals&lt;&#x2F;h3&gt;
&lt;p&gt;The most significant task our VM has to perform is turning the program into an arithmetic circuit, as the rest of the work, namely generating the proof and verifying it, is pretty straightforward with the Arkworks API.&lt;&#x2F;p&gt;
&lt;p&gt;Before continuing, you should have at least a basic understanding of arithmetic circuits and how Arkworks lets you work with them. You can read about it &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;lambdaclass.github.io&#x2F;simpleworks&#x2F;overview.html&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To generate the circuit, we go through the following steps:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Take the program&amp;#39;s source code and parse it into a `Program` containing all the relevant information about it (a list of all input and output instructions, whether they are public or private, a list of all regular instructions like add and its operands, etc.). We currently rely on SnarkVM&amp;#39;s parser but plan to write our own.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Instantiate an Arkworks `ConstraintSystem`, which will hold all our circuit&amp;#39;s constraints by the end.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * For every input instruction, instantiate its corresponding `Gadget`. You can think of a gadget as the equivalent of a native type (like `u8`) inside an arithmetic circuit. If the input is public, the gadget is made public; otherwise, it&amp;#39;s made a `witness`, i.e., private. In our example, the first instruction `input r0 as u16.public` becomes a call to `UInt16Gadget.new_input(...)` and the second instruction becomes `UInt16Gadget.new_witness(...)`.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * For every regular instruction, we use the gadget&amp;#39;s associated function to perform the operation and generate its constraints inside our `ConstraintSystem`. In our example, when we encounter the `add r0 r1 into r2;` instruction, we call `UInt16Gadget.addmany(...)`. This is an arkworks provided function that will take a list of `UInt16&amp;#39;s, add them, implicitly mutate the `ConstraintSystem` with all the associated constraints, then return the value of the sum. Not all instructions have a corresponding arkworks function implemented, so for those, we had to roll our own.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * For every output instruction, assign to the register the computed value.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Because a program can have multiple registers interacting with each other, to do the above, we have to keep track of each register and its value as we go. For this, we keep an internal hash table throughout execution.&lt;&#x2F;p&gt;
&lt;p&gt;Additionally, we ran some benchmarks comparing our VM with Aleo‚Äôs &lt;code&gt;SnarkVM&lt;&#x2F;code&gt;, and our results show we are a few times faster than it; details will be published in a separate post. The code for benchmarks is in &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;aleo_lambda_vm&quot;&gt;our VM Repo&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;vm-consensus-integration-layer&quot;&gt;VM-Consensus Integration Layer&lt;&#x2F;h2&gt;
&lt;p&gt;Above, we discussed how the VM allows running arbitrary Aleo programs that can be deployed, executed locally, and then verified on the Aleo blockchain. Each Aleo transaction is either the deployment or the proof of execution of a program (this is technically inaccurate, as there can be multiple of these per transaction, but we‚Äôll ignore that for simplicity). In the case of executions, nodes use the program‚Äôs verifying key to verify the correct execution before committing transactions to a block.&lt;&#x2F;p&gt;
&lt;p&gt;After we got a basic VM version working, we realized that getting a fully functional Aleo blockchain required more work than just the above. Transactions would be of very little use if they proved that some computation was done correctly. To be useful, they also need to &lt;em&gt;modify the state&lt;&#x2F;em&gt;. In Aleo, the state is managed through &lt;em&gt;records&lt;&#x2F;em&gt; in what is essentially a &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Unspent_transaction_output&quot;&gt;UTXO&lt;&#x2F;a&gt; model similar to Bitcoin. Typically, when a user sends a transaction, they will spend some records they own to create new ones in their place.&lt;&#x2F;p&gt;
&lt;p&gt;Because Aleo is entirely private, a transaction can‚Äôt just publish the records it wants to spend along with a signature; it has to &lt;em&gt;prove&lt;&#x2F;em&gt; ownership and existence of records in zero knowledge, then &lt;em&gt;encrypt&lt;&#x2F;em&gt; the records so only its owner can decrypt on-chain.&lt;&#x2F;p&gt;
&lt;p&gt;This means that, to integrate with the consensus layer and get a fully functional blockchain, we need a bit more. The VM can prove the correct execution of programs, but the Zero-Knowledge proof that comes with a transaction also needs to include the following:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * A signature in Zero-Knowledge, proof that the signature provided is the correct one. Remember, we can&amp;#39;t just show the user&amp;#39;s address sending the transaction.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * A proof that the caller of the transaction actually _owns_ the record they&amp;#39;re spending.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * A proof that the records being spent are on-chain. This is essentially verifying a Merkle path in Zero-Knowledge.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * A proof that the input records have not been spent. This is a bit involved as it requires deriving a record&amp;#39;s `serial number` (think of it as the `nullifier` if you know ZCash) in Zero-Knowledge.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We also talked about how records should be stored encrypted on-chain so that only someone possessing the record owner‚Äôs view key can decrypt them (in Aleo, the &lt;code&gt;view key&lt;&#x2F;code&gt; is just another key tied to an account that allows record decryption).&lt;&#x2F;p&gt;
&lt;p&gt;There‚Äôs a catch here, though. When, for instance, user A wants to send money to user B, they have to create a record owned by B and encrypt it so that only B can decrypt it. But &lt;code&gt;A&lt;&#x2F;code&gt; does not necessarily have &lt;code&gt;B&lt;&#x2F;code&gt;‚Äôs view key, only their address. This means the encryption scheme used by Aleo cannot be symmetric, as that would require user &lt;code&gt;A&lt;&#x2F;code&gt; to have &lt;code&gt;B&lt;&#x2F;code&gt;‚Äôs view key to send them money, not just their address.&lt;&#x2F;p&gt;
&lt;p&gt;To accomplish this, records are encrypted using a scheme called &lt;code&gt;ECIES&lt;&#x2F;code&gt; (Elliptic Curve Integrated Encryption Scheme). We‚Äôre not going to go into detail about how it works, but it‚Äôs a combination of a Diffie-Hellman key exchange with a symmetric encryption scheme.&lt;&#x2F;p&gt;
&lt;p&gt;We introduced a middle layer between our VM and the Consensus Layer to solve all the problems discussed above. This middle layer handles everything related to records, their encryption, and the snarks required for the state transition proofs.&lt;&#x2F;p&gt;
&lt;p&gt;In the original SnarkVM implementation, this middle layer does not really exist, as it‚Äôs part of the VM itself, but we found it more beneficial to separate these two concerns.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;work-in-progress&quot;&gt;Work in Progress&lt;&#x2F;h2&gt;
&lt;p&gt;This project is still in active development, and a few things are being worked on. They include:&lt;&#x2F;p&gt;
&lt;pre class=&quot;giallo&quot; style=&quot;color: #E1E4E8; background-color: #24292E;&quot;&gt;&lt;code data-lang=&quot;plain&quot;&gt;&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Support for some data types and instructions on the VM, including the `group` data type (elliptic curve elements) and things like `BHP` commitments. You can check out a complete list on [the README](https:&#x2F;&#x2F;github.com&#x2F;lambdaclass&#x2F;aleo_lambda_vm).&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Some of the circuits mentioned above prove the correctness of state transitions.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * The generation of the proof that input records exist on-chain.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;span class=&quot;giallo-l&quot;&gt;&lt;span&gt;    * Due to how we store record information on the blockchain and considering the privacy requirements of the blockchain, asking for a user balance or unspent records from the CLI is currently not trivial: We need to ask for all records that have ever existed in addition to all serial numbers from records that have been spent and attempt to decrypt them on the user&amp;#39;s side. Some strategies to optimize this process include keeping track of records locally and only adding newly-created ones as the blockchain grows.&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We plan to finish these tasks in the next four weeks. While many things could be improved, the project is already production ready.&lt;&#x2F;p&gt;
&lt;p&gt;We have many ideas and comments about improving the SnarkVM and Aleo in general, but we will leave that for another series of posts.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Interview with Noria‚Äôs creator: a promising dataflow research database implemented in Rust</title>
        <published>2019-10-22T00:00:00+00:00</published>
        <updated>2019-10-22T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/interview-with-norias-creator-a-promising-dataflow-database-implemented-in-rust/"/>
        <id>https://blog.lambdaclass.com/posts/interview-with-norias-creator-a-promising-dataflow-database-implemented-in-rust/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/interview-with-norias-creator-a-promising-dataflow-database-implemented-in-rust/">&lt;p&gt;After reading &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;download.tensorflow.org&#x2F;paper&#x2F;whitepaper2015.pdf&quot;&gt;Tensorflow‚Äôs original paper&lt;&#x2F;a&gt; I learnt that four of its authors were authors of Microsoft &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;sigops.org&#x2F;s&#x2F;conferences&#x2F;sosp&#x2F;2013&#x2F;papers&#x2F;p439-murray.pdf&quot;&gt;Naiad‚Äôs research paper&lt;&#x2F;a&gt; too. Naiad influenced many systems like Tensorflow.&lt;&#x2F;p&gt;
&lt;p&gt;The Naiad paper is really interesting since it brings together many computation patterns: batch computation, streaming computation, and graph computation. It seemed to be a higher level abstraction than the traditional MapReduce and at the same time a more elegant &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lambda_architecture&quot;&gt;Lambda architecture&lt;&#x2F;a&gt; that combines batch processing with streaming methods. Being an practitioner and not a researcher this paper helped me to understand to compare different projects like Hadoop, Spark, Storm, Samza, Flink, Kafka streams.&lt;&#x2F;p&gt;
&lt;p&gt;Naiad‚Äôs paper introduced a computational model called timely dataflow that has influenced other systems like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;mit-pdos&#x2F;noria&quot;&gt;Noria&lt;&#x2F;a&gt;. Noria is a fast storage backend for read-heavy web applications implemented in Rust with a MySQL adapter. One of its creators is Jon Gjengset, a PhD student in the Parallel and Distributed Operating Systems group at MIT. Jon has a great &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thesquareplanet.com&#x2F;blog&#x2F;&quot;&gt;blog&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UC_iD0xppBwwsrM9DegC5cQQ&quot;&gt;youtube channel&lt;&#x2F;a&gt; where he discusses everything from distributed algorithms to how to implement a ZooKeeper clone. He is into many subjects I love: Rust, distributed systems, databases and the relationship between Noria, Naiad and Tensorflow were enough reasons to interview him. We did not discuss Tensorflow but I hope to find out more about the relationship between it in the future.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;What is Noria?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Noria is a dynamic dataflow database that supports partial and incremental materialized views. To break that down a bit more, it is a database that is implemented using a streaming dataflow engine that can be changed on the fly (that‚Äôs the dynamic part). It supports pre-computing the results of queries (materialized views), and updates those materialized results as the data is updated (view maintenance).&lt;&#x2F;p&gt;
&lt;p&gt;When this happens, the results are updated in-place rather than recomputed wholesale (i.e., the maintenance is incremental). When queries have parameters (e.g., &lt;em&gt;foo = ?&lt;&#x2F;em&gt;), it supports materializing the results for only some value of &lt;em&gt;foo&lt;&#x2F;em&gt; , and will compute results for ‚Äúmissing‚Äù values only when they are required (i.e., the materializations are partial).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How does it compare to other software like a relational database, in memory key-value stores, map reduce systems like Spark, stream processing like Storm or timely data flow?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The answer here requires some nuance, so please bear with me.&lt;&#x2F;p&gt;
&lt;p&gt;Noria is very similar to a relation database on the outside. It has tables and SQL queries, and even supports the MySQL binary protocol. You interact with Noria through prepared statements, &lt;em&gt;SELECT&lt;&#x2F;em&gt; s, &lt;em&gt;INSERT&lt;&#x2F;em&gt; s, and &lt;em&gt;UPDATE&lt;&#x2F;em&gt; s. Internally though, it is quite different. Whereas a traditional database executes a query when it receives a &lt;em&gt;SELECT&lt;&#x2F;em&gt; , Noria generally executes &lt;strong&gt;all&lt;&#x2F;strong&gt; queries when data the query‚Äôs result depends on changes. In the steady state of the system, we expect queries to be executed on &lt;strong&gt;write&lt;&#x2F;strong&gt; , not on &lt;strong&gt;read&lt;&#x2F;strong&gt;. There are some smarts required here to not do undue work. For example, Noria only computes and maintains results for query parameters the application cares about (this is the ‚Äúpartial materialization‚Äù piece). Noria also executes queries ‚Äúincrementally‚Äù; if you add a new vote to an article with a million votes, it knows to increase the count by one, rather than count a million and one things all over again.&lt;&#x2F;p&gt;
&lt;p&gt;While Noria implements a key-value store internally to maintain and serve its materialized results, it does not seem like a key-value store to users of the system. Application authors write full SQL queries, and get structured results (rows of columns) back, just like with a normal database. The only sense in which Noria is like a key-value store is in its performance ‚Äî query results will generally be fetched about as fast as a key-value store lookup.&lt;&#x2F;p&gt;
&lt;p&gt;Streaming data-flow systems like Spark, Storm, Kafka, and timely dataflow share many similarities with Noria. They process data in the same streaming fashion, and have a similar distributed system design that relies on sharding and operator partitioning. Noria differs from these systems in a few principal ways though. First, users of Noria can change the running dataflow at any time without downtime. If a new SQL query is issued that the system has not seen before, it adapts the running dataflow on the fly to incorporate the operators from the new query. The adaptation also knows to re-use existing operators where possible to produce an overall more efficient dataflow than what you would get if you just ran each query as its own dataflow program.&lt;br &#x2F;&gt;
Second, Noria supports partial materialization. Existing dataflow systems that support materialization are usually either windowed (i.e., they only reflect ‚Äúrecent‚Äù updates) or fully materialized (i.e., all results are always stored and maintained). Neither of these would work for web applications. When you issue a query, you expect to get all the results for &lt;strong&gt;that&lt;&#x2F;strong&gt; query (so no windowing), but you also don‚Äôt expect the system to waste resources on results that your application does not care around. You can think of this latter requirement as ‚Äúyou need the ability to evict from your cache‚Äù. And finally, Noria has a familiar interface for its queries: you issue SQL queries, and the system automatically translates them into efficient dataflow and adapts the running system to them. The other systems do not generally support this.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The Noria paper states that it can scale to 5x higher load than a hand optimised MySQL database. How does it manage to do that?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The primary reason for this is Noria‚Äôs view materialization. When you issue a query to MySQL, the system has to &lt;strong&gt;execute&lt;&#x2F;strong&gt; that query to produce the query‚Äôs results. In Noria on the other hand, an application query effectively turns into a hashmap lookup in the common case, which is &lt;strong&gt;very&lt;&#x2F;strong&gt; fast. Noria‚Äôs reads can also happen entirely in parallel, with very little synchronization overhead, whereas MySQL includes a lot of machinery to support full-fledged transactions (which Noria does not support). Part of the trick here is Noria‚Äôs use of a neat little datastructure called an evmap (for ‚Äúeventual map‚Äù; named for its support for eventual consistency). It allows reads and writes to proceed entirely in parallel with very little overhead by having reads and writes go to different hashmaps, and then occasionally atomically switching between them.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What is the relationship between&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;jon.tsp.io&#x2F;papers&#x2F;osdi18-noria.pdf&quot;&gt;&lt;strong&gt;Noria‚Äôs paper&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;strong&gt;, the&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;timelydataflow&#x2F;differential-dataflow&#x2F;blob&#x2F;master&#x2F;differentialdataflow.pdf&quot;&gt;&lt;strong&gt;Differential dataflow&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;strong&gt;paper and&lt;&#x2F;strong&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;sigops.org&#x2F;s&#x2F;conferences&#x2F;sosp&#x2F;2013&#x2F;papers&#x2F;p439-murray.pdf&quot;&gt;&lt;strong&gt;Naiad: a timely dataflow system&lt;&#x2F;strong&gt;&lt;&#x2F;a&gt;&lt;strong&gt;?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Differential dataflow, timely dataflow, and its implementation in Naiad is one of the hallmark systems in the dataflow community. Most papers in this area relate to timely dataflow in one way or another. In the case of Noria, we also implement an incremental dataflow model, but we are trying to solve for a different use-case, and therefore arrive at different solutions.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, timely established a model for incrementally executing dataflow programs that include iteration and cycles with strong guarantees about consistency through a sophisticated timestamp tracking scheme. Noria does not support iteration or cycles, and is eventually consistent.&lt;&#x2F;p&gt;
&lt;p&gt;Instead, we provide high-performance materialized views for fast reads, partial state so only the working set needs to be kept in memory, automatic multi-query optimization, support for dynamically modifying the dataflow as it runs, and of course SQL support. Timely does not support these features, though you could likely manually implement some of them on top of timely‚Äôs core given enough time and research effort.&lt;&#x2F;p&gt;
&lt;p&gt;Overall, I don‚Äôt think it‚Äôs fair to say one system is &lt;strong&gt;better&lt;&#x2F;strong&gt; than another. In many ways they complement each other. Timely largely targets arbitrary batch computations over a large, interconnected dataset where reads are less frequent than just observing the ‚Äúoutput‚Äù of the computation. And it is &lt;strong&gt;very&lt;&#x2F;strong&gt; good at that. Noria targets read-heavy applications where repeated and similar queries are common, and where the queries change over time. And it is &lt;strong&gt;very&lt;&#x2F;strong&gt; good at that.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Do you implement the full SQL language?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It‚Äôs not clear what ‚Äúfull SQL‚Äù even really means, with all of the various extensions to the language that have been added to different implementations over time. Even if you restrict yourself to ANSI SQL though, the answer for Noria is no, although mostly for uninteresting reasons. Noria is a research prototype, and as such we have focused on the features that required active research to implement in Noria‚Äôs database model. Many of the remaining features we believe could be added with sufficient engineering effort, but without too much technical difficulty. To give some examples of things we don‚Äôt support: joins whose join condition is not a single column equality; &lt;em&gt;ORDER BY&lt;&#x2F;em&gt; without a limit; &lt;em&gt;CASE&lt;&#x2F;em&gt; statements; &lt;em&gt;LIKE&lt;&#x2F;em&gt; conditions; and of course the &lt;em&gt;SOUNDEX&lt;&#x2F;em&gt; operator. There are also patterns that we support, but that we believe could be optimized further, such as multi-way joins and multiple aggregations in a single query.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why did you choose to use RocksDB to persist the data?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This was a more or less arbitrary decision. We initially wrote all base table writes directly to disk as a log, but quickly realized we needed to also keep indices over that on-disk data, otherwise recovery would be far too slow. We looked for an off-the-shelf solution, and RocksDB seemed to fit the bill. The interface for this base storage layer is pretty straightforward, and it should not be too difficult to slot in another solution there. The biggest challenge in doing so is maintaining some invariants around what writes are visible when for the purposes of Noria‚Äôs upqueries, but we believe these are solveable without too much trouble.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why does Noria needs to have Zookeeper running? Why did you choose ZooKeeper over etcd or consul?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Zookeeper serves two purposes in Noria at the moment. Service discovery and leadership change. And in fact, if you run a Noria server and client in a single process, you don‚Äôt actually need Zookeeper running at all.&lt;&#x2F;p&gt;
&lt;p&gt;If you run a single Noria worker, and a separate client, Zookeeper is only used for the client to have an easy way to discover the location of the server. We‚Äôre considering adding a non-distributed mode to Noria which supports this single-worker use-case without Zookeeper. The bits are already in place (take a look at the &lt;em&gt;Authority&lt;&#x2F;em&gt; trait in the code if you are curious), it just hasn‚Äôt been a priority for us to fix. If you are running &lt;strong&gt;multiple&lt;&#x2F;strong&gt; Noria workers, then they need some way to agree on which worker is responsible for driving application-issued changes to the dataflow, and that is where Zookeeper‚Äôs consensus comes into play. We needed a system that allowed the workers to agree on who that should be, with a mechanism for failover, and Zookeeper provided that. The API we need from Zookeeper is very limited, and it should be straightforward to slot in another consensus provider in its place.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Is Noria production ready? Do you know anybody using it?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Noria is most definitely still a research prototype, though I think the thing standing between where it is now and a production-ready version is mostly just engineering effort. We are a small team of researchers working on it, and we focus our efforts on the aspects of the system that are related to our ongoing research. There is relatively little room for spending lots of time on doing ‚Äúproduction engineering‚Äù in the academic setting :)&lt;&#x2F;p&gt;
&lt;p&gt;That said, I know of several large companies who are very interested in&lt;br &#x2F;&gt;
using Noria in a production setting, and many of them have gotten in touch with me about what might be required to achieve that. I also know that multiple companies are trying Noria out privately internally to test its viability as a replacement for certain parts of their stack. What ultimately comes of that is unclear at the moment, but I of course hope that they find Noria promising, and that they are willing to invest time into making it production ready!&lt;&#x2F;p&gt;
&lt;p&gt;There are a few features missing from Noria that I think are the primary blockers from using it in production. The first is checkpointing of materialized views. Currently, if the system is turned off and then restarted, all the materialized views are empty. This would be equivalent to a full cache purge. An external system could heat the cache, but it‚Äôd be better if Noria internally kept some form of snapshot to aid in this process. The second is fault tolerance ‚Äî when Noria is run in a distributed setting, a machine failure results in related parts of the dataflow being blown away entirely and restarted. This is obviously problematic in production settings. We are actively pursuing research in this area, and have some ideas for how to fix it, but it is a complex subject. And finally, Noria currently requires ownership of its base storage. If you have an existing data store that you‚Äôd like to keep using, you‚Äôd have to feed changes to that data to Noria, and Noria would store it a second time. Changing Noria such that it can handle the base storage being managed by a different system is possible, though would require some careful engineering with respect to consistency of upqueries.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;When would you avoid using Noria?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Noria is not great for systems that do not have a well-defined working set. If your application is constantly issuing entirely new queries over your data that do not relate to previous queries, or if it rarely queries by the same set of keys more than once, then Noria‚Äôs materializations will be mostly useless and just add unnecessary overhead. Noria is also primarily built for read-heavy applications; if your application rarely does reads, but sustains a &lt;strong&gt;very&lt;&#x2F;strong&gt; high write load, then Noria in its current form is probably not what you want. I say ‚Äúprobably‚Äù because Noria already supports a fairly high write throughput, and if your inputs fall below that threshold, Noria will still work fine. And, crucially, its materializations will make your reads over this quickly growing collection really fast!&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why did you choose to use Rust to implement it?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The answer to this is perhaps less interesting than you‚Äôd expect. When&lt;br &#x2F;&gt;
we first started this iteration of the project in mid-2016, Rust was just starting to appear as a viable systems language. We were looking for a systems language to use for this new project, and did not particularly want to write C++, and wanted to explore something beyond Go. We heard about Rust, its claims were enticing, so as researchers we figured we‚Äôd try it out and see if it could live up to its promises. The cost of failure was low, as we could always start over, so we just ran with it. And now we‚Äôre 80k lines of code in, and I still think it was the right choice.&lt;&#x2F;p&gt;
&lt;p&gt;I can also give a post-hoc analysis of the journey. I think choosing Rust has worked out great for us; specifically, it has saved us countless hours of debugging. We write a lot of concurrent code in Noria, and the Rust compiler has caught a ridiculous number of concurrency bugs that would just have slipped right by in another language. Debugging those in a distributed context in a research system where we don‚Äôt even know if the underlying &lt;strong&gt;algorithm&lt;&#x2F;strong&gt; is sound would have been a major pain (and still is when it happens). Rust has also allowed us to write low-level code when we needed to squeeze out those last bits of performance in the core pieces of Noria. What is more, I have found the Rust ecosystem to be a joy to participate in and to rely on; solid libraries like tokio have allowed us to focus on the core research-y parts of the application, and lots of knowledgeable Rustaceans have helped us when we‚Äôve run into weird issues. Not only that, but we have been able to contribute back to that ecosystem by publishing libraries of our own and by contributing back to the compiler and other core libraries that we relied on.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;You have videos on how to implement a TCP stack, a minimal Zookeeper implementation and blog posts explaining how to implement Raft. What would gain a traditional full stack developer learning how to implement low level structures, distributed algorithms or distributed software?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I think much of this comes down to curiosity. If you‚Äôre curious about how stuff works, there‚Äôs an endless number of rabbit holes you can easily start down that teach you all of this stuff. For me, much of my distributed systems knowledge came from MIT‚Äôs 6.824 Distributed Systems class, which is &lt;strong&gt;excellent&lt;&#x2F;strong&gt;. All of their reading materials, lecture notes, and labs are also available online. For algorithms, the easiest way to get started in thinking about them is to read some of the early papers describing these algorithms, and then trying to implement them yourself! You‚Äôll find that many of them aren‚Äôt as difficult to build as you may think, and you will learn a lot in the process. Including how tobread academic papers! I can also recommend trying to follow some low-level OS-building resources. For example, Philipp Oppermann has a great blog series on implementing an operating system from scratch in Rust that goes through all of the low-level details you‚Äôll need to know.&lt;&#x2F;p&gt;
&lt;p&gt;Alternatively, all the components of MIT‚Äôs 6.828 Operating Systems class are also available online.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What papers, readings and exercises do you recommend doing to learn about distributed programming?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I would highly recommend following the 6.824 class schedule&lt;br &#x2F;&gt;
(&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;pdos.csail.mit.edu&#x2F;6.824&#x2F;schedule.html&quot;&gt;https:&#x2F;&#x2F;pdos.csail.mit.edu&#x2F;6.824&#x2F;schedule.html&lt;&#x2F;a&gt;). It covers both classic&lt;br &#x2F;&gt;
papers, established approaches, and new research in the area. Do the&lt;br &#x2F;&gt;
labs as well; they will force you to get &lt;strong&gt;intimately&lt;&#x2F;strong&gt; familiar with many&lt;br &#x2F;&gt;
subtle distributed systems problems!&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Interview with Dask‚Äôs creator: Scale your Python from one computer to a thousand</title>
        <published>2019-09-03T00:00:00+00:00</published>
        <updated>2019-09-03T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://blog.lambdaclass.com/posts/interview-with-dasks-creator-scale-your-python-from-one-computer-to-a-thousand/"/>
        <id>https://blog.lambdaclass.com/posts/interview-with-dasks-creator-scale-your-python-from-one-computer-to-a-thousand/</id>
        
        <content type="html" xml:base="https://blog.lambdaclass.com/posts/interview-with-dasks-creator-scale-your-python-from-one-computer-to-a-thousand/">&lt;p&gt;My love for building distributed systems with Erlang, databases and fetching huge volumes of data still lives on. But nowadays I want to have better theoretical and practical tools to understand the data. That is why I have been seriously studying probability, statistics and getting better at Python, numpy, pandas, scikit-learn, scipy and R. If you have read my earlier interviews you are probably aware of this.&lt;&#x2F;p&gt;
&lt;p&gt;That is why I decided to interview Dask‚Äôs creator Matthew Rocklin. Dask is a great bridge between the two areas that we specialize at my company &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;lambdaclass.com&#x2F;&quot;&gt;LambdaClass&lt;&#x2F;a&gt;: distributed systems and data science. Dask is a great tool to parallelize python libraries. When you have some spare time I highly recommend that you check its code. Meanwhile I leave you with Matthew‚Äôs answers.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;What is Dask? Why did you create it?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Dask is a Python library designed to parallelize other common Python libraries, like NumPy, Pandas, Scikit-Learn and others. It helps people use Python on either a single multi-core machine, or a large distributed cluster.&lt;&#x2F;p&gt;
&lt;p&gt;People tend to use it either as a ‚ÄúBig Pandas‚Äù or ‚ÄúBig NumPy‚Äù, or as a lower level library to build their own parallel systems.&lt;&#x2F;p&gt;
&lt;p&gt;Originally we created Dask to parallelize Numpy and Pandas. We quickly found that the internals of Dask were useful for many more things, so we quickly pivoted to exposing the internals as a generic parallel system.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Dask dataframes are a full replacement of pandas dataframes?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;No, the Pandas API is &lt;strong&gt;huge&lt;&#x2F;strong&gt; , so a full replacement is nearly impossible.&lt;&#x2F;p&gt;
&lt;p&gt;That being said, Dask Dataframe does implement the vast majority of popularly used Pandas functionality. Common staples like elementwise, reductions, groupbys, joins, rolling, timeseries, and more operations are all there. Additionally, because Dask dataframes are just a bunch of Pandas dataframes spread around a cluster it‚Äôs often pretty easy to convert custom code from Pandas to Dask easily.&lt;&#x2F;p&gt;
&lt;p&gt;It‚Äôs also worth noting that Dask != Dask Dataframes. Dataframes only account&lt;br &#x2F;&gt;
for about a third of Dask use out there. Dask goes way beyond just&lt;br &#x2F;&gt;
parallelizing Pandas.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Is there any downside of using Dask dataframes instead of pandas dataframes?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Oh definitely. If Pandas is solving your problem today, please don‚Äôt switch to Dask.&lt;&#x2F;p&gt;
&lt;p&gt;As with any distributed system, Dask adds a lot of complexity like network&lt;br &#x2F;&gt;
overheads, function serialization, and longer tracebacks in errors. We do a&lt;br &#x2F;&gt;
lot of work to keep our overhead small, both by keeping Dask lightweight and&lt;br &#x2F;&gt;
taking care of Python usability, but still, if you don‚Äôt need to switch, then&lt;br &#x2F;&gt;
don‚Äôt.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How is it different form other distributed computation solutions (eg Hadoop MapReduce, Spark, Storm, Luigi, Airflow)?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Dask is a bit lower level and more generic than those systems, and so can be used to build up similar solutions using existing Python libraries.&lt;&#x2F;p&gt;
&lt;p&gt;For example:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;When we combine Dask with Pandas we get &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;docs.dask.org&#x2F;en&#x2F;latest&#x2F;dataframe.html&quot;&gt;Dask Dataframes&lt;&#x2F;a&gt;, which are comparable with Spark DataFrames&lt;&#x2F;li&gt;
&lt;li&gt;When we combine Dask with Scikit-Learn we get &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;ml.dask.org&quot;&gt;Dask-ML&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;When we combine Dask with Python‚Äôs &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;docs.dask.org&#x2F;en&#x2F;latest&#x2F;futures.html&quot;&gt;futures or async&#x2F;await&lt;&#x2F;a&gt; APIs we get a real-time framework, somewhat similar to Storm&lt;&#x2F;li&gt;
&lt;li&gt;When we combine Dask with &lt;em&gt;cron&lt;&#x2F;em&gt; like logic, we get an ETL framework like Airflow or Luigi. In fact, some of the Airflow developers split off and made &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.prefect.io&#x2F;&quot;&gt;Prefect&lt;&#x2F;a&gt; a successor to Airflow which delegates the execution and data movement to Dask&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Additionally, Dask can be combined with other libraries to get novel systems&lt;br &#x2F;&gt;
that aren‚Äôt in your list. For example:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;When we combine Dask with Numpy we get a scalable multi-dimensional &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;docs.dask.org&#x2F;en&#x2F;latest&#x2F;array.html&quot;&gt;Dask Arrays&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;When we combine Dask with GPU-accelerated Pandas or Numpy like libraries like &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;rapids.ai&quot;&gt;RAPIDS&lt;&#x2F;a&gt; we get distributed GPU-accelerated dataframes and arrays.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Internally, Dask has the scalability of a system like MapReduce or Spark, with&lt;br &#x2F;&gt;
the flexibility of a system like Luigi or Airflow. This combination is nice both when you‚Äôre building new systems, and means that Dask gets used in a ton of novel work.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How does data locality affect the performance of Dask? Does it assume all data is local to workers?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;By data locality you might mean two things (both of which Dask handles well):&lt;&#x2F;p&gt;
&lt;p&gt;1. Where does the data live in some storage system, like S3 or HDFS?&lt;&#x2F;p&gt;
&lt;p&gt;Dask is more than happy to query a data-local storage system like HDFS,&lt;br &#x2F;&gt;
find out where all the data lives, and target computations appropriately.&lt;&#x2F;p&gt;
&lt;p&gt;However, this kind of workload is becoming increasingly rare. More often&lt;br &#x2F;&gt;
people are using storage systems that prefer global accessibility over data&lt;br &#x2F;&gt;
locality, so this matters less and less these days in practice.&lt;&#x2F;p&gt;
&lt;p&gt;2. Once data is in memory, can Dask avoid moving it around?&lt;&#x2F;p&gt;
&lt;p&gt;Dask thinks a lot about where to run computations, and avoiding needless&lt;br &#x2F;&gt;
data communication is a big part of this decision. Sometimes we do need to&lt;br &#x2F;&gt;
move data around, but yes, Dask certainly avoids this when possible.&lt;&#x2F;p&gt;
&lt;p&gt;Moreover, because Dask gets used with a &lt;strong&gt;wide&lt;&#x2F;strong&gt; variety of workloads, our&lt;br &#x2F;&gt;
scheduling heuristics have had to evolve quite a bit over the years. It‚Äôs very&lt;br &#x2F;&gt;
rare for us to find problems today on which Dask‚Äôs data locality heuristics&lt;br &#x2F;&gt;
don‚Äôt respond optimally.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What is the biggest Dask cluster you have seen in production?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;One thousand Windows machines.&lt;&#x2F;p&gt;
&lt;p&gt;Dask gets used on some of the world‚Äôs largest super-computers (I was&lt;br &#x2F;&gt;
logged into &lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.olcf.ornl.gov&#x2F;summit&quot;&gt;Summit&lt;&#x2F;a&gt;, the worlds largest super computer, just a few hours ago), and is deployed routinely on all major clouds.&lt;&#x2F;p&gt;
&lt;p&gt;However, Dask also scales down nicely. You can also just &lt;em&gt;import dask&lt;&#x2F;em&gt; and run it on a thread pool in a single python process or Jupyter notebook. As we like to say, &lt;em&gt;‚ÄúThe median cluster size is one‚Äù&lt;&#x2F;em&gt;. Dask is pure-Python, and super-lightweight if it needs to be. You can just &lt;code&gt;pip install dask&lt;&#x2F;code&gt; and it ships with the popular Anaconda distribution, which is deployed on millions of machines around the world.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The Dask scheduler and Dask worker architecture, implementation and protocol was inspired by any other project?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The central scheduler + distributed worker architecture is pretty common today. It‚Äôs a pragmatic choice for systems that want to scale between 1‚Äì1000 nodes.&lt;&#x2F;p&gt;
&lt;p&gt;So sure, Dask was inspired by other projects. All of them :). Notably, Dask tries hard not to reinvent too much. We rely a ton on other infrastructure within the Python ecosystem. We use Tornado and asyncio for concurrency and peer-to-peer networking, Numpy, Pandas, and Scikit-learn for computation, and other Python APIs like concurrent.futures and joblib for user APIs.&lt;&#x2F;p&gt;
&lt;p&gt;Dask is really just a smashing together of Python‚Äôs networking stack with its&lt;br &#x2F;&gt;
data science stack. Most of the work was already done by the time we got here.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Which do are for you the most interesting frameworks, tools or libraries implemented on top of Dask and why?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I‚Äôll list a few interesting frameworks, but there are a ton out there these days:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;xarray.pydata.org&quot;&gt;Xarray&lt;&#x2F;a&gt; is a library commonly used to study Earth system data, like the climate, meteorology, oceanography, satellite imagery, and more. It‚Äôs really gratifying to see people use Dask to finally be able to analyze these huge climate science simulations, and help us better understand the planet.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prefect.io&quot;&gt;Prefect&lt;&#x2F;a&gt; provides a bunch of niceties on top of Dask for common Data Engineering or ETL workloads, similar to Airflow&#x2F;Luigi. We got these feature requests constantly when we were starting out but declared them out of scope. It was great to have another project come by, take that feature set, and implement it way better than we ever could.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;epistasislab.github.io&#x2F;tpot&quot;&gt;TPot&lt;&#x2F;a&gt; is a library for automatic machine learning. You give it a dataset, and it tries out a bunch of models and pre-processors to find a good model. TPot existed well before Dask, and it has really gnarly parallelism internally, which makes it hard for non-experts to accelerate. Fortunately the TPot and Dask developers were able to get this going in a weekend, and now you can scale out this search with Dask on whatever parallel hardware you have.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener external&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;rapids.ai&quot;&gt;RAPIDS&lt;&#x2F;a&gt; is a GPU-accelerated data science stack by NVIDIA. They were building out their own fast GPU implementation of Pandas and Numpy and wanted something to solve the multi-node problem for them. Dask was able to step in, handle all of the distributed communication, scheduling, and load balancing, and then step aside while NVIDIA‚Äôs fast GPU algorithms took over. (disclaimer, this is my current employer).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Could you please tell us about the work you are doing at NVIDIA to offload Dask computations to the GPU?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Yeah, RAPIDS is really exciting. It turns out that GPUs are good for things other than graphics and deep learning. They‚Äôre surprisingly effective at accelerating more traditional computing in data science (and actual science). Operations like dataframe joins, CSV parsing, FFTs, text processing, and more can often be accelerated 10x-200x. Historically you had to know C and CUDA to use these libraries though, which made them accessible only to somewhat experience software developers.&lt;&#x2F;p&gt;
&lt;p&gt;The RAPIDS project within NVIDIA is wrapping up all of these algorithms in Python, and exposing APIs to data science users that look like drop-in replacements for Numpy&#x2F;Pandas&#x2F;Scikit-Learn.&lt;&#x2F;p&gt;
&lt;p&gt;They use Dask to provide multi-GPU parallelism (some people have many GPUs in a single machine) and multi-node parallelism across a cluster. Dask‚Äôs&lt;br &#x2F;&gt;
flexibility, and the fact that it‚Äôs pretty unopinionated about what you run as&lt;br &#x2F;&gt;
computation make it the perfect fit. It‚Äôs also one of the only task schedulers&lt;br &#x2F;&gt;
out there that run in a non-JVM language, which helps if you use natively&lt;br &#x2F;&gt;
compiled code, like CUDA.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Do you have any book, MOOC or resource that you would recommend to those of us that want to learn more about the implementation of schedulers, concurrency models and distributed systems?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Ha! Sadly no.&lt;&#x2F;p&gt;
&lt;p&gt;Centrally managed distributed schedulers are, unfortunately, not a common topic of research these days. From an academic&#x2F;intellectual level it‚Äôs a fairly&lt;br &#x2F;&gt;
simple problem. Most of the difficult parts are in the details of engineering,&lt;br &#x2F;&gt;
which are unfortunately not that interesting to anyone who isn‚Äôt building a&lt;br &#x2F;&gt;
distributed scheduler.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
