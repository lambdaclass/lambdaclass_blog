<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Julia GPU - LambdaClass Blog</title>
    <meta name="description" content="How the Julia language is making it easy for programmers to use GPU capabilities">

    <!-- Feeds -->
    <link rel="alternate" type="application/rss+xml" title="RSS" href="https://blog.lambdaclass.com/rss.xml">
    <link rel="alternate" type="application/atom+xml" title="Atom" href="https://blog.lambdaclass.com/atom.xml">

    <!-- Styles -->
    <link rel="stylesheet" href="https://blog.lambdaclass.com/style.css">

    <!-- Preload fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

    <!-- Math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script>
</head>
<body>
    <div class="site-wrapper">
        <header class="site-header">
            <nav class="nav-container">
                <a href="https://blog.lambdaclass.com" class="site-logo">
                    <span class="logo-text">LambdaClass</span>
                </a>
                <div class="nav-links">
                    <a href="https://blog.lambdaclass.com" >Home</a>
                    <a href="https://blog.lambdaclass.com/tags" >Topics</a>
                    <a href="https://github.com/lambdaclass" target="_blank" rel="noopener">GitHub</a>
                    <a href="https://x.com/class_lambda" target="_blank" rel="noopener">X</a>
                    <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                        <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg>
                        <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>
                    </button>
                </div>
            </nav>
        </header>

        <main class="site-main">
            
<article class="page-article">
    <header class="page-header">
        <h1 class="page-title">Julia GPU</h1>
        
        <div class="page-meta">
            <time datetime="2020-10-20">October 20, 2020</time>
        </div>
        
    </header>

    <div class="page-content prose">
        <h3 id="how-the-julia-language-is-making-it-easy-for-programmers-to-use-gpu-capabilities-with-juliagpu">How the Julia language is making it easy for programmers to use GPU capabilities with JuliaGPU</h3>
<p><img src="/images/max/2000/1-KJX3T1Y9T1Cj0aV3m-A22w.png" alt="" /></p>
<p>We are living in a time where more and more data is being created every day as well as new techniques and complex algorithms that try to extract the most out of it. As such, CPU capabilities are approaching a bottleneck in their computing power. GPU computing opened its way into a new paradigm for high-performance and parallel computation a long time ago, but it was not until recently that it become massively used for data science.<br />
In this interview, <a rel="noopener external" target="_blank" href="https://twitter.com/maleadt">Tim Besard</a>, one of the main contributors to the JuliaGPU project, digs into some of the details about GPU computing and the features that make Julia a language suited for such tasks, not only from a performance perspective but also from a user one.</p>
<hr />
<h4 id="please-tell-us-a-bit-about-yourself-what-is-your-background-what-is-your-current-position">Please tell us a bit about yourself. What is your background? what is your current position?</h4>
<p>I’ve always been interested in systems programming, and after obtaining my CS degree I got the opportunity to start a PhD at Ghent University, Belgium, right when Julia was first released around 2012. The language seemed intriguing, and since I wanted to gain some experience with LLVM, I decided to port some image processing research code from MATLAB and C++ to Julia. The goal was to match performance of the C++ version, but some of its kernels were implemented in CUDA C… So obviously Julia needed a GPU back-end!</p>
<p>That was easier said than done, of course, and much of my PhD was about implementing that back-end and (re)structuring the existing Julia compiler to facilitate these additional back-ends. Nowadays I’m at Julia Computing, where I still work on everything GPU-related.</p>
<h4 id="what-is-juliagpu-what-is-the-goal-of-the-project">What is JuliaGPU? What is the goal of the project?</h4>
<p>JuliaGPU is the name we use to group GPU-related resources in Julia: There’s a <a rel="noopener external" target="_blank" href="https://github.com/JuliaGPU">GitHub organization</a> where most packages are hosted, a <a rel="noopener external" target="_blank" href="https://juliagpu.org/">website</a> to point the way for new users, we have <a rel="noopener external" target="_blank" href="https://github.com/JuliaGPU/gitlab-ci">CI infrastructure</a> for JuliaGPU projects, there’s a Slack channel and Discourse category, etc.</p>
<p>The goal of all this is to make it easier to use GPUs for all kinds of users. Current technologies often impose significant barriers to entry: CUDA is fairly tricky to install, C and C++ are not familiar to many users, etc. With the software we develop as part of the JuliaGPU organization, we aim to make it easy to use GPUs, without hindering the ability to optimize or use low-level features that the hardware has to offer.</p>
<h4 id="what-is-gpu-computing-how-important-is-it-nowadays">What is GPU computing? How important is it nowadays?</h4>
<p>GPU computing means using the GPU, a device originally designed for graphics processing, to perform general-purpose computations. It has grown more important now that CPU performance is not improving as steadily as it used to. Instead, specialized devices like GPUs or FPGAs are increasingly used to improve the performance of certain computations. In the case of GPUs, the architecture is a great fit to perform highly-parallel applications. Machine learning networks are a good example of such parallel applications, and their popularity is one of the reasons GPUs have become so important.</p>
<h4 id="do-you-think-julia-is-an-appropriate-language-to-efficiently-use-gpu-capabilities-why">Do you think Julia is an appropriate language to efficiently use GPU capabilities? Why?</h4>
<p>Julia’s main advantage is that the language was designed to be compiled. Even though the syntax is high-level, the generated machine code is<br />
compact and has great performance characteristics (for more details, see <a rel="noopener external" target="_blank" href="http://janvitek.org/pubs/oopsla18b.pdf">this paper</a>). This is crucial for GPU execution, where we are required to run native binaries and cannot easily (or efficiently) interpret code as is often required by other language’s semantics.</p>
<p>Because we’re able to directly compile Julia for GPUs, we can use almost all of the language’s features to build powerful abstractions. For example, you can define your own types, use those in GPU arrays, compose that with existing abstractions like lazy “Transpose” wrappers, access those on the GPU while benefiting from automatic bounds-checking (if needed), etc.</p>
<h4 id="from-a-python-programmer-perspective-how-does-cuda-jl-compare-to-pycuda-are-their-functionalities-equivalent">From a Python programmer perspective, how does CUDA.jl compare to PyCUDA? Are their functionalities equivalent?</h4>
<p>PyCUDA gives the programmer access to the CUDA APIs, with high-level Python functions that are much easier to use. CUDA.jl provides the same, but in Julia. The <code>hello world</code> from PyCUDA’s home page looks almost identical in Julia:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>using CUDA</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>function multiply_them(dest, a, b)</span></span>
<span class="giallo-l"><span> i = threadIdx().x</span></span>
<span class="giallo-l"><span> dest[i] = a[i] * b[i]</span></span>
<span class="giallo-l"><span> return</span></span>
<span class="giallo-l"><span>end</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>a = CuArray(randn(Float32, 400))</span></span>
<span class="giallo-l"><span>b = CuArray(randn(Float32, 400))</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>dest = similar(a)</span></span>
<span class="giallo-l"><span>@cuda threads=400 multiply_them(dest, a, b)</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>println(dest-a.*b)</span></span></code></pre>
<p>There’s one very big difference: “multiply_them” here is a function written in Julia, whereas PyCUDA uses a kernel written in CUDA C. The reason is straightforward: Python is not simple to compile. Of course, projects like Numba prove that it is very much possible to do so, but in the end those are separate compilers that try to match the reference Python compilers as closely as possible. With CUDA.jl, we integrate with that reference compiler, so it’s much easier to guarantee consistent semantics and follow suit when the language changes (for more details,<br />
refer to <a rel="noopener external" target="_blank" href="https://arxiv.org/abs/1712.03112">this paper</a>).</p>
<h4 id="are-the-packages-in-the-juliagpu-organization-targeted-to-experienced-programmers-only">Are the packages in the JuliaGPU organization targeted to experienced programmers only?</h4>
<p>Not at all. CUDA.jl targets different kinds of (GPU) programmers. If you are confident writing your own kernels, you can do so, while using all of the low-level features CUDA GPUs have to offer. But if you are new to the world of GPU programming, you can use high-level array operations that use existing kernels in CUDA.jl. For example, the above element-wise multiplication could just as well be written as:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>using CUDA</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>a = CuArray(randn(Float32, 400))</span></span>
<span class="giallo-l"><span>b = CuArray(randn(Float32, 400))</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>dest = a .* b</span></span></code></pre><h4 id="is-it-necessary-to-know-how-to-code-in-cuda-jl-to-take-full-advantage-of-gpu-computing-in-julia">Is it necessary to know how to code in CUDA.jl to take full advantage of GPU computing in Julia?</h4>
<p>Not for most users. Julia has a powerful language of generic array operations (“map”, “reduce”, “broadcast”, “accumulate”, etc) which can be applied to all kinds of arrays, including GPU arrays. That means you can often re-use your codebase developed for the CPU with CUDA.jl (<a rel="noopener external" target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0965997818310123">this paper</a> shows some powerful examples). Doing so often requires minimal changes: changing the array type, making sure you use array operations instead of for loops, etc.</p>
<p>It’s possible you need to go beyond this style of programming, e.g., because your application doesn’t map cleanly onto array operations, to use specific GPU features, etc. In that case, some basic knowledge about CUDA and the GPU programming model is sufficient to write kernels in CUDA.jl.</p>
<h4 id="how-is-the-experience-of-coding-a-kernel-in-cuda-jl-in-comparison-to-cuda-c-and-how-transferable-is-the-knowledge-to-one-another">How is the experience of coding a kernel in CUDA.jl in comparison to CUDA C and how transferable is the knowledge to one another?</h4>
<p>It’s very similar, and that’s by design: We try to keep the kernel abstractions in CUDA.jl close to their CUDA C counterparts such that the programming environment is familiar to existing GPU programmers. Of course, by using a high-level source language there’s many quality-of-life improvements. You can allocated shared memory, for example, statically and dynamically as in CUDA C, but instead of a raw pointers we use an N-dimensional array object you can easily index. An example from the <a rel="noopener external" target="_blank" href="https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/">NVIDIA developer blog</a>:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>__global__ void staticReverse(int *d, int n)</span></span>
<span class="giallo-l"><span>{</span></span>
<span class="giallo-l"><span> __shared__ int s[64];</span></span>
<span class="giallo-l"><span> int t = threadIdx.x;</span></span>
<span class="giallo-l"><span> int tr = n-t-1;</span></span>
<span class="giallo-l"><span> s[t] = d[t];</span></span>
<span class="giallo-l"><span> __syncthreads();</span></span>
<span class="giallo-l"><span> d[t] = s[tr];</span></span>
<span class="giallo-l"><span>}</span></span></code></pre>
<p>The CUDA.jl equivalent of this kernel looks very familiar, but uses array objects instead of raw pointers:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>function staticReverse(d)</span></span>
<span class="giallo-l"><span> s = @cuStaticSharedMem(Int, 64)</span></span>
<span class="giallo-l"><span> t = threadIdx().x</span></span>
<span class="giallo-l"><span> tr = length(d)-t+1</span></span>
<span class="giallo-l"><span> s[t] = d[t]</span></span>
<span class="giallo-l"><span> sync_threads()</span></span>
<span class="giallo-l"><span> d[t] = s[tr]</span></span>
<span class="giallo-l"><span> return</span></span>
<span class="giallo-l"><span>end</span></span></code></pre>
<p>Using array objects has many advantages, e.g. multi-dimensional is greatly simplified and we can just do “d[i,j]”. But it’s also safer, because these accesses are bounds checked:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>julia&gt; a = CuArray(1:64)</span></span>
<span class="giallo-l"><span>64-element CuArray{Int64,1}:</span></span>
<span class="giallo-l"><span> 1</span></span>
<span class="giallo-l"><span> 2</span></span>
<span class="giallo-l"><span> 3</span></span>
<span class="giallo-l"><span> ⋮</span></span>
<span class="giallo-l"><span> 62</span></span>
<span class="giallo-l"><span> 63</span></span>
<span class="giallo-l"><span> 64</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>julia&gt; @cuda threads=65 staticReverse(a)</span></span>
<span class="giallo-l"><span>ERROR: a exception was thrown during kernel execution.</span></span>
<span class="giallo-l"><span>Stacktrace:</span></span>
<span class="giallo-l"><span> [1] throw_boundserror at abstractarray.jl:541</span></span></code></pre>
<p>Bounds checking isn’t free, of course, and once we’re certain our code is correct we can add an “@inbounds” annotation to our kernel and get the high-performance code we expect:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>julia&gt; @device_code_ptx @cuda threads=64 staticReverse(a)</span></span>
<span class="giallo-l"><span>.visible .entry staticReverse(.param .align 8 .b8 d[16]) {</span></span>
<span class="giallo-l"><span> .reg .b32 %r&lt;2&gt;;</span></span>
<span class="giallo-l"><span> .reg .b64 %rd&lt;15&gt;;</span></span>
<span class="giallo-l"><span> .shared .align 32 .b8 s[512];</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>mov.b64 %rd1, d;</span></span>
<span class="giallo-l"><span> ld.param.u64 %rd2, [%rd1];</span></span>
<span class="giallo-l"><span> ld.param.u64 %rd3, [%rd1+8];</span></span>
<span class="giallo-l"><span> mov.u32 %r1, %tid.x;</span></span>
<span class="giallo-l"><span> cvt.u64.u32 %rd4, %r1;</span></span>
<span class="giallo-l"><span> mul.wide.u32 %rd5, %r1, 8;</span></span>
<span class="giallo-l"><span> add.s64 %rd6, %rd5, -8;</span></span>
<span class="giallo-l"><span> add.s64 %rd7, %rd3, %rd6;</span></span>
<span class="giallo-l"><span> ld.global.u64 %rd8, [%rd7+8];</span></span>
<span class="giallo-l"><span> mov.u64 %rd9, s;</span></span>
<span class="giallo-l"><span> add.s64 %rd10, %rd9, %rd6;</span></span>
<span class="giallo-l"><span> st.shared.u64 [%rd10+8], %rd8;</span></span>
<span class="giallo-l"><span> bar.sync 0;</span></span>
<span class="giallo-l"><span> sub.s64 %rd11, %rd2, %rd4;</span></span>
<span class="giallo-l"><span> shl.b64 %rd12, %rd11, 3;</span></span>
<span class="giallo-l"><span> add.s64 %rd13, %rd9, %rd12;</span></span>
<span class="giallo-l"><span> ld.shared.u64 %rd14, [%rd13+-8];</span></span>
<span class="giallo-l"><span> st.global.u64 [%rd7+8], %rd14;</span></span>
<span class="giallo-l"><span> ret;</span></span>
<span class="giallo-l"><span>}</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>julia&gt; a</span></span>
<span class="giallo-l"><span>64-element CuArray{Int64,1}:</span></span>
<span class="giallo-l"><span> 64</span></span>
<span class="giallo-l"><span> 63</span></span>
<span class="giallo-l"><span> 62</span></span>
<span class="giallo-l"><span> ⋮</span></span>
<span class="giallo-l"><span> 3</span></span>
<span class="giallo-l"><span> 2</span></span>
<span class="giallo-l"><span> 1</span></span></code></pre>
<p>Tools like “@device_code_ptx” make it easy for an experienced developer to inspect generated code and ensure the compiler does what he wants.</p>
<h4 id="why-does-having-a-compiler-have-such-an-impact-in-libraries-like-cuda-jl-how-was-the-process-of-integrating-it-to-the-julia-compiler">Why does having a compiler have such an impact in libraries like CUDA.jl? (How was the process of integrating it to the Julia compiler?)</h4>
<p>Because we have a compiler at our disposal, we can rely on higher-order functions and other generic abstractions that specialize based on the arguments that users provide. That greatly simplifies our library, but also gives the user very powerful tools. As an example, we have carefully implemented a <code>mapreduce</code> function that uses shared memory, warp intrinsics, etc to perform a high-performance reduction. The implementation is generic though, and will automatically re-specialize (even at run time) based on the arguments to the function:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>julia&gt; mapreduce(identity, +, CuArray([1,2,3]))</span></span>
<span class="giallo-l"><span>6</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>julia&gt; mapreduce(sin, *, CuArray([1.1,2.2,3.3]))</span></span>
<span class="giallo-l"><span>-0.11366175839582586</span></span></code></pre>
<p>With this powerful <code>mapreduce</code> abstraction, implemented by a experienced GPU programmer, other developers can create derived abstractions without such experience. For example, let’s implement a <code>count</code> function that evaluates for how many items a predicate holds true:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>count(predicate, array) = mapreduce(predicate, +, array)</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>julia&gt; a = CUDA.rand(Int8, 4)</span></span>
<span class="giallo-l"><span>4-element CuArray{Int8,1}:</span></span>
<span class="giallo-l"><span> 51</span></span>
<span class="giallo-l"><span> 3</span></span>
<span class="giallo-l"><span> 70</span></span>
<span class="giallo-l"><span> 100</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>julia&gt; count(iseven, a)</span></span>
<span class="giallo-l"><span>2</span></span></code></pre>
<p>Even though our <code>mapreduce</code> implementation has not been specifically implemented for the <code>Int8</code> type or the <code>iseven</code> predicate, the Julia compiler automatically specializes the implementation, resulting in kernel optimized for this specific invocation.</p>
<h4 id="what-were-the-biggest-challenges-when-developing-packages-for-juliagpu-particularly-writing-a-low-level-package-such-as-cuda-jl-in-a-high-level-programming-language-such-as-julia">What were the biggest challenges when developing packages for JuliaGPU, particularly writing a low level package such as CUDA.jl in a high level programming language such as Julia?</h4>
<p>Much of the initial work focused on developing tools that make it possible to write low-level code in Julia. For example, we developed the <a rel="noopener external" target="_blank" href="https://github.com/maleadt/LLVM.jl">LLVM.jl</a> package that gives us access to the LLVM APIs. Recently, our focus has shifted towards generalizing this functionality so that other GPU back-ends, like <a rel="noopener external" target="_blank" href="https://github.com/JuliaGPU/AMDGPU.jl">AMDGPU.jl</a> or <a rel="noopener external" target="_blank" href="https://github.com/JuliaGPU/oneAPI.jl">oneAPI.jl</a> can benefit from developments to CUDA.jl. Vendor-neutral array operations, for examples, are now implemented in <a rel="noopener external" target="_blank" href="https://github.com/JuliaGPU/GPUArrays.jl">GPUArrays.jl</a> whereas shared compiler functionality now lives in <a rel="noopener external" target="_blank" href="https://github.com/JuliaGPU/GPUCompiler.jl">GPUCompiler.jl</a>. That should make it possible to work on several GPU back-ends, even though most of them are maintained by only a single developer.</p>
<h4 id="regarding-the-latest-release-announced-in-the-juliagpu-blog-about-multi-device-programming-what-are-the-difficulties-that-this-new-functionality-solves-is-this-relevant-in-the-industry-where-big-computational-resources-are-needed">Regarding the <a rel="noopener external" target="_blank" href="https://juliagpu.org/2020-07-18-cuda_1.3/">latest release</a> announced in the JuliaGPU blog about multi-device programming, what are the difficulties that this new functionality solves? Is this relevant in the industry where big computational resources are needed?</h4>
<p>In industry or large research labs, MPI is often used to distribute work across multiple nodes or GPUs. Julia’s MPI.jl supports that use case, and integrates with CUDA.jl where necessary. The multi-device functionality added to CUDA 1.3 additionally makes it possible to use multiple GPUs within a single process. It maps nicely on Julia’s task-based concurrency, and makes it easy to distribute work within a single node:</p>
<pre class="giallo" style="color: #E1E4E8; background-color: #24292E;"><code data-lang="plain"><span class="giallo-l"><span>Threads.@threads for dev in devices()</span></span>
<span class="giallo-l"><span> device!(dev)</span></span>
<span class="giallo-l"><span> # do some work here</span></span>
<span class="giallo-l"><span>end</span></span></code></pre><h4 id="what-are-the-plans-for-the-near-future"><strong>What are the plans for the near future?</strong></h4>
<p>There aren’t any specific roadmaps, but one upcoming major feature is proper support for reduced-precision inputs, like 16-bits floating point. We already support Float16 arrays where CUBLAS or CUDNN does, but the next version of Julia will make it possible to write kernels that operate on these values.</p>
<p>Other than that, features come as they do :-) Be sure to subscribe to the <a rel="noopener external" target="_blank" href="https://juliagpu.org/post/">JuliaGPU blog</a> where we publish a short post for every major release of Julia’s GPU back-ends.</p>
<hr />
<p>You can find Tim at @<a rel="noopener external" target="_blank" href="https://twitter.com/maleadt">maleadt</a> on Twitter!</p>

    </div>
</article>

        </main>

        <footer class="site-footer">
            <div class="footer-container">
                <div class="footer-content">
                    <p class="footer-copyright">&copy; 2026 LambdaClass. All rights reserved.</p>
                    <div class="footer-links">
                        <a href="https://github.com/lambdaclass" target="_blank" rel="noopener">GitHub</a>
                        <a href="https://x.com/class_lambda" target="_blank" rel="noopener">X</a>
                        <a href="https://blog.lambdaclass.com/rss.xml">RSS</a>
                    </div>
                </div>
            </div>
        </footer>
    </div>

    <script>
        // Theme toggle functionality
        const themeToggle = document.getElementById('theme-toggle');
        const html = document.documentElement;

        // Check for saved preference or system preference
        const savedTheme = localStorage.getItem('theme');
        const systemPrefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;

        if (savedTheme) {
            html.setAttribute('data-theme', savedTheme);
        } else if (systemPrefersDark) {
            html.setAttribute('data-theme', 'dark');
        }

        themeToggle.addEventListener('click', () => {
            const currentTheme = html.getAttribute('data-theme');
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            html.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        });
    </script>
</body>
</html>
